{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install osmnx geopandas matplotlib\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geopandas.geodataframe.GeoDataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取陕西省的GeoDataFrame\n",
    "shaanxi = ox.geocode_to_gdf(query='Shaanxi, China')\n",
    "# 检查数据类型\n",
    "type(shaanxi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Geographic 2D CRS: EPSG:4326>\n",
       "Name: WGS 84\n",
       "Axis Info [ellipsoidal]:\n",
       "- Lat[north]: Geodetic latitude (degree)\n",
       "- Lon[east]: Geodetic longitude (degree)\n",
       "Area of Use:\n",
       "- name: World.\n",
       "- bounds: (-180.0, -90.0, 180.0, 90.0)\n",
       "Datum: World Geodetic System 1984 ensemble\n",
       "- Ellipsoid: WGS 84\n",
       "- Prime Meridian: Greenwich"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shaanxi.crs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on GeoDataFrame in module geopandas.geodataframe object:\n",
      "\n",
      "class GeoDataFrame(geopandas.base.GeoPandasBase, pandas.core.frame.DataFrame)\n",
      " |  GeoDataFrame(data=None, *args, geometry=None, crs=None, **kwargs)\n",
      " |\n",
      " |  A GeoDataFrame object is a pandas.DataFrame that has a column\n",
      " |  with geometry. In addition to the standard DataFrame constructor arguments,\n",
      " |  GeoDataFrame also accepts the following keyword arguments:\n",
      " |\n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  crs : value (optional)\n",
      " |      Coordinate Reference System of the geometry objects. Can be anything accepted by\n",
      " |      :meth:`pyproj.CRS.from_user_input() <pyproj.crs.CRS.from_user_input>`,\n",
      " |      such as an authority string (eg \"EPSG:4326\") or a WKT string.\n",
      " |  geometry : str or array (optional)\n",
      " |      If str, column to use as geometry. If array, will be set as 'geometry'\n",
      " |      column on GeoDataFrame.\n",
      " |\n",
      " |  Examples\n",
      " |  --------\n",
      " |  Constructing GeoDataFrame from a dictionary.\n",
      " |\n",
      " |  >>> from shapely.geometry import Point\n",
      " |  >>> d = {'col1': ['name1', 'name2'], 'geometry': [Point(1, 2), Point(2, 1)]}\n",
      " |  >>> gdf = geopandas.GeoDataFrame(d, crs=\"EPSG:4326\")\n",
      " |  >>> gdf\n",
      " |      col1                 geometry\n",
      " |  0  name1  POINT (1.00000 2.00000)\n",
      " |  1  name2  POINT (2.00000 1.00000)\n",
      " |\n",
      " |  Notice that the inferred dtype of 'geometry' columns is geometry.\n",
      " |\n",
      " |  >>> gdf.dtypes\n",
      " |  col1          object\n",
      " |  geometry    geometry\n",
      " |  dtype: object\n",
      " |\n",
      " |  Constructing GeoDataFrame from a pandas DataFrame with a column of WKT geometries:\n",
      " |\n",
      " |  >>> import pandas as pd\n",
      " |  >>> d = {'col1': ['name1', 'name2'], 'wkt': ['POINT (1 2)', 'POINT (2 1)']}\n",
      " |  >>> df = pd.DataFrame(d)\n",
      " |  >>> gs = geopandas.GeoSeries.from_wkt(df['wkt'])\n",
      " |  >>> gdf = geopandas.GeoDataFrame(df, geometry=gs, crs=\"EPSG:4326\")\n",
      " |  >>> gdf\n",
      " |      col1          wkt                 geometry\n",
      " |  0  name1  POINT (1 2)  POINT (1.00000 2.00000)\n",
      " |  1  name2  POINT (2 1)  POINT (2.00000 1.00000)\n",
      " |\n",
      " |  See also\n",
      " |  --------\n",
      " |  GeoSeries : Series object designed to store shapely geometry objects\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      GeoDataFrame\n",
      " |      geopandas.base.GeoPandasBase\n",
      " |      pandas.core.frame.DataFrame\n",
      " |      pandas.core.generic.NDFrame\n",
      " |      pandas.core.base.PandasObject\n",
      " |      pandas.core.accessor.DirNamesMixin\n",
      " |      pandas.core.indexing.IndexingMixin\n",
      " |      pandas.core.arraylike.OpsMixin\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __and__(self, other)\n",
      " |      Implement & operator as for builtin set type\n",
      " |\n",
      " |  __finalize__(self, other, method=None, **kwargs)\n",
      " |      propagate metadata from other to self\n",
      " |\n",
      " |  __getitem__(self, key)\n",
      " |      If the result is a column containing only 'geometry', return a\n",
      " |      GeoSeries. If it's a DataFrame with any columns of GeometryDtype,\n",
      " |      return a GeoDataFrame.\n",
      " |\n",
      " |  __init__(self, data=None, *args, geometry=None, crs=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  __or__(self, other)\n",
      " |      Implement | operator as for builtin set type\n",
      " |\n",
      " |  __setattr__(self, attr, val)\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  __setitem__(self, key, value)\n",
      " |      Overwritten to preserve CRS of GeometryArray in cases like\n",
      " |      df['geometry'] = [geom... for geom in df.geometry]\n",
      " |\n",
      " |  __setstate__(self, state)\n",
      " |\n",
      " |  __sub__(self, other)\n",
      " |      Implement - operator as for builtin set type\n",
      " |\n",
      " |  __xor__(self, other)\n",
      " |      Implement ^ operator as for builtin set type\n",
      " |\n",
      " |  apply(self, func, axis=0, raw=False, result_type=None, args=(), **kwargs)\n",
      " |      Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n",
      " |\n",
      " |      Data structure also contains labeled axes (rows and columns).\n",
      " |      Arithmetic operations align on both row and column labels. Can be\n",
      " |      thought of as a dict-like container for Series objects. The primary\n",
      " |      pandas data structure.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n",
      " |          Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n",
      " |          data is a dict, column order follows insertion-order. If a dict contains Series\n",
      " |          which have an index defined, it is aligned by its index. This alignment also\n",
      " |          occurs if data is a Series or a DataFrame itself. Alignment is done on\n",
      " |          Series/DataFrame inputs.\n",
      " |\n",
      " |          If data is a list of dicts, column order follows insertion-order.\n",
      " |\n",
      " |      index : Index or array-like\n",
      " |          Index to use for resulting frame. Will default to RangeIndex if\n",
      " |          no indexing information part of input data and no index provided.\n",
      " |      columns : Index or array-like\n",
      " |          Column labels to use for resulting frame when data does not have them,\n",
      " |          defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n",
      " |          will perform column selection instead.\n",
      " |      dtype : dtype, default None\n",
      " |          Data type to force. Only a single dtype is allowed. If None, infer.\n",
      " |      copy : bool or None, default None\n",
      " |          Copy data from inputs.\n",
      " |          For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n",
      " |          or 2d ndarray input, the default of None behaves like ``copy=False``.\n",
      " |          If data is a dict containing one or more Series (possibly of different dtypes),\n",
      " |          ``copy=False`` will ensure that these inputs are not copied.\n",
      " |\n",
      " |          .. versionchanged:: 1.3.0\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.from_records : Constructor from tuples, also record arrays.\n",
      " |      DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n",
      " |      read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
      " |      read_table : Read general delimited file into DataFrame.\n",
      " |      read_clipboard : Read text from clipboard into DataFrame.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Please reference the :ref:`User Guide <basics.dataframe>` for more information.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Constructing DataFrame from a dictionary.\n",
      " |\n",
      " |      >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n",
      " |      >>> df = pd.DataFrame(data=d)\n",
      " |      >>> df\n",
      " |         col1  col2\n",
      " |      0     1     3\n",
      " |      1     2     4\n",
      " |\n",
      " |      Notice that the inferred dtype is int64.\n",
      " |\n",
      " |      >>> df.dtypes\n",
      " |      col1    int64\n",
      " |      col2    int64\n",
      " |      dtype: object\n",
      " |\n",
      " |      To enforce a single dtype:\n",
      " |\n",
      " |      >>> df = pd.DataFrame(data=d, dtype=np.int8)\n",
      " |      >>> df.dtypes\n",
      " |      col1    int8\n",
      " |      col2    int8\n",
      " |      dtype: object\n",
      " |\n",
      " |      Constructing DataFrame from a dictionary including Series:\n",
      " |\n",
      " |      >>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n",
      " |      >>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n",
      " |         col1  col2\n",
      " |      0     0   NaN\n",
      " |      1     1   NaN\n",
      " |      2     2   2.0\n",
      " |      3     3   3.0\n",
      " |\n",
      " |      Constructing DataFrame from numpy ndarray:\n",
      " |\n",
      " |      >>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n",
      " |      ...                    columns=['a', 'b', 'c'])\n",
      " |      >>> df2\n",
      " |         a  b  c\n",
      " |      0  1  2  3\n",
      " |      1  4  5  6\n",
      " |      2  7  8  9\n",
      " |\n",
      " |      Constructing DataFrame from a numpy ndarray that has labeled columns:\n",
      " |\n",
      " |      >>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n",
      " |      ...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n",
      " |      >>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n",
      " |      ...\n",
      " |      >>> df3\n",
      " |         c  a\n",
      " |      0  3  1\n",
      " |      1  6  4\n",
      " |      2  9  7\n",
      " |\n",
      " |      Constructing DataFrame from dataclass:\n",
      " |\n",
      " |      >>> from dataclasses import make_dataclass\n",
      " |      >>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n",
      " |      >>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n",
      " |         x  y\n",
      " |      0  0  0\n",
      " |      1  0  3\n",
      " |      2  2  3\n",
      " |\n",
      " |      Constructing DataFrame from Series/DataFrame:\n",
      " |\n",
      " |      >>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n",
      " |      >>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n",
      " |      >>> df\n",
      " |         0\n",
      " |      a  1\n",
      " |      c  3\n",
      " |\n",
      " |      >>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n",
      " |      >>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n",
      " |      >>> df2\n",
      " |         x\n",
      " |      a  1\n",
      " |      c  3\n",
      " |\n",
      " |  astype(self, dtype, copy=None, errors='raise', **kwargs)\n",
      " |      Cast a pandas object to a specified dtype ``dtype``.\n",
      " |\n",
      " |      Returns a GeoDataFrame when the geometry column is kept as geometries,\n",
      " |      otherwise returns a pandas DataFrame.\n",
      " |\n",
      " |      See the pandas.DataFrame.astype docstring for more details.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      GeoDataFrame or DataFrame\n",
      " |\n",
      " |  clip(self, mask, keep_geom_type=False)\n",
      " |      Clip points, lines, or polygon geometries to the mask extent.\n",
      " |\n",
      " |      Both layers must be in the same Coordinate Reference System (CRS).\n",
      " |      The GeoDataFrame will be clipped to the full extent of the ``mask`` object.\n",
      " |\n",
      " |      If there are multiple polygons in mask, data from the GeoDataFrame will be\n",
      " |      clipped to the total boundary of all polygons in mask.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      mask : GeoDataFrame, GeoSeries, (Multi)Polygon, list-like\n",
      " |          Polygon vector layer used to clip the GeoDataFrame.\n",
      " |          The mask's geometry is dissolved into one geometric feature\n",
      " |          and intersected with GeoDataFrame.\n",
      " |          If the mask is list-like with four elements ``(minx, miny, maxx, maxy)``,\n",
      " |          ``clip`` will use a faster rectangle clipping\n",
      " |          (:meth:`~GeoSeries.clip_by_rect`), possibly leading to slightly different\n",
      " |          results.\n",
      " |      keep_geom_type : boolean, default False\n",
      " |          If True, return only geometries of original type in case of intersection\n",
      " |          resulting in multiple geometry types or GeometryCollections.\n",
      " |          If False, return all resulting geometries (potentially mixed types).\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      GeoDataFrame\n",
      " |          Vector data (points, lines, polygons) from the GeoDataFrame clipped to\n",
      " |          polygon boundary from mask.\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      clip : equivalent top-level function\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Clip points (grocery stores) with polygons (the Near West Side community):\n",
      " |\n",
      " |      >>> import geodatasets\n",
      " |      >>> chicago = geopandas.read_file(\n",
      " |      ...     geodatasets.get_path(\"geoda.chicago_health\")\n",
      " |      ... )\n",
      " |      >>> near_west_side = chicago[chicago[\"community\"] == \"NEAR WEST SIDE\"]\n",
      " |      >>> groceries = geopandas.read_file(\n",
      " |      ...     geodatasets.get_path(\"geoda.groceries\")\n",
      " |      ... ).to_crs(chicago.crs)\n",
      " |      >>> groceries.shape\n",
      " |      (148, 8)\n",
      " |\n",
      " |      >>> nws_groceries = groceries.clip(near_west_side)\n",
      " |      >>> nws_groceries.shape\n",
      " |      (7, 8)\n",
      " |\n",
      " |  convert_dtypes(self, *args, **kwargs)\n",
      " |      Convert columns to best possible dtypes using dtypes supporting ``pd.NA``.\n",
      " |\n",
      " |      Always returns a GeoDataFrame as no conversions are applied to the\n",
      " |      geometry column.\n",
      " |\n",
      " |      See the pandas.DataFrame.convert_dtypes docstring for more details.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      GeoDataFrame\n",
      " |\n",
      " |  copy(self, deep=True)\n",
      " |      Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n",
      " |\n",
      " |      Data structure also contains labeled axes (rows and columns).\n",
      " |      Arithmetic operations align on both row and column labels. Can be\n",
      " |      thought of as a dict-like container for Series objects. The primary\n",
      " |      pandas data structure.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n",
      " |          Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n",
      " |          data is a dict, column order follows insertion-order. If a dict contains Series\n",
      " |          which have an index defined, it is aligned by its index. This alignment also\n",
      " |          occurs if data is a Series or a DataFrame itself. Alignment is done on\n",
      " |          Series/DataFrame inputs.\n",
      " |\n",
      " |          If data is a list of dicts, column order follows insertion-order.\n",
      " |\n",
      " |      index : Index or array-like\n",
      " |          Index to use for resulting frame. Will default to RangeIndex if\n",
      " |          no indexing information part of input data and no index provided.\n",
      " |      columns : Index or array-like\n",
      " |          Column labels to use for resulting frame when data does not have them,\n",
      " |          defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n",
      " |          will perform column selection instead.\n",
      " |      dtype : dtype, default None\n",
      " |          Data type to force. Only a single dtype is allowed. If None, infer.\n",
      " |      copy : bool or None, default None\n",
      " |          Copy data from inputs.\n",
      " |          For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n",
      " |          or 2d ndarray input, the default of None behaves like ``copy=False``.\n",
      " |          If data is a dict containing one or more Series (possibly of different dtypes),\n",
      " |          ``copy=False`` will ensure that these inputs are not copied.\n",
      " |\n",
      " |          .. versionchanged:: 1.3.0\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.from_records : Constructor from tuples, also record arrays.\n",
      " |      DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n",
      " |      read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
      " |      read_table : Read general delimited file into DataFrame.\n",
      " |      read_clipboard : Read text from clipboard into DataFrame.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Please reference the :ref:`User Guide <basics.dataframe>` for more information.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Constructing DataFrame from a dictionary.\n",
      " |\n",
      " |      >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n",
      " |      >>> df = pd.DataFrame(data=d)\n",
      " |      >>> df\n",
      " |         col1  col2\n",
      " |      0     1     3\n",
      " |      1     2     4\n",
      " |\n",
      " |      Notice that the inferred dtype is int64.\n",
      " |\n",
      " |      >>> df.dtypes\n",
      " |      col1    int64\n",
      " |      col2    int64\n",
      " |      dtype: object\n",
      " |\n",
      " |      To enforce a single dtype:\n",
      " |\n",
      " |      >>> df = pd.DataFrame(data=d, dtype=np.int8)\n",
      " |      >>> df.dtypes\n",
      " |      col1    int8\n",
      " |      col2    int8\n",
      " |      dtype: object\n",
      " |\n",
      " |      Constructing DataFrame from a dictionary including Series:\n",
      " |\n",
      " |      >>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n",
      " |      >>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n",
      " |         col1  col2\n",
      " |      0     0   NaN\n",
      " |      1     1   NaN\n",
      " |      2     2   2.0\n",
      " |      3     3   3.0\n",
      " |\n",
      " |      Constructing DataFrame from numpy ndarray:\n",
      " |\n",
      " |      >>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n",
      " |      ...                    columns=['a', 'b', 'c'])\n",
      " |      >>> df2\n",
      " |         a  b  c\n",
      " |      0  1  2  3\n",
      " |      1  4  5  6\n",
      " |      2  7  8  9\n",
      " |\n",
      " |      Constructing DataFrame from a numpy ndarray that has labeled columns:\n",
      " |\n",
      " |      >>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n",
      " |      ...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n",
      " |      >>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n",
      " |      ...\n",
      " |      >>> df3\n",
      " |         c  a\n",
      " |      0  3  1\n",
      " |      1  6  4\n",
      " |      2  9  7\n",
      " |\n",
      " |      Constructing DataFrame from dataclass:\n",
      " |\n",
      " |      >>> from dataclasses import make_dataclass\n",
      " |      >>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n",
      " |      >>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n",
      " |         x  y\n",
      " |      0  0  0\n",
      " |      1  0  3\n",
      " |      2  2  3\n",
      " |\n",
      " |      Constructing DataFrame from Series/DataFrame:\n",
      " |\n",
      " |      >>> ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n",
      " |      >>> df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n",
      " |      >>> df\n",
      " |         0\n",
      " |      a  1\n",
      " |      c  3\n",
      " |\n",
      " |      >>> df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n",
      " |      >>> df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n",
      " |      >>> df2\n",
      " |         x\n",
      " |      a  1\n",
      " |      c  3\n",
      " |\n",
      " |  dissolve(self, by=None, aggfunc='first', as_index=True, level=None, sort=True, observed=False, dropna=True, **kwargs)\n",
      " |      Dissolve geometries within `groupby` into single observation.\n",
      " |      This is accomplished by applying the `unary_union` method\n",
      " |      to all geometries within a groupself.\n",
      " |\n",
      " |      Observations associated with each `groupby` group will be aggregated\n",
      " |      using the `aggfunc`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      by : str or list-like, default None\n",
      " |          Column(s) whose values define the groups to be dissolved. If None,\n",
      " |          the entire GeoDataFrame is considered as a single group. If a list-like\n",
      " |          object is provided, the values in the list are treated as categorical\n",
      " |          labels, and polygons will be combined based on the equality of\n",
      " |          these categorical labels.\n",
      " |      aggfunc : function or string, default \"first\"\n",
      " |          Aggregation function for manipulation of data associated\n",
      " |          with each group. Passed to pandas `groupby.agg` method.\n",
      " |          Accepted combinations are:\n",
      " |\n",
      " |          - function\n",
      " |          - string function name\n",
      " |          - list of functions and/or function names, e.g. [np.sum, 'mean']\n",
      " |          - dict of axis labels -> functions, function names or list of such.\n",
      " |      as_index : boolean, default True\n",
      " |          If true, groupby columns become index of result.\n",
      " |      level : int or str or sequence of int or sequence of str, default None\n",
      " |          If the axis is a MultiIndex (hierarchical), group by a\n",
      " |          particular level or levels.\n",
      " |\n",
      " |          .. versionadded:: 0.9.0\n",
      " |      sort : bool, default True\n",
      " |          Sort group keys. Get better performance by turning this off.\n",
      " |          Note this does not influence the order of observations within\n",
      " |          each group. Groupby preserves the order of rows within each group.\n",
      " |\n",
      " |          .. versionadded:: 0.9.0\n",
      " |      observed : bool, default False\n",
      " |          This only applies if any of the groupers are Categoricals.\n",
      " |          If True: only show observed values for categorical groupers.\n",
      " |          If False: show all values for categorical groupers.\n",
      " |\n",
      " |          .. versionadded:: 0.9.0\n",
      " |      dropna : bool, default True\n",
      " |          If True, and if group keys contain NA values, NA values\n",
      " |          together with row/column will be dropped. If False, NA\n",
      " |          values will also be treated as the key in groups.\n",
      " |\n",
      " |          .. versionadded:: 0.9.0\n",
      " |      **kwargs :\n",
      " |          Keyword arguments to be passed to the pandas `DataFrameGroupby.agg` method\n",
      " |          which is used by `dissolve`. In particular, `numeric_only` may be\n",
      " |          supplied, which will be required in pandas 2.0 for certain aggfuncs.\n",
      " |\n",
      " |          .. versionadded:: 0.13.0\n",
      " |      Returns\n",
      " |      -------\n",
      " |      GeoDataFrame\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Point\n",
      " |      >>> d = {\n",
      " |      ...     \"col1\": [\"name1\", \"name2\", \"name1\"],\n",
      " |      ...     \"geometry\": [Point(1, 2), Point(2, 1), Point(0, 1)],\n",
      " |      ... }\n",
      " |      >>> gdf = geopandas.GeoDataFrame(d, crs=4326)\n",
      " |      >>> gdf\n",
      " |          col1                 geometry\n",
      " |      0  name1  POINT (1.00000 2.00000)\n",
      " |      1  name2  POINT (2.00000 1.00000)\n",
      " |      2  name1  POINT (0.00000 1.00000)\n",
      " |\n",
      " |      >>> dissolved = gdf.dissolve('col1')\n",
      " |      >>> dissolved  # doctest: +SKIP\n",
      " |                                                  geometry\n",
      " |      col1\n",
      " |      name1  MULTIPOINT (0.00000 1.00000, 1.00000 2.00000)\n",
      " |      name2                        POINT (2.00000 1.00000)\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoDataFrame.explode : explode multi-part geometries into single geometries\n",
      " |\n",
      " |  estimate_utm_crs(self, datum_name='WGS 84')\n",
      " |      Returns the estimated UTM CRS based on the bounds of the dataset.\n",
      " |\n",
      " |      .. versionadded:: 0.9\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      datum_name : str, optional\n",
      " |          The name of the datum to use in the query. Default is WGS 84.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      pyproj.CRS\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import geodatasets\n",
      " |      >>> df = geopandas.read_file(\n",
      " |      ...     geodatasets.get_path(\"geoda.chicago_health\")\n",
      " |      ... )\n",
      " |      >>> df.estimate_utm_crs()  # doctest: +SKIP\n",
      " |      <Derived Projected CRS: EPSG:32616>\n",
      " |      Name: WGS 84 / UTM zone 16N\n",
      " |      Axis Info [cartesian]:\n",
      " |      - E[east]: Easting (metre)\n",
      " |      - N[north]: Northing (metre)\n",
      " |      Area of Use:\n",
      " |      - name: Between 90°W and 84°W, northern hemisphere between equator and 84°N...\n",
      " |      - bounds: (-90.0, 0.0, -84.0, 84.0)\n",
      " |      Coordinate Operation:\n",
      " |      - name: UTM zone 16N\n",
      " |      - method: Transverse Mercator\n",
      " |      Datum: World Geodetic System 1984 ensemble\n",
      " |      - Ellipsoid: WGS 84\n",
      " |      - Prime Meridian: Greenwich\n",
      " |\n",
      " |  explode(self, column=None, ignore_index=False, index_parts=None, **kwargs)\n",
      " |      Explode multi-part geometries into multiple single geometries.\n",
      " |\n",
      " |      Each row containing a multi-part geometry will be split into\n",
      " |      multiple rows with single geometries, thereby increasing the vertical\n",
      " |      size of the GeoDataFrame.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      column : string, default None\n",
      " |          Column to explode. In the case of a geometry column, multi-part\n",
      " |          geometries are converted to single-part.\n",
      " |          If None, the active geometry column is used.\n",
      " |      ignore_index : bool, default False\n",
      " |          If True, the resulting index will be labelled 0, 1, …, n - 1,\n",
      " |          ignoring `index_parts`.\n",
      " |      index_parts : boolean, default True\n",
      " |          If True, the resulting index will be a multi-index (original\n",
      " |          index with an additional level indicating the multiple\n",
      " |          geometries: a new zero-based index for each single part geometry\n",
      " |          per multi-part geometry).\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      GeoDataFrame\n",
      " |          Exploded geodataframe with each single geometry\n",
      " |          as a separate entry in the geodataframe.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely.geometry import MultiPoint\n",
      " |      >>> d = {\n",
      " |      ...     \"col1\": [\"name1\", \"name2\"],\n",
      " |      ...     \"geometry\": [\n",
      " |      ...         MultiPoint([(1, 2), (3, 4)]),\n",
      " |      ...         MultiPoint([(2, 1), (0, 0)]),\n",
      " |      ...     ],\n",
      " |      ... }\n",
      " |      >>> gdf = geopandas.GeoDataFrame(d, crs=4326)\n",
      " |      >>> gdf\n",
      " |          col1                                       geometry\n",
      " |      0  name1  MULTIPOINT (1.00000 2.00000, 3.00000 4.00000)\n",
      " |      1  name2  MULTIPOINT (2.00000 1.00000, 0.00000 0.00000)\n",
      " |\n",
      " |      >>> exploded = gdf.explode(index_parts=True)\n",
      " |      >>> exploded\n",
      " |            col1                 geometry\n",
      " |      0 0  name1  POINT (1.00000 2.00000)\n",
      " |        1  name1  POINT (3.00000 4.00000)\n",
      " |      1 0  name2  POINT (2.00000 1.00000)\n",
      " |        1  name2  POINT (0.00000 0.00000)\n",
      " |\n",
      " |      >>> exploded = gdf.explode(index_parts=False)\n",
      " |      >>> exploded\n",
      " |          col1                 geometry\n",
      " |      0  name1  POINT (1.00000 2.00000)\n",
      " |      0  name1  POINT (3.00000 4.00000)\n",
      " |      1  name2  POINT (2.00000 1.00000)\n",
      " |      1  name2  POINT (0.00000 0.00000)\n",
      " |\n",
      " |      >>> exploded = gdf.explode(ignore_index=True)\n",
      " |      >>> exploded\n",
      " |          col1                 geometry\n",
      " |      0  name1  POINT (1.00000 2.00000)\n",
      " |      1  name1  POINT (3.00000 4.00000)\n",
      " |      2  name2  POINT (2.00000 1.00000)\n",
      " |      3  name2  POINT (0.00000 0.00000)\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoDataFrame.dissolve : dissolve geometries into a single observation.\n",
      " |\n",
      " |  explore(self, *args, **kwargs)\n",
      " |      Interactive map based on GeoPandas and folium/leaflet.js\n",
      " |\n",
      " |      Generate an interactive leaflet map based on :class:`~geopandas.GeoDataFrame`\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      column : str, np.array, pd.Series (default None)\n",
      " |          The name of the dataframe column, :class:`numpy.array`,\n",
      " |          or :class:`pandas.Series` to be plotted. If :class:`numpy.array` or\n",
      " |          :class:`pandas.Series` are used then it must have same length as dataframe.\n",
      " |      cmap : str, matplotlib.Colormap, branca.colormap or function (default None)\n",
      " |          The name of a colormap recognized by ``matplotlib``, a list-like of colors,\n",
      " |          :class:`matplotlib.colors.Colormap`, a :class:`branca.colormap.ColorMap` or\n",
      " |          function that returns a named color or hex based on the column\n",
      " |          value, e.g.::\n",
      " |\n",
      " |              def my_colormap(value):  # scalar value defined in 'column'\n",
      " |                  if value > 1:\n",
      " |                      return \"green\"\n",
      " |                  return \"red\"\n",
      " |\n",
      " |      color : str, array-like (default None)\n",
      " |          Named color or a list-like of colors (named or hex).\n",
      " |      m : folium.Map (default None)\n",
      " |          Existing map instance on which to draw the plot.\n",
      " |      tiles : str, xyzservices.TileProvider (default 'OpenStreetMap Mapnik')\n",
      " |          Map tileset to use. Can choose from the list supported by folium, query a\n",
      " |          :class:`xyzservices.TileProvider` by a name from ``xyzservices.providers``,\n",
      " |          pass :class:`xyzservices.TileProvider` object or pass custom XYZ URL.\n",
      " |          The current list of built-in providers (when ``xyzservices`` is not available):\n",
      " |\n",
      " |          ``[\"OpenStreetMap\", \"CartoDB positron\", “CartoDB dark_matter\"]``\n",
      " |\n",
      " |          You can pass a custom tileset to Folium by passing a Leaflet-style URL\n",
      " |          to the tiles parameter: ``http://{s}.yourtiles.com/{z}/{x}/{y}.png``.\n",
      " |          Be sure to check their terms and conditions and to provide attribution with\n",
      " |          the ``attr`` keyword.\n",
      " |      attr : str (default None)\n",
      " |          Map tile attribution; only required if passing custom tile URL.\n",
      " |      tooltip : bool, str, int, list (default True)\n",
      " |          Display GeoDataFrame attributes when hovering over the object.\n",
      " |          ``True`` includes all columns. ``False`` removes tooltip. Pass string or list of\n",
      " |          strings to specify a column(s). Integer specifies first n columns to be\n",
      " |          included. Defaults to ``True``.\n",
      " |      popup : bool, str, int, list (default False)\n",
      " |          Input GeoDataFrame attributes for object displayed when clicking.\n",
      " |          ``True`` includes all columns. ``False`` removes popup. Pass string or list of\n",
      " |          strings to specify a column(s). Integer specifies first n columns to be\n",
      " |          included. Defaults to ``False``.\n",
      " |      highlight : bool (default True)\n",
      " |          Enable highlight functionality when hovering over a geometry.\n",
      " |      categorical : bool (default False)\n",
      " |          If ``False``, ``cmap`` will reflect numerical values of the\n",
      " |          column being plotted. For non-numerical columns, this\n",
      " |          will be set to True.\n",
      " |      legend : bool (default True)\n",
      " |          Plot a legend in choropleth plots.\n",
      " |          Ignored if no ``column`` is given.\n",
      " |      scheme : str (default None)\n",
      " |          Name of a choropleth classification scheme (requires ``mapclassify`` >= 2.4.0).\n",
      " |          A :func:`mapclassify.classify` will be used\n",
      " |          under the hood. Supported are all schemes provided by ``mapclassify`` (e.g.\n",
      " |          ``'BoxPlot'``, ``'EqualInterval'``, ``'FisherJenks'``, ``'FisherJenksSampled'``,\n",
      " |          ``'HeadTailBreaks'``, ``'JenksCaspall'``, ``'JenksCaspallForced'``,\n",
      " |          ``'JenksCaspallSampled'``, ``'MaxP'``, ``'MaximumBreaks'``,\n",
      " |          ``'NaturalBreaks'``, ``'Quantiles'``, ``'Percentiles'``, ``'StdMean'``,\n",
      " |          ``'UserDefined'``). Arguments can be passed in ``classification_kwds``.\n",
      " |      k : int (default 5)\n",
      " |          Number of classes\n",
      " |      vmin : None or float (default None)\n",
      " |          Minimum value of ``cmap``. If ``None``, the minimum data value\n",
      " |          in the column to be plotted is used.\n",
      " |      vmax : None or float (default None)\n",
      " |          Maximum value of ``cmap``. If ``None``, the maximum data value\n",
      " |          in the column to be plotted is used.\n",
      " |      width : pixel int or percentage string (default: '100%')\n",
      " |          Width of the folium :class:`~folium.folium.Map`. If the argument\n",
      " |          m is given explicitly, width is ignored.\n",
      " |      height : pixel int or percentage string (default: '100%')\n",
      " |          Height of the folium :class:`~folium.folium.Map`. If the argument\n",
      " |          m is given explicitly, height is ignored.\n",
      " |      categories : list-like\n",
      " |          Ordered list-like object of categories to be used for categorical plot.\n",
      " |      classification_kwds : dict (default None)\n",
      " |          Keyword arguments to pass to mapclassify\n",
      " |      control_scale : bool, (default True)\n",
      " |          Whether to add a control scale on the map.\n",
      " |      marker_type : str, folium.Circle, folium.CircleMarker, folium.Marker (default None)\n",
      " |          Allowed string options are ('marker', 'circle', 'circle_marker'). Defaults to\n",
      " |          folium.CircleMarker.\n",
      " |      marker_kwds: dict (default {})\n",
      " |          Additional keywords to be passed to the selected ``marker_type``, e.g.:\n",
      " |\n",
      " |          radius : float (default 2 for ``circle_marker`` and 50 for ``circle``))\n",
      " |              Radius of the circle, in meters (for ``circle``) or pixels\n",
      " |              (for ``circle_marker``).\n",
      " |          fill : bool (default True)\n",
      " |              Whether to fill the ``circle`` or ``circle_marker`` with color.\n",
      " |          icon : folium.map.Icon\n",
      " |              the :class:`folium.map.Icon` object to use to render the marker.\n",
      " |          draggable : bool (default False)\n",
      " |              Set to True to be able to drag the marker around the map.\n",
      " |\n",
      " |      style_kwds : dict (default {})\n",
      " |          Additional style to be passed to folium ``style_function``:\n",
      " |\n",
      " |          stroke : bool (default True)\n",
      " |              Whether to draw stroke along the path. Set it to ``False`` to\n",
      " |              disable borders on polygons or circles.\n",
      " |          color : str\n",
      " |              Stroke color\n",
      " |          weight : int\n",
      " |              Stroke width in pixels\n",
      " |          opacity : float (default 1.0)\n",
      " |              Stroke opacity\n",
      " |          fill : boolean (default True)\n",
      " |              Whether to fill the path with color. Set it to ``False`` to\n",
      " |              disable filling on polygons or circles.\n",
      " |          fillColor : str\n",
      " |              Fill color. Defaults to the value of the color option\n",
      " |          fillOpacity : float (default 0.5)\n",
      " |              Fill opacity.\n",
      " |          style_function : callable\n",
      " |              Function mapping a GeoJson Feature to a style ``dict``.\n",
      " |\n",
      " |              * Style properties :func:`folium.vector_layers.path_options`\n",
      " |              * GeoJson features :class:`GeoDataFrame.__geo_interface__`\n",
      " |\n",
      " |              e.g.::\n",
      " |\n",
      " |                  lambda x: {\"color\":\"red\" if x[\"properties\"][\"gdp_md_est\"]<10**6\n",
      " |                                               else \"blue\"}\n",
      " |\n",
      " |          Plus all supported by :func:`folium.vector_layers.path_options`. See the\n",
      " |          documentation of :class:`folium.features.GeoJson` for details.\n",
      " |\n",
      " |      highlight_kwds : dict (default {})\n",
      " |          Style to be passed to folium highlight_function. Uses the same keywords\n",
      " |          as ``style_kwds``. When empty, defaults to ``{\"fillOpacity\": 0.75}``.\n",
      " |      tooltip_kwds : dict (default {})\n",
      " |          Additional keywords to be passed to :class:`folium.features.GeoJsonTooltip`,\n",
      " |          e.g. ``aliases``, ``labels``, or ``sticky``.\n",
      " |      popup_kwds : dict (default {})\n",
      " |          Additional keywords to be passed to :class:`folium.features.GeoJsonPopup`,\n",
      " |          e.g. ``aliases`` or ``labels``.\n",
      " |      legend_kwds : dict (default {})\n",
      " |          Additional keywords to be passed to the legend.\n",
      " |\n",
      " |          Currently supported customisation:\n",
      " |\n",
      " |          caption : string\n",
      " |              Custom caption of the legend. Defaults to the column name.\n",
      " |\n",
      " |          Additional accepted keywords when ``scheme`` is specified:\n",
      " |\n",
      " |          colorbar : bool (default True)\n",
      " |              An option to control the style of the legend. If True, continuous\n",
      " |              colorbar will be used. If False, categorical legend will be used for bins.\n",
      " |          scale : bool (default True)\n",
      " |              Scale bins along the colorbar axis according to the bin edges (True)\n",
      " |              or use the equal length for each bin (False)\n",
      " |          fmt : string (default \"{:.2f}\")\n",
      " |              A formatting specification for the bin edges of the classes in the\n",
      " |              legend. For example, to have no decimals: ``{\"fmt\": \"{:.0f}\"}``. Applies\n",
      " |              if ``colorbar=False``.\n",
      " |          labels : list-like\n",
      " |              A list of legend labels to override the auto-generated labels.\n",
      " |              Needs to have the same number of elements as the number of\n",
      " |              classes (`k`). Applies if ``colorbar=False``.\n",
      " |          interval : boolean (default False)\n",
      " |              An option to control brackets from mapclassify legend.\n",
      " |              If True, open/closed interval brackets are shown in the legend.\n",
      " |              Applies if ``colorbar=False``.\n",
      " |          max_labels : int, default 10\n",
      " |              Maximum number of colorbar tick labels (requires branca>=0.5.0)\n",
      " |      map_kwds : dict (default {})\n",
      " |          Additional keywords to be passed to folium :class:`~folium.folium.Map`,\n",
      " |          e.g. ``dragging``, or ``scrollWheelZoom``.\n",
      " |\n",
      " |\n",
      " |      **kwargs : dict\n",
      " |          Additional options to be passed on to the folium object.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      m : folium.folium.Map\n",
      " |          folium :class:`~folium.folium.Map` instance\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import geodatasets\n",
      " |      >>> df = geopandas.read_file(\n",
      " |      ...     geodatasets.get_path(\"geoda.chicago_health\")\n",
      " |      ... )\n",
      " |      >>> df.head(2)  # doctest: +SKIP\n",
      " |         ComAreaID  ...                                           geometry\n",
      " |      0         35  ...  POLYGON ((-87.60914 41.84469, -87.60915 41.844...\n",
      " |      1         36  ...  POLYGON ((-87.59215 41.81693, -87.59231 41.816...\n",
      " |\n",
      " |      [2 rows x 87 columns]\n",
      " |\n",
      " |      >>> df.explore(\"Pop2012\", cmap=\"Blues\")  # doctest: +SKIP\n",
      " |\n",
      " |  iterfeatures(self, na='null', show_bbox=False, drop_id=False)\n",
      " |      Returns an iterator that yields feature dictionaries that comply with\n",
      " |      __geo_interface__\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      na : str, optional\n",
      " |          Options are {'null', 'drop', 'keep'}, default 'null'.\n",
      " |          Indicates how to output missing (NaN) values in the GeoDataFrame\n",
      " |\n",
      " |          - null: output the missing entries as JSON null\n",
      " |          - drop: remove the property from the feature. This applies to each feature individually so that features may have different properties\n",
      " |          - keep: output the missing entries as NaN\n",
      " |\n",
      " |      show_bbox : bool, optional\n",
      " |          Include bbox (bounds) in the geojson. Default False.\n",
      " |      drop_id : bool, default: False\n",
      " |          Whether to retain the index of the GeoDataFrame as the id property\n",
      " |          in the generated GeoJSON. Default is False, but may want True\n",
      " |          if the index is just arbitrary row numbers.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely.geometry import Point\n",
      " |      >>> d = {'col1': ['name1', 'name2'], 'geometry': [Point(1, 2), Point(2, 1)]}\n",
      " |      >>> gdf = geopandas.GeoDataFrame(d, crs=\"EPSG:4326\")\n",
      " |      >>> gdf\n",
      " |          col1                 geometry\n",
      " |      0  name1  POINT (1.00000 2.00000)\n",
      " |      1  name2  POINT (2.00000 1.00000)\n",
      " |\n",
      " |      >>> feature = next(gdf.iterfeatures())\n",
      " |      >>> feature\n",
      " |      {'id': '0', 'type': 'Feature', 'properties': {'col1': 'name1'}, 'geometry': {'type': 'Point', 'coordinates': (1.0, 2.0)}}\n",
      " |\n",
      " |  merge(self, *args, **kwargs)\n",
      " |      Merge two ``GeoDataFrame`` objects with a database-style join.\n",
      " |\n",
      " |              Returns a ``GeoDataFrame`` if a geometry column is present; otherwise,\n",
      " |              returns a pandas ``DataFrame``.\n",
      " |\n",
      " |              Returns\n",
      " |              -------\n",
      " |              GeoDataFrame or DataFrame\n",
      " |\n",
      " |              Notes\n",
      " |              -----\n",
      " |              The extra arguments ``*args`` and keyword arguments ``**kwargs`` are\n",
      " |              passed to DataFrame.merge.\n",
      " |              See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas\\\n",
      " |      .DataFrame.merge.html\n",
      " |              for more details.\n",
      " |\n",
      " |  overlay(self, right, how='intersection', keep_geom_type=None, make_valid=True)\n",
      " |      Perform spatial overlay between GeoDataFrames.\n",
      " |\n",
      " |      Currently only supports data GeoDataFrames with uniform geometry types,\n",
      " |      i.e. containing only (Multi)Polygons, or only (Multi)Points, or a\n",
      " |      combination of (Multi)LineString and LinearRing shapes.\n",
      " |      Implements several methods that are all effectively subsets of the union.\n",
      " |\n",
      " |      See the User Guide page :doc:`../../user_guide/set_operations` for details.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      right : GeoDataFrame\n",
      " |      how : string\n",
      " |          Method of spatial overlay: 'intersection', 'union',\n",
      " |          'identity', 'symmetric_difference' or 'difference'.\n",
      " |      keep_geom_type : bool\n",
      " |          If True, return only geometries of the same geometry type the GeoDataFrame\n",
      " |          has, if False, return all resulting geometries. Default is None,\n",
      " |          which will set keep_geom_type to True but warn upon dropping\n",
      " |          geometries.\n",
      " |      make_valid : bool, default True\n",
      " |          If True, any invalid input geometries are corrected with a call to\n",
      " |          `buffer(0)`, if False, a `ValueError` is raised if any input geometries\n",
      " |          are invalid.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      df : GeoDataFrame\n",
      " |          GeoDataFrame with new set of polygons and attributes\n",
      " |          resulting from the overlay\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon\n",
      " |      >>> polys1 = geopandas.GeoSeries([Polygon([(0,0), (2,0), (2,2), (0,2)]),\n",
      " |      ...                               Polygon([(2,2), (4,2), (4,4), (2,4)])])\n",
      " |      >>> polys2 = geopandas.GeoSeries([Polygon([(1,1), (3,1), (3,3), (1,3)]),\n",
      " |      ...                               Polygon([(3,3), (5,3), (5,5), (3,5)])])\n",
      " |      >>> df1 = geopandas.GeoDataFrame({'geometry': polys1, 'df1_data':[1,2]})\n",
      " |      >>> df2 = geopandas.GeoDataFrame({'geometry': polys2, 'df2_data':[1,2]})\n",
      " |\n",
      " |      >>> df1.overlay(df2, how='union')\n",
      " |      df1_data  df2_data                                           geometry\n",
      " |      0       1.0       1.0  POLYGON ((2.00000 2.00000, 2.00000 1.00000, 1....\n",
      " |      1       2.0       1.0  POLYGON ((2.00000 2.00000, 2.00000 3.00000, 3....\n",
      " |      2       2.0       2.0  POLYGON ((4.00000 4.00000, 4.00000 3.00000, 3....\n",
      " |      3       1.0       NaN  POLYGON ((2.00000 0.00000, 0.00000 0.00000, 0....\n",
      " |      4       2.0       NaN  MULTIPOLYGON (((3.00000 4.00000, 3.00000 3.000...\n",
      " |      5       NaN       1.0  MULTIPOLYGON (((2.00000 3.00000, 2.00000 2.000...\n",
      " |      6       NaN       2.0  POLYGON ((3.00000 5.00000, 5.00000 5.00000, 5....\n",
      " |\n",
      " |      >>> df1.overlay(df2, how='intersection')\n",
      " |      df1_data  df2_data                                           geometry\n",
      " |      0         1         1  POLYGON ((2.00000 2.00000, 2.00000 1.00000, 1....\n",
      " |      1         2         1  POLYGON ((2.00000 2.00000, 2.00000 3.00000, 3....\n",
      " |      2         2         2  POLYGON ((4.00000 4.00000, 4.00000 3.00000, 3....\n",
      " |\n",
      " |      >>> df1.overlay(df2, how='symmetric_difference')\n",
      " |      df1_data  df2_data                                           geometry\n",
      " |      0       1.0       NaN  POLYGON ((2.00000 0.00000, 0.00000 0.00000, 0....\n",
      " |      1       2.0       NaN  MULTIPOLYGON (((3.00000 4.00000, 3.00000 3.000...\n",
      " |      2       NaN       1.0  MULTIPOLYGON (((2.00000 3.00000, 2.00000 2.000...\n",
      " |      3       NaN       2.0  POLYGON ((3.00000 5.00000, 5.00000 5.00000, 5....\n",
      " |\n",
      " |      >>> df1.overlay(df2, how='difference')\n",
      " |                                              geometry  df1_data\n",
      " |      0  POLYGON ((2.00000 0.00000, 0.00000 0.00000, 0....         1\n",
      " |      1  MULTIPOLYGON (((3.00000 4.00000, 3.00000 3.000...         2\n",
      " |\n",
      " |      >>> df1.overlay(df2, how='identity')\n",
      " |      df1_data  df2_data                                           geometry\n",
      " |      0       1.0       1.0  POLYGON ((2.00000 2.00000, 2.00000 1.00000, 1....\n",
      " |      1       2.0       1.0  POLYGON ((2.00000 2.00000, 2.00000 3.00000, 3....\n",
      " |      2       2.0       2.0  POLYGON ((4.00000 4.00000, 4.00000 3.00000, 3....\n",
      " |      3       1.0       NaN  POLYGON ((2.00000 0.00000, 0.00000 0.00000, 0....\n",
      " |      4       2.0       NaN  MULTIPOLYGON (((3.00000 4.00000, 3.00000 3.000...\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoDataFrame.sjoin : spatial join\n",
      " |      overlay : equivalent top-level function\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Every operation in GeoPandas is planar, i.e. the potential third\n",
      " |      dimension is not taken into account.\n",
      " |\n",
      " |  rename_geometry(self, col, inplace=False)\n",
      " |      Renames the GeoDataFrame geometry column to\n",
      " |      the specified name. By default yields a new object.\n",
      " |\n",
      " |      The original geometry column is replaced with the input.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col : new geometry column label\n",
      " |      inplace : boolean, default False\n",
      " |          Modify the GeoDataFrame in place (do not create a new object)\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Point\n",
      " |      >>> d = {'col1': ['name1', 'name2'], 'geometry': [Point(1, 2), Point(2, 1)]}\n",
      " |      >>> df = geopandas.GeoDataFrame(d, crs=\"EPSG:4326\")\n",
      " |      >>> df1 = df.rename_geometry('geom1')\n",
      " |      >>> df1.geometry.name\n",
      " |      'geom1'\n",
      " |      >>> df.rename_geometry('geom1', inplace=True)\n",
      " |      >>> df.geometry.name\n",
      " |      'geom1'\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      geodataframe : GeoDataFrame\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoDataFrame.set_geometry : set the active geometry\n",
      " |\n",
      " |  set_crs(self, crs=None, epsg=None, inplace=False, allow_override=False)\n",
      " |      Set the Coordinate Reference System (CRS) of the ``GeoDataFrame``.\n",
      " |\n",
      " |      If there are multiple geometry columns within the GeoDataFrame, only\n",
      " |      the CRS of the active geometry column is set.\n",
      " |\n",
      " |      NOTE: The underlying geometries are not transformed to this CRS. To\n",
      " |      transform the geometries to a new CRS, use the ``to_crs`` method.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      crs : pyproj.CRS, optional if `epsg` is specified\n",
      " |          The value can be anything accepted\n",
      " |          by :meth:`pyproj.CRS.from_user_input() <pyproj.crs.CRS.from_user_input>`,\n",
      " |          such as an authority string (eg \"EPSG:4326\") or a WKT string.\n",
      " |      epsg : int, optional if `crs` is specified\n",
      " |          EPSG code specifying the projection.\n",
      " |      inplace : bool, default False\n",
      " |          If True, the CRS of the GeoDataFrame will be changed in place\n",
      " |          (while still returning the result) instead of making a copy of\n",
      " |          the GeoDataFrame.\n",
      " |      allow_override : bool, default False\n",
      " |          If the the GeoDataFrame already has a CRS, allow to replace the\n",
      " |          existing CRS, even when both are not equal.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Point\n",
      " |      >>> d = {'col1': ['name1', 'name2'], 'geometry': [Point(1, 2), Point(2, 1)]}\n",
      " |      >>> gdf = geopandas.GeoDataFrame(d)\n",
      " |      >>> gdf\n",
      " |          col1                 geometry\n",
      " |      0  name1  POINT (1.00000 2.00000)\n",
      " |      1  name2  POINT (2.00000 1.00000)\n",
      " |\n",
      " |      Setting CRS to a GeoDataFrame without one:\n",
      " |\n",
      " |      >>> gdf.crs is None\n",
      " |      True\n",
      " |\n",
      " |      >>> gdf = gdf.set_crs('epsg:3857')\n",
      " |      >>> gdf.crs  # doctest: +SKIP\n",
      " |      <Projected CRS: EPSG:3857>\n",
      " |      Name: WGS 84 / Pseudo-Mercator\n",
      " |      Axis Info [cartesian]:\n",
      " |      - X[east]: Easting (metre)\n",
      " |      - Y[north]: Northing (metre)\n",
      " |      Area of Use:\n",
      " |      - name: World - 85°S to 85°N\n",
      " |      - bounds: (-180.0, -85.06, 180.0, 85.06)\n",
      " |      Coordinate Operation:\n",
      " |      - name: Popular Visualisation Pseudo-Mercator\n",
      " |      - method: Popular Visualisation Pseudo Mercator\n",
      " |      Datum: World Geodetic System 1984\n",
      " |      - Ellipsoid: WGS 84\n",
      " |      - Prime Meridian: Greenwich\n",
      " |\n",
      " |      Overriding existing CRS:\n",
      " |\n",
      " |      >>> gdf = gdf.set_crs(4326, allow_override=True)\n",
      " |\n",
      " |      Without ``allow_override=True``, ``set_crs`` returns an error if you try to\n",
      " |      override CRS.\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoDataFrame.to_crs : re-project to another CRS\n",
      " |\n",
      " |  set_geometry(self, col, drop=False, inplace=False, crs=None)\n",
      " |      Set the GeoDataFrame geometry using either an existing column or\n",
      " |      the specified input. By default yields a new object.\n",
      " |\n",
      " |      The original geometry column is replaced with the input.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col : column label or array\n",
      " |      drop : boolean, default False\n",
      " |          Delete column to be used as the new geometry\n",
      " |      inplace : boolean, default False\n",
      " |          Modify the GeoDataFrame in place (do not create a new object)\n",
      " |      crs : pyproj.CRS, optional\n",
      " |          Coordinate system to use. The value can be anything accepted\n",
      " |          by :meth:`pyproj.CRS.from_user_input() <pyproj.crs.CRS.from_user_input>`,\n",
      " |          such as an authority string (eg \"EPSG:4326\") or a WKT string.\n",
      " |          If passed, overrides both DataFrame and col's crs.\n",
      " |          Otherwise, tries to get crs from passed col values or DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Point\n",
      " |      >>> d = {'col1': ['name1', 'name2'], 'geometry': [Point(1, 2), Point(2, 1)]}\n",
      " |      >>> gdf = geopandas.GeoDataFrame(d, crs=\"EPSG:4326\")\n",
      " |      >>> gdf\n",
      " |          col1                 geometry\n",
      " |      0  name1  POINT (1.00000 2.00000)\n",
      " |      1  name2  POINT (2.00000 1.00000)\n",
      " |\n",
      " |      Passing an array:\n",
      " |\n",
      " |      >>> df1 = gdf.set_geometry([Point(0,0), Point(1,1)])\n",
      " |      >>> df1\n",
      " |          col1                 geometry\n",
      " |      0  name1  POINT (0.00000 0.00000)\n",
      " |      1  name2  POINT (1.00000 1.00000)\n",
      " |\n",
      " |      Using existing column:\n",
      " |\n",
      " |      >>> gdf[\"buffered\"] = gdf.buffer(2)\n",
      " |      >>> df2 = gdf.set_geometry(\"buffered\")\n",
      " |      >>> df2.geometry\n",
      " |      0    POLYGON ((3.00000 2.00000, 2.99037 1.80397, 2....\n",
      " |      1    POLYGON ((4.00000 1.00000, 3.99037 0.80397, 3....\n",
      " |      Name: buffered, dtype: geometry\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      GeoDataFrame\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoDataFrame.rename_geometry : rename an active geometry column\n",
      " |\n",
      " |  sjoin(self, df, *args, **kwargs)\n",
      " |      Spatial join of two GeoDataFrames.\n",
      " |\n",
      " |      See the User Guide page :doc:`../../user_guide/mergingdata` for details.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      df : GeoDataFrame\n",
      " |      how : string, default 'inner'\n",
      " |          The type of join:\n",
      " |\n",
      " |          * 'left': use keys from left_df; retain only left_df geometry column\n",
      " |          * 'right': use keys from right_df; retain only right_df geometry column\n",
      " |          * 'inner': use intersection of keys from both dfs; retain only\n",
      " |            left_df geometry column\n",
      " |\n",
      " |      predicate : string, default 'intersects'\n",
      " |          Binary predicate. Valid values are determined by the spatial index used.\n",
      " |          You can check the valid values in left_df or right_df as\n",
      " |          ``left_df.sindex.valid_query_predicates`` or\n",
      " |          ``right_df.sindex.valid_query_predicates``\n",
      " |      lsuffix : string, default 'left'\n",
      " |          Suffix to apply to overlapping column names (left GeoDataFrame).\n",
      " |      rsuffix : string, default 'right'\n",
      " |          Suffix to apply to overlapping column names (right GeoDataFrame).\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import geodatasets\n",
      " |      >>> chicago = geopandas.read_file(\n",
      " |      ...     geodatasets.get_path(\"geoda.chicago_commpop\")\n",
      " |      ... )\n",
      " |      >>> groceries = geopandas.read_file(\n",
      " |      ...     geodatasets.get_path(\"geoda.groceries\")\n",
      " |      ... ).to_crs(chicago.crs)\n",
      " |\n",
      " |      >>> chicago.head()  # doctest: +SKIP\n",
      " |               community  ...                                           geometry\n",
      " |      0          DOUGLAS  ...  MULTIPOLYGON (((-87.60914 41.84469, -87.60915 ...\n",
      " |      1          OAKLAND  ...  MULTIPOLYGON (((-87.59215 41.81693, -87.59231 ...\n",
      " |      2      FULLER PARK  ...  MULTIPOLYGON (((-87.62880 41.80189, -87.62879 ...\n",
      " |      3  GRAND BOULEVARD  ...  MULTIPOLYGON (((-87.60671 41.81681, -87.60670 ...\n",
      " |      4          KENWOOD  ...  MULTIPOLYGON (((-87.59215 41.81693, -87.59215 ...\n",
      " |\n",
      " |      [5 rows x 9 columns]\n",
      " |\n",
      " |      >>> groceries.head()  # doctest: +SKIP\n",
      " |         OBJECTID     Ycoord  ...  Category                         geometry\n",
      " |      0        16  41.973266  ...       NaN  MULTIPOINT (-87.65661 41.97321)\n",
      " |      1        18  41.696367  ...       NaN  MULTIPOINT (-87.68136 41.69713)\n",
      " |      2        22  41.868634  ...       NaN  MULTIPOINT (-87.63918 41.86847)\n",
      " |      3        23  41.877590  ...       new  MULTIPOINT (-87.65495 41.87783)\n",
      " |      4        27  41.737696  ...       NaN  MULTIPOINT (-87.62715 41.73623)\n",
      " |      [5 rows x 8 columns]\n",
      " |\n",
      " |      >>> groceries_w_communities = groceries.sjoin(chicago)\n",
      " |      >>> groceries_w_communities[[\"OBJECTID\", \"community\", \"geometry\"]].head()\n",
      " |           OBJECTID    community                         geometry\n",
      " |      0          16       UPTOWN  MULTIPOINT (-87.65661 41.97321)\n",
      " |      87        365       UPTOWN  MULTIPOINT (-87.65465 41.96138)\n",
      " |      90        373       UPTOWN  MULTIPOINT (-87.65598 41.96297)\n",
      " |      140       582       UPTOWN  MULTIPOINT (-87.67417 41.96977)\n",
      " |      1          18  MORGAN PARK  MULTIPOINT (-87.68136 41.69713)\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Every operation in GeoPandas is planar, i.e. the potential third\n",
      " |      dimension is not taken into account.\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoDataFrame.sjoin_nearest : nearest neighbor join\n",
      " |      sjoin : equivalent top-level function\n",
      " |\n",
      " |  sjoin_nearest(self, right, how='inner', max_distance=None, lsuffix='left', rsuffix='right', distance_col=None, exclusive=False)\n",
      " |      Spatial join of two GeoDataFrames based on the distance between their\n",
      " |      geometries.\n",
      " |\n",
      " |      Results will include multiple output records for a single input record\n",
      " |      where there are multiple equidistant nearest or intersected neighbors.\n",
      " |\n",
      " |      See the User Guide page\n",
      " |      https://geopandas.readthedocs.io/en/latest/docs/user_guide/mergingdata.html\n",
      " |      for more details.\n",
      " |\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      right : GeoDataFrame\n",
      " |      how : string, default 'inner'\n",
      " |          The type of join:\n",
      " |\n",
      " |          * 'left': use keys from left_df; retain only left_df geometry column\n",
      " |          * 'right': use keys from right_df; retain only right_df geometry column\n",
      " |          * 'inner': use intersection of keys from both dfs; retain only\n",
      " |            left_df geometry column\n",
      " |\n",
      " |      max_distance : float, default None\n",
      " |          Maximum distance within which to query for nearest geometry.\n",
      " |          Must be greater than 0.\n",
      " |          The max_distance used to search for nearest items in the tree may have a\n",
      " |          significant impact on performance by reducing the number of input\n",
      " |          geometries that are evaluated for nearest items in the tree.\n",
      " |      lsuffix : string, default 'left'\n",
      " |          Suffix to apply to overlapping column names (left GeoDataFrame).\n",
      " |      rsuffix : string, default 'right'\n",
      " |          Suffix to apply to overlapping column names (right GeoDataFrame).\n",
      " |      distance_col : string, default None\n",
      " |          If set, save the distances computed between matching geometries under a\n",
      " |          column of this name in the joined GeoDataFrame.\n",
      " |      exclusive : bool, optional, default False\n",
      " |          If True, the nearest geometries that are equal to the input geometry\n",
      " |          will not be returned, default False.\n",
      " |          Requires Shapely >= 2.0\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import geodatasets\n",
      " |      >>> groceries = geopandas.read_file(\n",
      " |      ...     geodatasets.get_path(\"geoda.groceries\")\n",
      " |      ... )\n",
      " |      >>> chicago = geopandas.read_file(\n",
      " |      ...     geodatasets.get_path(\"geoda.chicago_health\")\n",
      " |      ... ).to_crs(groceries.crs)\n",
      " |\n",
      " |      >>> chicago.head()  # doctest: +SKIP\n",
      " |          ComAreaID  ...                                           geometry\n",
      " |      0         35  ...  POLYGON ((-87.60914 41.84469, -87.60915 41.844...\n",
      " |      1         36  ...  POLYGON ((-87.59215 41.81693, -87.59231 41.816...\n",
      " |      2         37  ...  POLYGON ((-87.62880 41.80189, -87.62879 41.801...\n",
      " |      3         38  ...  POLYGON ((-87.60671 41.81681, -87.60670 41.816...\n",
      " |      4         39  ...  POLYGON ((-87.59215 41.81693, -87.59215 41.816...\n",
      " |      [5 rows x 87 columns]\n",
      " |\n",
      " |      >>> groceries.head()  # doctest: +SKIP\n",
      " |          OBJECTID     Ycoord  ...  Category                         geometry\n",
      " |      0        16  41.973266  ...       NaN  MULTIPOINT (-87.65661 41.97321)\n",
      " |      1        18  41.696367  ...       NaN  MULTIPOINT (-87.68136 41.69713)\n",
      " |      2        22  41.868634  ...       NaN  MULTIPOINT (-87.63918 41.86847)\n",
      " |      3        23  41.877590  ...       new  MULTIPOINT (-87.65495 41.87783)\n",
      " |      4        27  41.737696  ...       NaN  MULTIPOINT (-87.62715 41.73623)\n",
      " |      [5 rows x 8 columns]\n",
      " |\n",
      " |      >>> groceries_w_communities = groceries.sjoin_nearest(chicago)\n",
      " |      >>> groceries_w_communities[[\"Chain\", \"community\", \"geometry\"]].head(2)\n",
      " |                   Chain community                              geometry\n",
      " |      0   VIET HOA PLAZA    UPTOWN  MULTIPOINT (1168268.672 1933554.350)\n",
      " |      87      JEWEL OSCO    UPTOWN  MULTIPOINT (1168837.980 1929246.962)\n",
      " |\n",
      " |\n",
      " |      To include the distances:\n",
      " |\n",
      " |      >>> groceries_w_communities = groceries.sjoin_nearest(chicago, distance_col=\"distances\")\n",
      " |      >>> groceries_w_communities[[\"Chain\", \"community\", \"distances\"]].head(2)  # doctest: +SKIP\n",
      " |                   Chain community  distances\n",
      " |      0   VIET HOA PLAZA    UPTOWN        0.0\n",
      " |      87      JEWEL OSCO    UPTOWN        0.0\n",
      " |\n",
      " |      In the following example, we get multiple groceries for Uptown because all\n",
      " |      results are equidistant (in this case zero because they intersect).\n",
      " |      In fact, we get 4 results in total:\n",
      " |\n",
      " |      >>> chicago_w_groceries = groceries.sjoin_nearest(chicago, distance_col=\"distances\", how=\"right\")\n",
      " |      >>> uptown_results = chicago_w_groceries[chicago_w_groceries[\"community\"] == \"UPTOWN\"]\n",
      " |      >>> uptown_results[[\"Chain\", \"community\"]]  # doctest: +SKIP\n",
      " |                  Chain community\n",
      " |      30  VIET HOA PLAZA    UPTOWN\n",
      " |      30      JEWEL OSCO    UPTOWN\n",
      " |      30          TARGET    UPTOWN\n",
      " |      30       Mariano's    UPTOWN\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoDataFrame.sjoin : binary predicate joins\n",
      " |      sjoin_nearest : equivalent top-level function\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Since this join relies on distances, results will be inaccurate\n",
      " |      if your geometries are in a geographic CRS.\n",
      " |\n",
      " |      Every operation in GeoPandas is planar, i.e. the potential third\n",
      " |      dimension is not taken into account.\n",
      " |\n",
      " |  to_crs(self, crs=None, epsg=None, inplace=False)\n",
      " |      Transform geometries to a new coordinate reference system.\n",
      " |\n",
      " |      Transform all geometries in an active geometry column to a different coordinate\n",
      " |      reference system.  The ``crs`` attribute on the current GeoSeries must\n",
      " |      be set.  Either ``crs`` or ``epsg`` may be specified for output.\n",
      " |\n",
      " |      This method will transform all points in all objects. It has no notion\n",
      " |      of projecting entire geometries.  All segments joining points are\n",
      " |      assumed to be lines in the current projection, not geodesics. Objects\n",
      " |      crossing the dateline (or other projection boundary) will have\n",
      " |      undesirable behavior.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      crs : pyproj.CRS, optional if `epsg` is specified\n",
      " |          The value can be anything accepted by\n",
      " |          :meth:`pyproj.CRS.from_user_input() <pyproj.crs.CRS.from_user_input>`,\n",
      " |          such as an authority string (eg \"EPSG:4326\") or a WKT string.\n",
      " |      epsg : int, optional if `crs` is specified\n",
      " |          EPSG code specifying output projection.\n",
      " |      inplace : bool, optional, default: False\n",
      " |          Whether to return a new GeoDataFrame or do the transformation in\n",
      " |          place.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      GeoDataFrame\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Point\n",
      " |      >>> d = {'col1': ['name1', 'name2'], 'geometry': [Point(1, 2), Point(2, 1)]}\n",
      " |      >>> gdf = geopandas.GeoDataFrame(d, crs=4326)\n",
      " |      >>> gdf\n",
      " |          col1                 geometry\n",
      " |      0  name1  POINT (1.00000 2.00000)\n",
      " |      1  name2  POINT (2.00000 1.00000)\n",
      " |      >>> gdf.crs  # doctest: +SKIP\n",
      " |      <Geographic 2D CRS: EPSG:4326>\n",
      " |      Name: WGS 84\n",
      " |      Axis Info [ellipsoidal]:\n",
      " |      - Lat[north]: Geodetic latitude (degree)\n",
      " |      - Lon[east]: Geodetic longitude (degree)\n",
      " |      Area of Use:\n",
      " |      - name: World\n",
      " |      - bounds: (-180.0, -90.0, 180.0, 90.0)\n",
      " |      Datum: World Geodetic System 1984\n",
      " |      - Ellipsoid: WGS 84\n",
      " |      - Prime Meridian: Greenwich\n",
      " |\n",
      " |      >>> gdf = gdf.to_crs(3857)\n",
      " |      >>> gdf\n",
      " |          col1                       geometry\n",
      " |      0  name1  POINT (111319.491 222684.209)\n",
      " |      1  name2  POINT (222638.982 111325.143)\n",
      " |      >>> gdf.crs  # doctest: +SKIP\n",
      " |      <Projected CRS: EPSG:3857>\n",
      " |      Name: WGS 84 / Pseudo-Mercator\n",
      " |      Axis Info [cartesian]:\n",
      " |      - X[east]: Easting (metre)\n",
      " |      - Y[north]: Northing (metre)\n",
      " |      Area of Use:\n",
      " |      - name: World - 85°S to 85°N\n",
      " |      - bounds: (-180.0, -85.06, 180.0, 85.06)\n",
      " |      Coordinate Operation:\n",
      " |      - name: Popular Visualisation Pseudo-Mercator\n",
      " |      - method: Popular Visualisation Pseudo Mercator\n",
      " |      Datum: World Geodetic System 1984\n",
      " |      - Ellipsoid: WGS 84\n",
      " |      - Prime Meridian: Greenwich\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoDataFrame.set_crs : assign CRS without re-projection\n",
      " |\n",
      " |  to_feather(self, path, index=None, compression=None, schema_version=None, **kwargs)\n",
      " |      Write a GeoDataFrame to the Feather format.\n",
      " |\n",
      " |      Any geometry columns present are serialized to WKB format in the file.\n",
      " |\n",
      " |      Requires 'pyarrow' >= 0.17.\n",
      " |\n",
      " |      .. versionadded:: 0.8\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str, path object\n",
      " |      index : bool, default None\n",
      " |          If ``True``, always include the dataframe's index(es) as columns\n",
      " |          in the file output.\n",
      " |          If ``False``, the index(es) will not be written to the file.\n",
      " |          If ``None``, the index(ex) will be included as columns in the file\n",
      " |          output except `RangeIndex` which is stored as metadata only.\n",
      " |      compression : {'zstd', 'lz4', 'uncompressed'}, optional\n",
      " |          Name of the compression to use. Use ``\"uncompressed\"`` for no\n",
      " |          compression. By default uses LZ4 if available, otherwise uncompressed.\n",
      " |      schema_version : {'0.1.0', '0.4.0', '1.0.0', None}\n",
      " |          GeoParquet specification version; if not provided will default to\n",
      " |          latest supported version.\n",
      " |      kwargs\n",
      " |          Additional keyword arguments passed to to\n",
      " |          :func:`pyarrow.feather.write_feather`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> gdf.to_feather('data.feather')  # doctest: +SKIP\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoDataFrame.to_parquet : write GeoDataFrame to parquet\n",
      " |      GeoDataFrame.to_file : write GeoDataFrame to file\n",
      " |\n",
      " |  to_file(self, filename, driver=None, schema=None, index=None, **kwargs)\n",
      " |      Write the ``GeoDataFrame`` to a file.\n",
      " |\n",
      " |      By default, an ESRI shapefile is written, but any OGR data source\n",
      " |      supported by Fiona can be written. A dictionary of supported OGR\n",
      " |      providers is available via:\n",
      " |\n",
      " |      >>> import fiona\n",
      " |      >>> fiona.supported_drivers  # doctest: +SKIP\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      filename : string\n",
      " |          File path or file handle to write to. The path may specify a\n",
      " |          GDAL VSI scheme.\n",
      " |      driver : string, default None\n",
      " |          The OGR format driver used to write the vector file.\n",
      " |          If not specified, it attempts to infer it from the file extension.\n",
      " |          If no extension is specified, it saves ESRI Shapefile to a folder.\n",
      " |      schema : dict, default None\n",
      " |          If specified, the schema dictionary is passed to Fiona to\n",
      " |          better control how the file is written. If None, GeoPandas\n",
      " |          will determine the schema based on each column's dtype.\n",
      " |          Not supported for the \"pyogrio\" engine.\n",
      " |      index : bool, default None\n",
      " |          If True, write index into one or more columns (for MultiIndex).\n",
      " |          Default None writes the index into one or more columns only if\n",
      " |          the index is named, is a MultiIndex, or has a non-integer data\n",
      " |          type. If False, no index is written.\n",
      " |\n",
      " |          .. versionadded:: 0.7\n",
      " |              Previously the index was not written.\n",
      " |      mode : string, default 'w'\n",
      " |          The write mode, 'w' to overwrite the existing file and 'a' to append.\n",
      " |          Not all drivers support appending. The drivers that support appending\n",
      " |          are listed in fiona.supported_drivers or\n",
      " |          https://github.com/Toblerity/Fiona/blob/master/fiona/drvsupport.py\n",
      " |      crs : pyproj.CRS, default None\n",
      " |          If specified, the CRS is passed to Fiona to\n",
      " |          better control how the file is written. If None, GeoPandas\n",
      " |          will determine the crs based on crs df attribute.\n",
      " |          The value can be anything accepted\n",
      " |          by :meth:`pyproj.CRS.from_user_input() <pyproj.crs.CRS.from_user_input>`,\n",
      " |          such as an authority string (eg \"EPSG:4326\") or a WKT string.\n",
      " |      engine : str, \"fiona\" or \"pyogrio\"\n",
      " |          The underlying library that is used to write the file. Currently, the\n",
      " |          supported options are \"fiona\" and \"pyogrio\". Defaults to \"fiona\" if\n",
      " |          installed, otherwise tries \"pyogrio\".\n",
      " |      **kwargs :\n",
      " |          Keyword args to be passed to the engine, and can be used to write\n",
      " |          to multi-layer data, store data within archives (zip files), etc.\n",
      " |          In case of the \"fiona\" engine, the keyword arguments are passed to\n",
      " |          fiona.open`. For more information on possible keywords, type:\n",
      " |          ``import fiona; help(fiona.open)``. In case of the \"pyogrio\" engine,\n",
      " |          the keyword arguments are passed to `pyogrio.write_dataframe`.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      The format drivers will attempt to detect the encoding of your data, but\n",
      " |      may fail. In this case, the proper encoding can be specified explicitly\n",
      " |      by using the encoding keyword parameter, e.g. ``encoding='utf-8'``.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      GeoSeries.to_file\n",
      " |      GeoDataFrame.to_postgis : write GeoDataFrame to PostGIS database\n",
      " |      GeoDataFrame.to_parquet : write GeoDataFrame to parquet\n",
      " |      GeoDataFrame.to_feather : write GeoDataFrame to feather\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> gdf.to_file('dataframe.shp')  # doctest: +SKIP\n",
      " |\n",
      " |      >>> gdf.to_file('dataframe.gpkg', driver='GPKG', layer='name')  # doctest: +SKIP\n",
      " |\n",
      " |      >>> gdf.to_file('dataframe.geojson', driver='GeoJSON')  # doctest: +SKIP\n",
      " |\n",
      " |      With selected drivers you can also append to a file with `mode=\"a\"`:\n",
      " |\n",
      " |      >>> gdf.to_file('dataframe.shp', mode=\"a\")  # doctest: +SKIP\n",
      " |\n",
      " |      Using the engine-specific keyword arguments it is possible to e.g. create a\n",
      " |      spatialite file with a custom layer name:\n",
      " |\n",
      " |      >>> gdf.to_file(\n",
      " |      ...     'dataframe.sqlite', driver='SQLite', spatialite=True, layer='test'\n",
      " |      ... )  # doctest: +SKIP\n",
      " |\n",
      " |  to_json(self, na='null', show_bbox=False, drop_id=False, to_wgs84=False, **kwargs)\n",
      " |      Returns a GeoJSON representation of the ``GeoDataFrame`` as a string.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      na : {'null', 'drop', 'keep'}, default 'null'\n",
      " |          Indicates how to output missing (NaN) values in the GeoDataFrame.\n",
      " |          See below.\n",
      " |      show_bbox : bool, optional, default: False\n",
      " |          Include bbox (bounds) in the geojson\n",
      " |      drop_id : bool, default: False\n",
      " |          Whether to retain the index of the GeoDataFrame as the id property\n",
      " |          in the generated GeoJSON. Default is False, but may want True\n",
      " |          if the index is just arbitrary row numbers.\n",
      " |      to_wgs84: bool, optional, default: False\n",
      " |          If the CRS is set on the active geometry column it is exported as\n",
      " |          WGS84 (EPSG:4326) to meet the `2016 GeoJSON specification\n",
      " |          <https://tools.ietf.org/html/rfc7946>`_.\n",
      " |          Set to True to force re-projection and set to False to ignore CRS. False by\n",
      " |          default.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      The remaining *kwargs* are passed to json.dumps().\n",
      " |\n",
      " |      Missing (NaN) values in the GeoDataFrame can be represented as follows:\n",
      " |\n",
      " |      - ``null``: output the missing entries as JSON null.\n",
      " |      - ``drop``: remove the property from the feature. This applies to each\n",
      " |        feature individually so that features may have different properties.\n",
      " |      - ``keep``: output the missing entries as NaN.\n",
      " |\n",
      " |      If the GeoDataFrame has a defined CRS, its definition will be included\n",
      " |      in the output unless it is equal to WGS84 (default GeoJSON CRS) or not\n",
      " |      possible to represent in the URN OGC format, or unless ``to_wgs84=True``\n",
      " |      is specified.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely.geometry import Point\n",
      " |      >>> d = {'col1': ['name1', 'name2'], 'geometry': [Point(1, 2), Point(2, 1)]}\n",
      " |      >>> gdf = geopandas.GeoDataFrame(d, crs=\"EPSG:3857\")\n",
      " |      >>> gdf\n",
      " |          col1             geometry\n",
      " |      0  name1  POINT (1.000 2.000)\n",
      " |      1  name2  POINT (2.000 1.000)\n",
      " |\n",
      " |      >>> gdf.to_json()\n",
      " |      '{\"type\": \"FeatureCollection\", \"features\": [{\"id\": \"0\", \"type\": \"Feature\", \"properties\": {\"col1\": \"name1\"}, \"geometry\": {\"type\": \"Point\", \"coordinates\": [1.0, 2.0]}}, {\"id\": \"1\", \"type\": \"Feature\", \"properties\": {\"col1\": \"name2\"}, \"geometry\": {\"type\": \"Point\", \"coordinates\": [2.0, 1.0]}}], \"crs\": {\"type\": \"name\", \"properties\": {\"name\": \"urn:ogc:def:crs:EPSG::3857\"}}}'\n",
      " |\n",
      " |      Alternatively, you can write GeoJSON to file:\n",
      " |\n",
      " |      >>> gdf.to_file(path, driver=\"GeoJSON\")  # doctest: +SKIP\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoDataFrame.to_file : write GeoDataFrame to file\n",
      " |\n",
      " |  to_parquet(self, path, index=None, compression='snappy', schema_version=None, **kwargs)\n",
      " |      Write a GeoDataFrame to the Parquet format.\n",
      " |\n",
      " |      Any geometry columns present are serialized to WKB format in the file.\n",
      " |\n",
      " |      Requires 'pyarrow'.\n",
      " |\n",
      " |      .. versionadded:: 0.8\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str, path object\n",
      " |      index : bool, default None\n",
      " |          If ``True``, always include the dataframe's index(es) as columns\n",
      " |          in the file output.\n",
      " |          If ``False``, the index(es) will not be written to the file.\n",
      " |          If ``None``, the index(ex) will be included as columns in the file\n",
      " |          output except `RangeIndex` which is stored as metadata only.\n",
      " |      compression : {'snappy', 'gzip', 'brotli', None}, default 'snappy'\n",
      " |          Name of the compression to use. Use ``None`` for no compression.\n",
      " |      schema_version : {'0.1.0', '0.4.0', '1.0.0', None}\n",
      " |          GeoParquet specification version; if not provided will default to\n",
      " |          latest supported version.\n",
      " |      kwargs\n",
      " |          Additional keyword arguments passed to :func:`pyarrow.parquet.write_table`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> gdf.to_parquet('data.parquet')  # doctest: +SKIP\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoDataFrame.to_feather : write GeoDataFrame to feather\n",
      " |      GeoDataFrame.to_file : write GeoDataFrame to file\n",
      " |\n",
      " |  to_postgis(self, name, con, schema=None, if_exists='fail', index=False, index_label=None, chunksize=None, dtype=None)\n",
      " |      Upload GeoDataFrame into PostGIS database.\n",
      " |\n",
      " |      This method requires SQLAlchemy and GeoAlchemy2, and a PostgreSQL\n",
      " |      Python driver (e.g. psycopg2) to be installed.\n",
      " |\n",
      " |      It is also possible to use :meth:`~GeoDataFrame.to_file` to write to a database.\n",
      " |      Especially for file geodatabases like GeoPackage or SpatiaLite this can be\n",
      " |      easier.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          Name of the target table.\n",
      " |      con : sqlalchemy.engine.Connection or sqlalchemy.engine.Engine\n",
      " |          Active connection to the PostGIS database.\n",
      " |      if_exists : {'fail', 'replace', 'append'}, default 'fail'\n",
      " |          How to behave if the table already exists:\n",
      " |\n",
      " |          - fail: Raise a ValueError.\n",
      " |          - replace: Drop the table before inserting new values.\n",
      " |          - append: Insert new values to the existing table.\n",
      " |      schema : string, optional\n",
      " |          Specify the schema. If None, use default schema: 'public'.\n",
      " |      index : bool, default False\n",
      " |          Write DataFrame index as a column.\n",
      " |          Uses *index_label* as the column name in the table.\n",
      " |      index_label : string or sequence, default None\n",
      " |          Column label for index column(s).\n",
      " |          If None is given (default) and index is True,\n",
      " |          then the index names are used.\n",
      " |      chunksize : int, optional\n",
      " |          Rows will be written in batches of this size at a time.\n",
      " |          By default, all rows will be written at once.\n",
      " |      dtype : dict of column name to SQL type, default None\n",
      " |          Specifying the datatype for columns.\n",
      " |          The keys should be the column names and the values\n",
      " |          should be the SQLAlchemy types.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from sqlalchemy import create_engine\n",
      " |      >>> engine = create_engine(\"postgresql://myusername:mypassword@myhost:5432/mydatabase\")  # doctest: +SKIP\n",
      " |      >>> gdf.to_postgis(\"my_table\", engine)  # doctest: +SKIP\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoDataFrame.to_file : write GeoDataFrame to file\n",
      " |      read_postgis : read PostGIS database to GeoDataFrame\n",
      " |\n",
      " |  to_wkb(self, hex=False, **kwargs)\n",
      " |      Encode all geometry columns in the GeoDataFrame to WKB.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      hex : bool\n",
      " |          If true, export the WKB as a hexadecimal string.\n",
      " |          The default is to return a binary bytes object.\n",
      " |      kwargs\n",
      " |          Additional keyword args will be passed to\n",
      " |          :func:`shapely.to_wkb` if shapely >= 2 is installed or\n",
      " |          :func:`pygeos.to_wkb` if pygeos is installed.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          geometry columns are encoded to WKB\n",
      " |\n",
      " |  to_wkt(self, **kwargs)\n",
      " |      Encode all geometry columns in the GeoDataFrame to WKT.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      kwargs\n",
      " |          Keyword args will be passed to :func:`pygeos.to_wkt`\n",
      " |          if pygeos is installed.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          geometry columns are encoded to WKT\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |\n",
      " |  from_dict(data, geometry=None, crs=None, **kwargs)\n",
      " |      Construct GeoDataFrame from dict of array-like or dicts by\n",
      " |      overriding DataFrame.from_dict method with geometry and crs\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : dict\n",
      " |          Of the form {field : array-like} or {field : dict}.\n",
      " |      geometry : str or array (optional)\n",
      " |          If str, column to use as geometry. If array, will be set as 'geometry'\n",
      " |          column on GeoDataFrame.\n",
      " |      crs : str or dict (optional)\n",
      " |          Coordinate reference system to set on the resulting frame.\n",
      " |      kwargs : key-word arguments\n",
      " |          These arguments are passed to DataFrame.from_dict\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      GeoDataFrame\n",
      " |\n",
      " |  from_features(features, crs=None, columns=None)\n",
      " |      Alternate constructor to create GeoDataFrame from an iterable of\n",
      " |      features or a feature collection.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      features\n",
      " |          - Iterable of features, where each element must be a feature\n",
      " |            dictionary or implement the __geo_interface__.\n",
      " |          - Feature collection, where the 'features' key contains an\n",
      " |            iterable of features.\n",
      " |          - Object holding a feature collection that implements the\n",
      " |            ``__geo_interface__``.\n",
      " |      crs : str or dict (optional)\n",
      " |          Coordinate reference system to set on the resulting frame.\n",
      " |      columns : list of column names, optional\n",
      " |          Optionally specify the column names to include in the output frame.\n",
      " |          This does not overwrite the property names of the input, but can\n",
      " |          ensure a consistent output format.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      GeoDataFrame\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      For more information about the ``__geo_interface__``, see\n",
      " |      https://gist.github.com/sgillies/2217756\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> feature_coll = {\n",
      " |      ...     \"type\": \"FeatureCollection\",\n",
      " |      ...     \"features\": [\n",
      " |      ...         {\n",
      " |      ...             \"id\": \"0\",\n",
      " |      ...             \"type\": \"Feature\",\n",
      " |      ...             \"properties\": {\"col1\": \"name1\"},\n",
      " |      ...             \"geometry\": {\"type\": \"Point\", \"coordinates\": (1.0, 2.0)},\n",
      " |      ...             \"bbox\": (1.0, 2.0, 1.0, 2.0),\n",
      " |      ...         },\n",
      " |      ...         {\n",
      " |      ...             \"id\": \"1\",\n",
      " |      ...             \"type\": \"Feature\",\n",
      " |      ...             \"properties\": {\"col1\": \"name2\"},\n",
      " |      ...             \"geometry\": {\"type\": \"Point\", \"coordinates\": (2.0, 1.0)},\n",
      " |      ...             \"bbox\": (2.0, 1.0, 2.0, 1.0),\n",
      " |      ...         },\n",
      " |      ...     ],\n",
      " |      ...     \"bbox\": (1.0, 1.0, 2.0, 2.0),\n",
      " |      ... }\n",
      " |      >>> df = geopandas.GeoDataFrame.from_features(feature_coll)\n",
      " |      >>> df\n",
      " |                        geometry   col1\n",
      " |      0  POINT (1.00000 2.00000)  name1\n",
      " |      1  POINT (2.00000 1.00000)  name2\n",
      " |\n",
      " |  from_file(filename, **kwargs)\n",
      " |      Alternate constructor to create a ``GeoDataFrame`` from a file.\n",
      " |\n",
      " |      It is recommended to use :func:`geopandas.read_file` instead.\n",
      " |\n",
      " |      Can load a ``GeoDataFrame`` from a file in any format recognized by\n",
      " |      `fiona`. See http://fiona.readthedocs.io/en/latest/manual.html for details.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      filename : str\n",
      " |          File path or file handle to read from. Depending on which kwargs\n",
      " |          are included, the content of filename may vary. See\n",
      " |          http://fiona.readthedocs.io/en/latest/README.html#usage for usage details.\n",
      " |      kwargs : key-word arguments\n",
      " |          These arguments are passed to fiona.open, and can be used to\n",
      " |          access multi-layer data, data stored within archives (zip files),\n",
      " |          etc.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import geodatasets\n",
      " |      >>> path = geodatasets.get_path('nybb')\n",
      " |      >>> gdf = geopandas.GeoDataFrame.from_file(path)\n",
      " |      >>> gdf  # doctest: +SKIP\n",
      " |         BoroCode       BoroName     Shape_Leng    Shape_Area                                           geometry\n",
      " |      0         5  Staten Island  330470.010332  1.623820e+09  MULTIPOLYGON (((970217.022 145643.332, 970227....\n",
      " |      1         4         Queens  896344.047763  3.045213e+09  MULTIPOLYGON (((1029606.077 156073.814, 102957...\n",
      " |      2         3       Brooklyn  741080.523166  1.937479e+09  MULTIPOLYGON (((1021176.479 151374.797, 102100...\n",
      " |      3         1      Manhattan  359299.096471  6.364715e+08  MULTIPOLYGON (((981219.056 188655.316, 980940....\n",
      " |      4         2          Bronx  464392.991824  1.186925e+09  MULTIPOLYGON (((1012821.806 229228.265, 101278...\n",
      " |\n",
      " |      The recommended method of reading files is :func:`geopandas.read_file`:\n",
      " |\n",
      " |      >>> gdf = geopandas.read_file(path)\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      read_file : read file to GeoDataFame\n",
      " |      GeoDataFrame.to_file : write GeoDataFrame to file\n",
      " |\n",
      " |  from_postgis(sql, con, geom_col='geom', crs=None, index_col=None, coerce_float=True, parse_dates=None, params=None, chunksize=None)\n",
      " |      Alternate constructor to create a ``GeoDataFrame`` from a sql query\n",
      " |      containing a geometry column in WKB representation.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sql : string\n",
      " |      con : sqlalchemy.engine.Connection or sqlalchemy.engine.Engine\n",
      " |      geom_col : string, default 'geom'\n",
      " |          column name to convert to shapely geometries\n",
      " |      crs : optional\n",
      " |          Coordinate reference system to use for the returned GeoDataFrame\n",
      " |      index_col : string or list of strings, optional, default: None\n",
      " |          Column(s) to set as index(MultiIndex)\n",
      " |      coerce_float : boolean, default True\n",
      " |          Attempt to convert values of non-string, non-numeric objects (like\n",
      " |          decimal.Decimal) to floating point, useful for SQL result sets\n",
      " |      parse_dates : list or dict, default None\n",
      " |          - List of column names to parse as dates.\n",
      " |          - Dict of ``{column_name: format string}`` where format string is\n",
      " |            strftime compatible in case of parsing string times, or is one of\n",
      " |            (D, s, ns, ms, us) in case of parsing integer timestamps.\n",
      " |          - Dict of ``{column_name: arg dict}``, where the arg dict\n",
      " |            corresponds to the keyword arguments of\n",
      " |            :func:`pandas.to_datetime`. Especially useful with databases\n",
      " |            without native Datetime support, such as SQLite.\n",
      " |      params : list, tuple or dict, optional, default None\n",
      " |          List of parameters to pass to execute method.\n",
      " |      chunksize : int, default None\n",
      " |          If specified, return an iterator where chunksize is the number\n",
      " |          of rows to include in each chunk.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      PostGIS\n",
      " |\n",
      " |      >>> from sqlalchemy import create_engine  # doctest: +SKIP\n",
      " |      >>> db_connection_url = \"postgresql://myusername:mypassword@myhost:5432/mydb\"\n",
      " |      >>> con = create_engine(db_connection_url)  # doctest: +SKIP\n",
      " |      >>> sql = \"SELECT geom, highway FROM roads\"\n",
      " |      >>> df = geopandas.GeoDataFrame.from_postgis(sql, con)  # doctest: +SKIP\n",
      " |\n",
      " |      SpatiaLite\n",
      " |\n",
      " |      >>> sql = \"SELECT ST_Binary(geom) AS geom, highway FROM roads\"\n",
      " |      >>> df = geopandas.GeoDataFrame.from_postgis(sql, con)  # doctest: +SKIP\n",
      " |\n",
      " |      The recommended method of reading from PostGIS is\n",
      " |      :func:`geopandas.read_postgis`:\n",
      " |\n",
      " |      >>> df = geopandas.read_postgis(sql, con)  # doctest: +SKIP\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      geopandas.read_postgis : read PostGIS database to GeoDataFrame\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  __geo_interface__\n",
      " |      Returns a ``GeoDataFrame`` as a python feature collection.\n",
      " |\n",
      " |      Implements the `geo_interface`. The returned python data structure\n",
      " |      represents the ``GeoDataFrame`` as a GeoJSON-like\n",
      " |      ``FeatureCollection``.\n",
      " |\n",
      " |      This differs from `_to_geo()` only in that it is a property with\n",
      " |      default args instead of a method.\n",
      " |\n",
      " |      CRS of the dataframe is not passed on to the output, unlike\n",
      " |      :meth:`~GeoDataFrame.to_json()`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely.geometry import Point\n",
      " |      >>> d = {'col1': ['name1', 'name2'], 'geometry': [Point(1, 2), Point(2, 1)]}\n",
      " |      >>> gdf = geopandas.GeoDataFrame(d, crs=\"EPSG:4326\")\n",
      " |      >>> gdf\n",
      " |          col1                 geometry\n",
      " |      0  name1  POINT (1.00000 2.00000)\n",
      " |      1  name2  POINT (2.00000 1.00000)\n",
      " |\n",
      " |      >>> gdf.__geo_interface__\n",
      " |      {'type': 'FeatureCollection', 'features': [{'id': '0', 'type': 'Feature', 'properties': {'col1': 'name1'}, 'geometry': {'type': 'Point', 'coordinates': (1.0, 2.0)}, 'bbox': (1.0, 2.0, 1.0, 2.0)}, {'id': '1', 'type': 'Feature', 'properties': {'col1': 'name2'}, 'geometry': {'type': 'Point', 'coordinates': (2.0, 1.0)}, 'bbox': (2.0, 1.0, 2.0, 1.0)}], 'bbox': (1.0, 1.0, 2.0, 2.0)}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  crs\n",
      " |      The Coordinate Reference System (CRS) represented as a ``pyproj.CRS``\n",
      " |      object.\n",
      " |\n",
      " |      Returns None if the CRS is not set, and to set the value it\n",
      " |      :getter: Returns a ``pyproj.CRS`` or None. When setting, the value\n",
      " |      can be anything accepted by\n",
      " |      :meth:`pyproj.CRS.from_user_input() <pyproj.crs.CRS.from_user_input>`,\n",
      " |      such as an authority string (eg \"EPSG:4326\") or a WKT string.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> gdf.crs  # doctest: +SKIP\n",
      " |      <Geographic 2D CRS: EPSG:4326>\n",
      " |      Name: WGS 84\n",
      " |      Axis Info [ellipsoidal]:\n",
      " |      - Lat[north]: Geodetic latitude (degree)\n",
      " |      - Lon[east]: Geodetic longitude (degree)\n",
      " |      Area of Use:\n",
      " |      - name: World\n",
      " |      - bounds: (-180.0, -90.0, 180.0, 90.0)\n",
      " |      Datum: World Geodetic System 1984\n",
      " |      - Ellipsoid: WGS 84\n",
      " |      - Prime Meridian: Greenwich\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoDataFrame.set_crs : assign CRS\n",
      " |      GeoDataFrame.to_crs : re-project to another CRS\n",
      " |\n",
      " |  geometry\n",
      " |      Geometry data for GeoDataFrame\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {}\n",
      " |\n",
      " |  plot = <class 'geopandas.plotting.GeoplotAccessor'>\n",
      " |      Plot a GeoDataFrame.\n",
      " |\n",
      " |      Generate a plot of a GeoDataFrame with matplotlib.  If a\n",
      " |      column is specified, the plot coloring will be based on values\n",
      " |      in that column.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      column : str, np.array, pd.Series (default None)\n",
      " |          The name of the dataframe column, np.array, or pd.Series to be plotted.\n",
      " |          If np.array or pd.Series are used then it must have same length as\n",
      " |          dataframe. Values are used to color the plot. Ignored if `color` is\n",
      " |          also set.\n",
      " |      kind: str\n",
      " |          The kind of plots to produce. The default is to create a map (\"geo\").\n",
      " |          Other supported kinds of plots from pandas:\n",
      " |\n",
      " |          - 'line' : line plot\n",
      " |          - 'bar' : vertical bar plot\n",
      " |          - 'barh' : horizontal bar plot\n",
      " |          - 'hist' : histogram\n",
      " |          - 'box' : BoxPlot\n",
      " |          - 'kde' : Kernel Density Estimation plot\n",
      " |          - 'density' : same as 'kde'\n",
      " |          - 'area' : area plot\n",
      " |          - 'pie' : pie plot\n",
      " |          - 'scatter' : scatter plot\n",
      " |          - 'hexbin' : hexbin plot.\n",
      " |      cmap : str (default None)\n",
      " |          The name of a colormap recognized by matplotlib.\n",
      " |      color : str, np.array, pd.Series (default None)\n",
      " |          If specified, all objects will be colored uniformly.\n",
      " |      ax : matplotlib.pyplot.Artist (default None)\n",
      " |          axes on which to draw the plot\n",
      " |      cax : matplotlib.pyplot Artist (default None)\n",
      " |          axes on which to draw the legend in case of color map.\n",
      " |      categorical : bool (default False)\n",
      " |          If False, cmap will reflect numerical values of the\n",
      " |          column being plotted.  For non-numerical columns, this\n",
      " |          will be set to True.\n",
      " |      legend : bool (default False)\n",
      " |          Plot a legend. Ignored if no `column` is given, or if `color` is given.\n",
      " |      scheme : str (default None)\n",
      " |          Name of a choropleth classification scheme (requires mapclassify).\n",
      " |          A mapclassify.MapClassifier object will be used\n",
      " |          under the hood. Supported are all schemes provided by mapclassify (e.g.\n",
      " |          'BoxPlot', 'EqualInterval', 'FisherJenks', 'FisherJenksSampled',\n",
      " |          'HeadTailBreaks', 'JenksCaspall', 'JenksCaspallForced',\n",
      " |          'JenksCaspallSampled', 'MaxP', 'MaximumBreaks',\n",
      " |          'NaturalBreaks', 'Quantiles', 'Percentiles', 'StdMean',\n",
      " |          'UserDefined'). Arguments can be passed in classification_kwds.\n",
      " |      k : int (default 5)\n",
      " |          Number of classes (ignored if scheme is None)\n",
      " |      vmin : None or float (default None)\n",
      " |          Minimum value of cmap. If None, the minimum data value\n",
      " |          in the column to be plotted is used.\n",
      " |      vmax : None or float (default None)\n",
      " |          Maximum value of cmap. If None, the maximum data value\n",
      " |          in the column to be plotted is used.\n",
      " |      markersize : str or float or sequence (default None)\n",
      " |          Only applies to point geometries within a frame.\n",
      " |          If a str, will use the values in the column of the frame specified\n",
      " |          by markersize to set the size of markers. Otherwise can be a value\n",
      " |          to apply to all points, or a sequence of the same length as the\n",
      " |          number of points.\n",
      " |      figsize : tuple of integers (default None)\n",
      " |          Size of the resulting matplotlib.figure.Figure. If the argument\n",
      " |          axes is given explicitly, figsize is ignored.\n",
      " |      legend_kwds : dict (default None)\n",
      " |          Keyword arguments to pass to :func:`matplotlib.pyplot.legend` or\n",
      " |          :func:`matplotlib.pyplot.colorbar`.\n",
      " |          Additional accepted keywords when `scheme` is specified:\n",
      " |\n",
      " |          fmt : string\n",
      " |              A formatting specification for the bin edges of the classes in the\n",
      " |              legend. For example, to have no decimals: ``{\"fmt\": \"{:.0f}\"}``.\n",
      " |          labels : list-like\n",
      " |              A list of legend labels to override the auto-generated labels.\n",
      " |              Needs to have the same number of elements as the number of\n",
      " |              classes (`k`).\n",
      " |          interval : boolean (default False)\n",
      " |              An option to control brackets from mapclassify legend.\n",
      " |              If True, open/closed interval brackets are shown in the legend.\n",
      " |      categories : list-like\n",
      " |          Ordered list-like object of categories to be used for categorical plot.\n",
      " |      classification_kwds : dict (default None)\n",
      " |          Keyword arguments to pass to mapclassify\n",
      " |      missing_kwds : dict (default None)\n",
      " |          Keyword arguments specifying color options (as style_kwds)\n",
      " |          to be passed on to geometries with missing values in addition to\n",
      " |          or overwriting other style kwds. If None, geometries with missing\n",
      " |          values are not plotted.\n",
      " |      aspect : 'auto', 'equal', None or float (default 'auto')\n",
      " |          Set aspect of axis. If 'auto', the default aspect for map plots is 'equal'; if\n",
      " |          however data are not projected (coordinates are long/lat), the aspect is by\n",
      " |          default set to 1/cos(df_y * pi/180) with df_y the y coordinate of the middle of\n",
      " |          the GeoDataFrame (the mean of the y range of bounding box) so that a long/lat\n",
      " |          square appears square in the middle of the plot. This implies an\n",
      " |          Equirectangular projection. If None, the aspect of `ax` won't be changed. It can\n",
      " |          also be set manually (float) as the ratio of y-unit to x-unit.\n",
      " |\n",
      " |      **style_kwds : dict\n",
      " |          Style options to be passed on to the actual plot function, such\n",
      " |          as ``edgecolor``, ``facecolor``, ``linewidth``, ``markersize``,\n",
      " |          ``alpha``.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      ax : matplotlib axes instance\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import geodatasets\n",
      " |      >>> df = geopandas.read_file(geodatasets.get_path(\"nybb\"))\n",
      " |      >>> df.head()  # doctest: +SKIP\n",
      " |         BoroCode  ...                                           geometry\n",
      " |      0         5  ...  MULTIPOLYGON (((970217.022 145643.332, 970227....\n",
      " |      1         4  ...  MULTIPOLYGON (((1029606.077 156073.814, 102957...\n",
      " |      2         3  ...  MULTIPOLYGON (((1021176.479 151374.797, 102100...\n",
      " |      3         1  ...  MULTIPOLYGON (((981219.056 188655.316, 980940....\n",
      " |      4         2  ...  MULTIPOLYGON (((1012821.806 229228.265, 101278...\n",
      " |\n",
      " |      >>> df.plot(\"BoroName\", cmap=\"Set1\")  # doctest: +SKIP\n",
      " |\n",
      " |      See the User Guide page :doc:`../../user_guide/mapping` for details.\n",
      " |\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from geopandas.base.GeoPandasBase:\n",
      " |\n",
      " |  affine_transform(self, matrix)\n",
      " |      Return a ``GeoSeries`` with translated geometries.\n",
      " |\n",
      " |      See http://shapely.readthedocs.io/en/stable/manual.html#shapely.affinity.affine_transform\n",
      " |      for details.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      matrix: List or tuple\n",
      " |          6 or 12 items for 2D or 3D transformations respectively.\n",
      " |\n",
      " |          For 2D affine transformations,\n",
      " |          the 6 parameter matrix is ``[a, b, d, e, xoff, yoff]``\n",
      " |\n",
      " |          For 3D affine transformations,\n",
      " |          the 12 parameter matrix is ``[a, b, c, d, e, f, g, h, i, xoff, yoff, zoff]``\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Point, LineString, Polygon\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Point(1, 1),\n",
      " |      ...         LineString([(1, -1), (1, 0)]),\n",
      " |      ...         Polygon([(3, -1), (4, 0), (3, 1)]),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0                              POINT (1.00000 1.00000)\n",
      " |      1       LINESTRING (1.00000 -1.00000, 1.00000 0.00000)\n",
      " |      2    POLYGON ((3.00000 -1.00000, 4.00000 0.00000, 3...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.affine_transform([2, 3, 2, 4, 5, 2])\n",
      " |      0                             POINT (10.00000 8.00000)\n",
      " |      1        LINESTRING (4.00000 0.00000, 7.00000 4.00000)\n",
      " |      2    POLYGON ((8.00000 4.00000, 13.00000 10.00000, ...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |  buffer(self, distance, resolution=16, **kwargs)\n",
      " |      Returns a ``GeoSeries`` of geometries representing all points within\n",
      " |      a given ``distance`` of each geometric object.\n",
      " |\n",
      " |      See http://shapely.readthedocs.io/en/latest/manual.html#object.buffer\n",
      " |      for details.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      distance : float, np.array, pd.Series\n",
      " |          The radius of the buffer. If np.array or pd.Series are used\n",
      " |          then it must have same length as the GeoSeries.\n",
      " |      resolution : int (optional, default 16)\n",
      " |          The resolution of the buffer around each vertex.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Point, LineString, Polygon\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Point(0, 0),\n",
      " |      ...         LineString([(1, -1), (1, 0), (2, 0), (2, 1)]),\n",
      " |      ...         Polygon([(3, -1), (4, 0), (3, 1)]),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0                              POINT (0.00000 0.00000)\n",
      " |      1    LINESTRING (1.00000 -1.00000, 1.00000 0.00000,...\n",
      " |      2    POLYGON ((3.00000 -1.00000, 4.00000 0.00000, 3...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.buffer(0.2)\n",
      " |      0    POLYGON ((0.20000 0.00000, 0.19904 -0.01960, 0...\n",
      " |      1    POLYGON ((0.80000 0.00000, 0.80096 0.01960, 0....\n",
      " |      2    POLYGON ((2.80000 -1.00000, 2.80000 1.00000, 2...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      ``**kwargs`` accept further specification as ``join_style`` and ``cap_style``.\n",
      " |      See the following illustration of different options.\n",
      " |\n",
      " |      .. plot:: _static/code/buffer.py\n",
      " |\n",
      " |  clip_by_rect(self, xmin, ymin, xmax, ymax)\n",
      " |      Returns a ``GeoSeries`` of the portions of geometry within the given\n",
      " |      rectangle.\n",
      " |\n",
      " |      Note that the results are not exactly equal to\n",
      " |      :meth:`~GeoSeries.intersection()`. E.g. in edge cases,\n",
      " |      :meth:`~GeoSeries.clip_by_rect()` will not return a point just touching the\n",
      " |      rectangle. Check the examples section below for some of these exceptions.\n",
      " |\n",
      " |      The geometry is clipped in a fast but possibly dirty way. The output is not\n",
      " |      guaranteed to be valid. No exceptions will be raised for topological errors.\n",
      " |\n",
      " |      Note: empty geometries or geometries that do not overlap with the specified\n",
      " |      bounds will result in ``GEOMETRYCOLLECTION EMPTY``.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      xmin: float\n",
      " |          Minimum x value of the rectangle\n",
      " |      ymin: float\n",
      " |          Minimum y value of the rectangle\n",
      " |      xmax: float\n",
      " |          Maximum x value of the rectangle\n",
      " |      ymax: float\n",
      " |          Maximum y value of the rectangle\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      GeoSeries\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         LineString([(0, 0), (2, 2)]),\n",
      " |      ...         LineString([(2, 0), (0, 2)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ...     crs=3857,\n",
      " |      ... )\n",
      " |      >>> bounds = (0, 0, 1, 1)\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.000 0.000, 2.000 2.000, 0.000 2.00...\n",
      " |      1    POLYGON ((0.000 0.000, 2.000 2.000, 0.000 2.00...\n",
      " |      2                LINESTRING (0.000 0.000, 2.000 2.000)\n",
      " |      3                LINESTRING (2.000 0.000, 0.000 2.000)\n",
      " |      4                                  POINT (0.000 1.000)\n",
      " |      dtype: geometry\n",
      " |      >>> s.clip_by_rect(*bounds)\n",
      " |      0    POLYGON ((0.000 0.000, 0.000 1.000, 1.000 1.00...\n",
      " |      1    POLYGON ((0.000 0.000, 0.000 1.000, 1.000 1.00...\n",
      " |      2                LINESTRING (0.000 0.000, 1.000 1.000)\n",
      " |      3                             GEOMETRYCOLLECTION EMPTY\n",
      " |      4                             GEOMETRYCOLLECTION EMPTY\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.intersection\n",
      " |\n",
      " |  concave_hull(self, ratio=0.0, allow_holes=False)\n",
      " |      Returns a ``GeoSeries`` of geometries representing the concave hull\n",
      " |      of each geometry.\n",
      " |\n",
      " |      The concave hull of a geometry is the smallest concave `Polygon`\n",
      " |      containing all the points in each geometry, unless the number of points\n",
      " |      in the geometric object is less than three. For two points, the concave\n",
      " |      hull collapses to a `LineString`; for 1, a `Point`.\n",
      " |\n",
      " |      The hull is constructed by removing border triangles of the Delaunay\n",
      " |      Triangulation of the points as long as their \"size\" is larger than the\n",
      " |      maximum edge length ratio and optionally allowing holes. The edge length factor\n",
      " |      is a fraction of the length difference between the longest and shortest edges\n",
      " |      in the Delaunay Triangulation of the input points. For further information\n",
      " |      on the algorithm used, see\n",
      " |      https://libgeos.org/doxygen/classgeos_1_1algorithm_1_1hull_1_1ConcaveHull.html\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ratio : float, (optional, default 0.0)\n",
      " |          Number in the range [0, 1]. Higher numbers will include fewer vertices\n",
      " |          in the hull.\n",
      " |      allow_holes : bool, (optional, default False)\n",
      " |          If set to True, the concave hull may have holes.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point, MultiPoint\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         LineString([(0, 0), (1, 1), (1, 0)]),\n",
      " |      ...         MultiPoint([(0, 0), (1, 1), (0, 1), (1, 0), (0.5, 0.5)]),\n",
      " |      ...         MultiPoint([(0, 0), (1, 1)]),\n",
      " |      ...         Point(0, 0),\n",
      " |      ...     ],\n",
      " |      ...     crs=3857\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.000 0.000, 1.000 1.000, 0.000 1.00...\n",
      " |      1    LINESTRING (0.000 0.000, 1.000 1.000, 1.000 0....\n",
      " |      2    MULTIPOINT (0.000 0.000, 1.000 1.000, 0.000 1....\n",
      " |      3                MULTIPOINT (0.000 0.000, 1.000 1.000)\n",
      " |      4                                  POINT (0.000 0.000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.concave_hull()\n",
      " |      0    POLYGON ((0.000 1.000, 1.000 1.000, 0.000 0.00...\n",
      " |      1    POLYGON ((0.000 0.000, 1.000 1.000, 1.000 0.00...\n",
      " |      2    POLYGON ((0.500 0.500, 0.000 1.000, 1.000 1.00...\n",
      " |      3                LINESTRING (0.000 0.000, 1.000 1.000)\n",
      " |      4                                  POINT (0.000 0.000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.convex_hull : convex hull geometry\n",
      " |\n",
      " |  contains(self, other, align=True)\n",
      " |      Returns a ``Series`` of ``dtype('bool')`` with value ``True`` for\n",
      " |      each aligned geometry that contains `other`.\n",
      " |\n",
      " |      An object is said to contain `other` if at least one point of `other` lies in\n",
      " |      the interior and no points of `other` lie in the exterior of the object.\n",
      " |      (Therefore, any given polygon does not contain its own boundary – there is not\n",
      " |      any point that lies in the interior.)\n",
      " |      If either object is empty, this operation returns ``False``.\n",
      " |\n",
      " |      This is the inverse of :meth:`within` in the sense that the expression\n",
      " |      ``a.contains(b) == b.within(a)`` always evaluates to ``True``.\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : GeoSeries or geometric object\n",
      " |          The GeoSeries (elementwise) or geometric object to test if it\n",
      " |          is contained.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series (bool)\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         LineString([(0, 0), (0, 2)]),\n",
      " |      ...         LineString([(0, 0), (0, 1)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ...     index=range(0, 4),\n",
      " |      ... )\n",
      " |      >>> s2 = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         Polygon([(0, 0), (1, 2), (0, 2)]),\n",
      " |      ...         LineString([(0, 0), (0, 2)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ...     index=range(1, 5),\n",
      " |      ... )\n",
      " |\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 0....\n",
      " |      1        LINESTRING (0.00000 0.00000, 0.00000 2.00000)\n",
      " |      2        LINESTRING (0.00000 0.00000, 0.00000 1.00000)\n",
      " |      3                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s2\n",
      " |      1    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      2    POLYGON ((0.00000 0.00000, 1.00000 2.00000, 0....\n",
      " |      3        LINESTRING (0.00000 0.00000, 0.00000 2.00000)\n",
      " |      4                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can check if each geometry of GeoSeries contains a single\n",
      " |      geometry:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-03.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      >>> point = Point(0, 1)\n",
      " |      >>> s.contains(point)\n",
      " |      0    False\n",
      " |      1     True\n",
      " |      2    False\n",
      " |      3     True\n",
      " |      dtype: bool\n",
      " |\n",
      " |      We can also check two GeoSeries against each other, row by row.\n",
      " |      The GeoSeries above have different indices. We can either align both GeoSeries\n",
      " |      based on index values and compare elements with the same index using\n",
      " |      ``align=True`` or ignore index and compare elements based on their matching\n",
      " |      order using ``align=False``:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-02.svg\n",
      " |\n",
      " |      >>> s2.contains(s, align=True)\n",
      " |      0    False\n",
      " |      1    False\n",
      " |      2    False\n",
      " |      3     True\n",
      " |      4    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      >>> s2.contains(s, align=False)\n",
      " |      1     True\n",
      " |      2    False\n",
      " |      3     True\n",
      " |      4     True\n",
      " |      dtype: bool\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method works in a row-wise manner. It does not check if an element\n",
      " |      of one GeoSeries ``contains`` *any* element of the other one.\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.within\n",
      " |\n",
      " |  covered_by(self, other, align=True)\n",
      " |      Returns a ``Series`` of ``dtype('bool')`` with value ``True`` for\n",
      " |      each aligned geometry that is entirely covered by `other`.\n",
      " |\n",
      " |      An object A is said to cover another object B if no points of B lie\n",
      " |      in the exterior of A.\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      See\n",
      " |      https://lin-ear-th-inking.blogspot.com/2007/06/subtleties-of-ogc-covers-spatial.html\n",
      " |      for reference.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Geoseries or geometric object\n",
      " |          The Geoseries (elementwise) or geometric object to check is being covered.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series (bool)\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0.5, 0.5), (1.5, 0.5), (1.5, 1.5), (0.5, 1.5)]),\n",
      " |      ...         Polygon([(0, 0), (2, 0), (2, 2), (0, 2)]),\n",
      " |      ...         LineString([(1, 1), (1.5, 1.5)]),\n",
      " |      ...         Point(0, 0),\n",
      " |      ...     ],\n",
      " |      ... )\n",
      " |      >>> s2 = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (2, 0), (2, 2), (0, 2)]),\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         LineString([(0, 0), (2, 2)]),\n",
      " |      ...         Point(0, 0),\n",
      " |      ...     ],\n",
      " |      ...     index=range(1, 5),\n",
      " |      ... )\n",
      " |\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.50000 0.50000, 1.50000 0.50000, 1....\n",
      " |      1    POLYGON ((0.00000 0.00000, 2.00000 0.00000, 2....\n",
      " |      2        LINESTRING (1.00000 1.00000, 1.50000 1.50000)\n",
      " |      3                              POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s2\n",
      " |      1    POLYGON ((0.00000 0.00000, 2.00000 0.00000, 2....\n",
      " |      2    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      3        LINESTRING (0.00000 0.00000, 2.00000 2.00000)\n",
      " |      4                              POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can check if each geometry of GeoSeries is covered by a single\n",
      " |      geometry:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-03.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      >>> poly = Polygon([(0, 0), (2, 0), (2, 2), (0, 2)])\n",
      " |      >>> s.covered_by(poly)\n",
      " |      0    True\n",
      " |      1    True\n",
      " |      2    True\n",
      " |      3    True\n",
      " |      dtype: bool\n",
      " |\n",
      " |      We can also check two GeoSeries against each other, row by row.\n",
      " |      The GeoSeries above have different indices. We can either align both GeoSeries\n",
      " |      based on index values and compare elements with the same index using\n",
      " |      ``align=True`` or ignore index and compare elements based on their matching\n",
      " |      order using ``align=False``:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-02.svg\n",
      " |\n",
      " |      >>> s.covered_by(s2, align=True)\n",
      " |      0    False\n",
      " |      1     True\n",
      " |      2     True\n",
      " |      3     True\n",
      " |      4    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      >>> s.covered_by(s2, align=False)\n",
      " |      0     True\n",
      " |      1    False\n",
      " |      2     True\n",
      " |      3     True\n",
      " |      dtype: bool\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method works in a row-wise manner. It does not check if an element\n",
      " |      of one GeoSeries is ``covered_by`` *any* element of the other one.\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.covers\n",
      " |      GeoSeries.overlaps\n",
      " |\n",
      " |  covers(self, other, align=True)\n",
      " |      Returns a ``Series`` of ``dtype('bool')`` with value ``True`` for\n",
      " |      each aligned geometry that is entirely covering `other`.\n",
      " |\n",
      " |      An object A is said to cover another object B if no points of B lie\n",
      " |      in the exterior of A.\n",
      " |      If either object is empty, this operation returns ``False``.\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      See\n",
      " |      https://lin-ear-th-inking.blogspot.com/2007/06/subtleties-of-ogc-covers-spatial.html\n",
      " |      for reference.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Geoseries or geometric object\n",
      " |          The Geoseries (elementwise) or geometric object to check is being covered.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series (bool)\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (2, 0), (2, 2), (0, 2)]),\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         LineString([(0, 0), (2, 2)]),\n",
      " |      ...         Point(0, 0),\n",
      " |      ...     ],\n",
      " |      ... )\n",
      " |      >>> s2 = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0.5, 0.5), (1.5, 0.5), (1.5, 1.5), (0.5, 1.5)]),\n",
      " |      ...         Polygon([(0, 0), (2, 0), (2, 2), (0, 2)]),\n",
      " |      ...         LineString([(1, 1), (1.5, 1.5)]),\n",
      " |      ...         Point(0, 0),\n",
      " |      ...     ],\n",
      " |      ...     index=range(1, 5),\n",
      " |      ... )\n",
      " |\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 2.00000 0.00000, 2....\n",
      " |      1    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      2        LINESTRING (0.00000 0.00000, 2.00000 2.00000)\n",
      " |      3                              POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s2\n",
      " |      1    POLYGON ((0.50000 0.50000, 1.50000 0.50000, 1....\n",
      " |      2    POLYGON ((0.00000 0.00000, 2.00000 0.00000, 2....\n",
      " |      3        LINESTRING (1.00000 1.00000, 1.50000 1.50000)\n",
      " |      4                              POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can check if each geometry of GeoSeries covers a single\n",
      " |      geometry:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-03.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      >>> poly = Polygon([(0, 0), (2, 0), (2, 2), (0, 2)])\n",
      " |      >>> s.covers(poly)\n",
      " |      0     True\n",
      " |      1    False\n",
      " |      2    False\n",
      " |      3    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      We can also check two GeoSeries against each other, row by row.\n",
      " |      The GeoSeries above have different indices. We can either align both GeoSeries\n",
      " |      based on index values and compare elements with the same index using\n",
      " |      ``align=True`` or ignore index and compare elements based on their matching\n",
      " |      order using ``align=False``:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-02.svg\n",
      " |\n",
      " |      >>> s.covers(s2, align=True)\n",
      " |      0    False\n",
      " |      1    False\n",
      " |      2    False\n",
      " |      3    False\n",
      " |      4    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      >>> s.covers(s2, align=False)\n",
      " |      0     True\n",
      " |      1    False\n",
      " |      2     True\n",
      " |      3     True\n",
      " |      dtype: bool\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method works in a row-wise manner. It does not check if an element\n",
      " |      of one GeoSeries ``covers`` *any* element of the other one.\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.covered_by\n",
      " |      GeoSeries.overlaps\n",
      " |\n",
      " |  crosses(self, other, align=True)\n",
      " |      Returns a ``Series`` of ``dtype('bool')`` with value ``True`` for\n",
      " |      each aligned geometry that cross `other`.\n",
      " |\n",
      " |      An object is said to cross `other` if its `interior` intersects the\n",
      " |      `interior` of the other but does not contain it, and the dimension of\n",
      " |      the intersection is less than the dimension of the one or the other.\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : GeoSeries or geometric object\n",
      " |          The GeoSeries (elementwise) or geometric object to test if is\n",
      " |          crossed.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series (bool)\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         LineString([(0, 0), (2, 2)]),\n",
      " |      ...         LineString([(2, 0), (0, 2)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ... )\n",
      " |      >>> s2 = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         LineString([(1, 0), (1, 3)]),\n",
      " |      ...         LineString([(2, 0), (0, 2)]),\n",
      " |      ...         Point(1, 1),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ...     index=range(1, 5),\n",
      " |      ... )\n",
      " |\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      1        LINESTRING (0.00000 0.00000, 2.00000 2.00000)\n",
      " |      2        LINESTRING (2.00000 0.00000, 0.00000 2.00000)\n",
      " |      3                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s2\n",
      " |      1    LINESTRING (1.00000 0.00000, 1.00000 3.00000)\n",
      " |      2    LINESTRING (2.00000 0.00000, 0.00000 2.00000)\n",
      " |      3                          POINT (1.00000 1.00000)\n",
      " |      4                          POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can check if each geometry of GeoSeries crosses a single\n",
      " |      geometry:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-03.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      >>> line = LineString([(-1, 1), (3, 1)])\n",
      " |      >>> s.crosses(line)\n",
      " |      0     True\n",
      " |      1     True\n",
      " |      2     True\n",
      " |      3    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      We can also check two GeoSeries against each other, row by row.\n",
      " |      The GeoSeries above have different indices. We can either align both GeoSeries\n",
      " |      based on index values and compare elements with the same index using\n",
      " |      ``align=True`` or ignore index and compare elements based on their matching\n",
      " |      order using ``align=False``:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-02.svg\n",
      " |\n",
      " |      >>> s.crosses(s2, align=True)\n",
      " |      0    False\n",
      " |      1     True\n",
      " |      2    False\n",
      " |      3    False\n",
      " |      4    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      >>> s.crosses(s2, align=False)\n",
      " |      0     True\n",
      " |      1     True\n",
      " |      2    False\n",
      " |      3    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      Notice that a line does not cross a point that it contains.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method works in a row-wise manner. It does not check if an element\n",
      " |      of one GeoSeries ``crosses`` *any* element of the other one.\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.disjoint\n",
      " |      GeoSeries.intersects\n",
      " |\n",
      " |  delaunay_triangles(self, tolerance=0.0, only_edges=False)\n",
      " |      Returns a ``GeoSeries`` consisting of objects representing\n",
      " |      the computed Delaunay triangulation around the vertices of\n",
      " |      an input geometry.\n",
      " |\n",
      " |      The output is a ``GeometryCollection`` containing polygons\n",
      " |      (default) or linestrings (see only_edges).\n",
      " |\n",
      " |      Returns an empty GeometryCollection if an input geometry\n",
      " |      contains less than 3 vertices.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tolerance : float | array-like, default 0.0\n",
      " |          Snap input vertices together if their distance is less than this value.\n",
      " |      only_edges : bool | array_like, (optional, default False)\n",
      " |          If set to True, the triangulation will return a collection of\n",
      " |          linestrings instead of polygons.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely import LineString, MultiPoint, Polygon\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         MultiPoint([(50, 30), (60, 30), (100, 100)]),\n",
      " |      ...         Polygon([(50, 30), (60, 30), (100, 100), (50, 30)]),\n",
      " |      ...         LineString([(50, 30), (60, 30), (100, 100)]),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0   MULTIPOINT (50.000 30.000, 60.000 30.000, 100....\n",
      " |      1   POLYGON ((50.000 30.000, 60.000 30.000, 100.00...\n",
      " |      2   LINESTRING (50.000 30.000, 60.000 30.000, 100....\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.delaunay_triangles()\n",
      " |      0    GEOMETRYCOLLECTION (POLYGON ((50.000 30.000, 6...\n",
      " |      1    GEOMETRYCOLLECTION (POLYGON ((50.000 30.000, 6...\n",
      " |      2    GEOMETRYCOLLECTION (POLYGON ((50.000 30.000, 6...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.delaunay_triangles(only_edges=True)\n",
      " |      0    MULTILINESTRING ((50.000 30.000, 100.000 100.0...\n",
      " |      1    MULTILINESTRING ((50.000 30.000, 100.000 100.0...\n",
      " |      2    MULTILINESTRING ((50.000 30.000, 100.000 100.0...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |  difference(self, other, align=True)\n",
      " |      Returns a ``GeoSeries`` of the points in each aligned geometry that\n",
      " |      are not in `other`.\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_geo-difference.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Geoseries or geometric object\n",
      " |          The Geoseries (elementwise) or geometric object to find the\n",
      " |          difference to.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      GeoSeries\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         LineString([(0, 0), (2, 2)]),\n",
      " |      ...         LineString([(2, 0), (0, 2)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ... )\n",
      " |      >>> s2 = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         LineString([(1, 0), (1, 3)]),\n",
      " |      ...         LineString([(2, 0), (0, 2)]),\n",
      " |      ...         Point(1, 1),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ...     index=range(1, 6),\n",
      " |      ... )\n",
      " |\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      1    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      2        LINESTRING (0.00000 0.00000, 2.00000 2.00000)\n",
      " |      3        LINESTRING (2.00000 0.00000, 0.00000 2.00000)\n",
      " |      4                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s2\n",
      " |      1    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 0....\n",
      " |      2        LINESTRING (1.00000 0.00000, 1.00000 3.00000)\n",
      " |      3        LINESTRING (2.00000 0.00000, 0.00000 2.00000)\n",
      " |      4                              POINT (1.00000 1.00000)\n",
      " |      5                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can do difference of each geometry and a single\n",
      " |      shapely geometry:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-03.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      >>> s.difference(Polygon([(0, 0), (1, 1), (0, 1)]))\n",
      " |      0    POLYGON ((0.00000 2.00000, 2.00000 2.00000, 1....\n",
      " |      1    POLYGON ((0.00000 2.00000, 2.00000 2.00000, 1....\n",
      " |      2        LINESTRING (1.00000 1.00000, 2.00000 2.00000)\n",
      " |      3    MULTILINESTRING ((2.00000 0.00000, 1.00000 1.0...\n",
      " |      4                                          POINT EMPTY\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can also check two GeoSeries against each other, row by row.\n",
      " |      The GeoSeries above have different indices. We can either align both GeoSeries\n",
      " |      based on index values and compare elements with the same index using\n",
      " |      ``align=True`` or ignore index and compare elements based on their matching\n",
      " |      order using ``align=False``:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-02.svg\n",
      " |\n",
      " |      >>> s.difference(s2, align=True)\n",
      " |      0                                                 None\n",
      " |      1    POLYGON ((0.00000 2.00000, 2.00000 2.00000, 1....\n",
      " |      2    MULTILINESTRING ((0.00000 0.00000, 1.00000 1.0...\n",
      " |      3                                   LINESTRING Z EMPTY\n",
      " |      4                              POINT (0.00000 1.00000)\n",
      " |      5                                                 None\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.difference(s2, align=False)\n",
      " |      0    POLYGON ((0.00000 2.00000, 2.00000 2.00000, 1....\n",
      " |      1    POLYGON ((0.00000 0.00000, 0.00000 2.00000, 1....\n",
      " |      2    MULTILINESTRING ((0.00000 0.00000, 1.00000 1.0...\n",
      " |      3        LINESTRING (2.00000 0.00000, 0.00000 2.00000)\n",
      " |      4                                          POINT EMPTY\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      GeoSeries.symmetric_difference\n",
      " |      GeoSeries.union\n",
      " |      GeoSeries.intersection\n",
      " |\n",
      " |  disjoint(self, other, align=True)\n",
      " |      Returns a ``Series`` of ``dtype('bool')`` with value ``True`` for\n",
      " |      each aligned geometry disjoint to `other`.\n",
      " |\n",
      " |      An object is said to be disjoint to `other` if its `boundary` and\n",
      " |      `interior` does not intersect at all with those of the other.\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : GeoSeries or geometric object\n",
      " |          The GeoSeries (elementwise) or geometric object to test if is\n",
      " |          disjoint.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series (bool)\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         LineString([(0, 0), (2, 2)]),\n",
      " |      ...         LineString([(2, 0), (0, 2)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ... )\n",
      " |      >>> s2 = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(-1, 0), (-1, 2), (0, -2)]),\n",
      " |      ...         LineString([(0, 0), (0, 1)]),\n",
      " |      ...         Point(1, 1),\n",
      " |      ...         Point(0, 0),\n",
      " |      ...     ],\n",
      " |      ... )\n",
      " |\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      1        LINESTRING (0.00000 0.00000, 2.00000 2.00000)\n",
      " |      2        LINESTRING (2.00000 0.00000, 0.00000 2.00000)\n",
      " |      3                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s2\n",
      " |      0    POLYGON ((-1.00000 0.00000, -1.00000 2.00000, ...\n",
      " |      1        LINESTRING (0.00000 0.00000, 0.00000 1.00000)\n",
      " |      2                              POINT (1.00000 1.00000)\n",
      " |      3                              POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can check each geometry of GeoSeries to a single\n",
      " |      geometry:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-03.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      >>> line = LineString([(0, 0), (2, 0)])\n",
      " |      >>> s.disjoint(line)\n",
      " |      0    False\n",
      " |      1    False\n",
      " |      2    False\n",
      " |      3     True\n",
      " |      dtype: bool\n",
      " |\n",
      " |      We can also check two GeoSeries against each other, row by row.\n",
      " |      We can either align both GeoSeries\n",
      " |      based on index values and compare elements with the same index using\n",
      " |      ``align=True`` or ignore index and compare elements based on their matching\n",
      " |      order using ``align=False``:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-02.svg\n",
      " |\n",
      " |      >>> s.disjoint(s2)\n",
      " |      0     True\n",
      " |      1    False\n",
      " |      2    False\n",
      " |      3     True\n",
      " |      dtype: bool\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method works in a row-wise manner. It does not check if an element\n",
      " |      of one GeoSeries is equal to *any* element of the other one.\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.intersects\n",
      " |      GeoSeries.touches\n",
      " |\n",
      " |  distance(self, other, align=True)\n",
      " |      Returns a ``Series`` containing the distance to aligned `other`.\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Geoseries or geometric object\n",
      " |          The Geoseries (elementwise) or geometric object to find the\n",
      " |          distance to.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series (float)\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 0), (1, 1)]),\n",
      " |      ...         Polygon([(0, 0), (-1, 0), (-1, 1)]),\n",
      " |      ...         LineString([(1, 1), (0, 0)]),\n",
      " |      ...         Point(0, 0),\n",
      " |      ...     ],\n",
      " |      ... )\n",
      " |      >>> s2 = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0.5, 0.5), (1.5, 0.5), (1.5, 1.5), (0.5, 1.5)]),\n",
      " |      ...         Point(3, 1),\n",
      " |      ...         LineString([(1, 0), (2, 0)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ...     index=range(1, 5),\n",
      " |      ... )\n",
      " |\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 1.00000 0.00000, 1....\n",
      " |      1    POLYGON ((0.00000 0.00000, -1.00000 0.00000, -...\n",
      " |      2        LINESTRING (1.00000 1.00000, 0.00000 0.00000)\n",
      " |      3                              POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s2\n",
      " |      1    POLYGON ((0.50000 0.50000, 1.50000 0.50000, 1....\n",
      " |      2                              POINT (3.00000 1.00000)\n",
      " |      3        LINESTRING (1.00000 0.00000, 2.00000 0.00000)\n",
      " |      4                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can check the distance of each geometry of GeoSeries to a single\n",
      " |      geometry:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-03.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      >>> point = Point(-1, 0)\n",
      " |      >>> s.distance(point)\n",
      " |      0    1.0\n",
      " |      1    0.0\n",
      " |      2    1.0\n",
      " |      3    1.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |      We can also check two GeoSeries against each other, row by row.\n",
      " |      The GeoSeries above have different indices. We can either align both GeoSeries\n",
      " |      based on index values and use elements with the same index using\n",
      " |      ``align=True`` or ignore index and use elements based on their matching\n",
      " |      order using ``align=False``:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-02.svg\n",
      " |\n",
      " |      >>> s.distance(s2, align=True)\n",
      " |      0         NaN\n",
      " |      1    0.707107\n",
      " |      2    2.000000\n",
      " |      3    1.000000\n",
      " |      4         NaN\n",
      " |      dtype: float64\n",
      " |\n",
      " |      >>> s.distance(s2, align=False)\n",
      " |      0    0.000000\n",
      " |      1    3.162278\n",
      " |      2    0.707107\n",
      " |      3    1.000000\n",
      " |      dtype: float64\n",
      " |\n",
      " |  extract_unique_points(self)\n",
      " |      Returns a ``GeoSeries`` of MultiPoints representing all\n",
      " |      distinct vertices of an input geometry.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely import LineString, Polygon\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         LineString([(0, 0), (0, 0), (1, 1), (1, 1)]),\n",
      " |      ...         Polygon([(0, 0), (0, 0), (1, 1), (1, 1)])\n",
      " |      ...     ],\n",
      " |      ...     crs=3857\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    LINESTRING (0.000 0.000, 0.000 0.000, 1.000 1....\n",
      " |      1    POLYGON ((0.000 0.000, 0.000 0.000, 1.000 1.00...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.extract_unique_points()\n",
      " |      0    MULTIPOINT (0.000 0.000, 1.000 1.000)\n",
      " |      1    MULTIPOINT (0.000 0.000, 1.000 1.000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |\n",
      " |      GeoSeries.get_coordinates : extract coordinates as a :class:`~pandas.DataFrame`\n",
      " |\n",
      " |  frechet_distance(self, other, align=True, densify=None)\n",
      " |      Returns a ``Series`` containing the Frechet distance to aligned `other`.\n",
      " |\n",
      " |      The Fréchet distance is a measure of similarity: it is the greatest distance\n",
      " |      between any point in A and the closest point in B. The discrete distance is an\n",
      " |      approximation of this metric: only vertices are considered. The parameter\n",
      " |      ``densify`` makes this approximation less coarse by splitting the line segments\n",
      " |      between vertices before computing the distance.\n",
      " |\n",
      " |      Fréchet distance sweep continuously along their respective curves and the\n",
      " |      direction of curves is significant. This makes it a better measure of similarity\n",
      " |      than Hausdorff distance for curve or surface matching.\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : GeoSeries or geometric object\n",
      " |          The Geoseries (elementwise) or geometric object to find the\n",
      " |          distance to.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |      densify : float (default None)\n",
      " |          A value between 0 and 1, that splits each subsegment of a line string\n",
      " |          into equal length segments, making the approximation less coarse.\n",
      " |          A densify value of 0.5 will add a point halfway between each pair of\n",
      " |          points. A densify value of 0.25 will add a point every quarter of the way\n",
      " |          between each pair of points.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series (float)\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 0), (1, 1)]),\n",
      " |      ...         Polygon([(0, 0), (-1, 0), (-1, 1)]),\n",
      " |      ...         LineString([(1, 1), (0, 0)]),\n",
      " |      ...         Point(0, 0),\n",
      " |      ...     ],\n",
      " |      ... )\n",
      " |      >>> s2 = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0.5, 0.5), (1.5, 0.5), (1.5, 1.5), (0.5, 1.5)]),\n",
      " |      ...         Point(3, 1),\n",
      " |      ...         LineString([(1, 0), (2, 0)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ...     index=range(1, 5),\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 1.00000 0.00000, 1....\n",
      " |      1    POLYGON ((0.00000 0.00000, -1.00000 0.00000, -...\n",
      " |      2        LINESTRING (1.00000 1.00000, 0.00000 0.00000)\n",
      " |      3                              POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |      >>> s2\n",
      " |      1    POLYGON ((0.50000 0.50000, 1.50000 0.50000, 1....\n",
      " |      2                              POINT (3.00000 1.00000)\n",
      " |      3        LINESTRING (1.00000 0.00000, 2.00000 0.00000)\n",
      " |      4                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can check the frechet distance of each geometry of GeoSeries\n",
      " |      to a single geometry:\n",
      " |\n",
      " |      >>> point = Point(-1, 0)\n",
      " |      >>> s.frechet_distance(point)\n",
      " |      0    2.236068\n",
      " |      1    1.000000\n",
      " |      2    2.236068\n",
      " |      3    1.000000\n",
      " |      dtype: float64\n",
      " |\n",
      " |      We can also check two GeoSeries against each other, row by row.\n",
      " |      The GeoSeries above have different indices. We can either align both GeoSeries\n",
      " |      based on index values and use elements with the same index using\n",
      " |      ``align=True`` or ignore index and use elements based on their matching\n",
      " |      order using ``align=False``:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-02.svg\n",
      " |\n",
      " |      >>> s.frechet_distance(s2, align=True)\n",
      " |      0         NaN\n",
      " |      1    2.121320\n",
      " |      2    3.162278\n",
      " |      3    2.000000\n",
      " |      4         NaN\n",
      " |      dtype: float64\n",
      " |      >>> s.frechet_distance(s2, align=False)\n",
      " |      0    0.707107\n",
      " |      1    4.123106\n",
      " |      2    2.000000\n",
      " |      3    1.000000\n",
      " |      dtype: float64\n",
      " |\n",
      " |      We can also set a ``densify`` value, which is a float between 0 and 1 and\n",
      " |      signifies the fraction of the distance between each pair of points that will\n",
      " |      be used as the distance between the points when densifying.\n",
      " |\n",
      " |      >>> l1 = geopandas.GeoSeries([LineString([(0, 0), (10, 0), (0, 15)])])\n",
      " |      >>> l2 = geopandas.GeoSeries([LineString([(0, 0), (20, 15), (9, 11)])])\n",
      " |      >>> l1.frechet_distance(l2)\n",
      " |      0    18.027756\n",
      " |      dtype: float64\n",
      " |      >>> l1.frechet_distance(l2, densify=0.25)\n",
      " |      0    16.77051\n",
      " |      dtype: float64\n",
      " |\n",
      " |  geom_almost_equals(self, other, decimal=6, align=True)\n",
      " |      Returns a ``Series`` of ``dtype('bool')`` with value ``True`` if\n",
      " |      each aligned geometry is approximately equal to `other`.\n",
      " |\n",
      " |      Approximate equality is tested at all points to the specified `decimal`\n",
      " |      place precision.\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : GeoSeries or geometric object\n",
      " |          The GeoSeries (elementwise) or geometric object to compare to.\n",
      " |      decimal : int\n",
      " |          Decimal place precision used when testing for approximate equality.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series (bool)\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Point(0, 1.1),\n",
      " |      ...         Point(0, 1.01),\n",
      " |      ...         Point(0, 1.001),\n",
      " |      ...     ],\n",
      " |      ... )\n",
      " |\n",
      " |      >>> s\n",
      " |      0    POINT (0.00000 1.10000)\n",
      " |      1    POINT (0.00000 1.01000)\n",
      " |      2    POINT (0.00000 1.00100)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |\n",
      " |      >>> s.geom_almost_equals(Point(0, 1), decimal=2)\n",
      " |      0    False\n",
      " |      1    False\n",
      " |      2     True\n",
      " |      dtype: bool\n",
      " |\n",
      " |      >>> s.geom_almost_equals(Point(0, 1), decimal=1)\n",
      " |      0    False\n",
      " |      1     True\n",
      " |      2     True\n",
      " |      dtype: bool\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method works in a row-wise manner. It does not check if an element\n",
      " |      of one GeoSeries is equal to *any* element of the other one.\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.geom_equals\n",
      " |      GeoSeries.geom_equals_exact\n",
      " |\n",
      " |  geom_equals(self, other, align=True)\n",
      " |      Returns a ``Series`` of ``dtype('bool')`` with value ``True`` for\n",
      " |      each aligned geometry equal to `other`.\n",
      " |\n",
      " |      An object is said to be equal to `other` if its set-theoretic\n",
      " |      `boundary`, `interior`, and `exterior` coincides with those of the\n",
      " |      other.\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : GeoSeries or geometric object\n",
      " |          The GeoSeries (elementwise) or geometric object to test for\n",
      " |          equality.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series (bool)\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         Polygon([(0, 0), (1, 2), (0, 2)]),\n",
      " |      ...         LineString([(0, 0), (0, 2)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ... )\n",
      " |      >>> s2 = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         Polygon([(0, 0), (1, 2), (0, 2)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...         LineString([(0, 0), (0, 2)]),\n",
      " |      ...     ],\n",
      " |      ...     index=range(1, 5),\n",
      " |      ... )\n",
      " |\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      1    POLYGON ((0.00000 0.00000, 1.00000 2.00000, 0....\n",
      " |      2        LINESTRING (0.00000 0.00000, 0.00000 2.00000)\n",
      " |      3                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s2\n",
      " |      1    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      2    POLYGON ((0.00000 0.00000, 1.00000 2.00000, 0....\n",
      " |      3                              POINT (0.00000 1.00000)\n",
      " |      4        LINESTRING (0.00000 0.00000, 0.00000 2.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can check if each geometry of GeoSeries contains a single\n",
      " |      geometry:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-03.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      >>> polygon = Polygon([(0, 0), (2, 2), (0, 2)])\n",
      " |      >>> s.geom_equals(polygon)\n",
      " |      0     True\n",
      " |      1    False\n",
      " |      2    False\n",
      " |      3    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      We can also check two GeoSeries against each other, row by row.\n",
      " |      The GeoSeries above have different indices. We can either align both GeoSeries\n",
      " |      based on index values and compare elements with the same index using\n",
      " |      ``align=True`` or ignore index and compare elements based on their matching\n",
      " |      order using ``align=False``:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-02.svg\n",
      " |\n",
      " |      >>> s.geom_equals(s2)\n",
      " |      0    False\n",
      " |      1    False\n",
      " |      2    False\n",
      " |      3     True\n",
      " |      4    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      >>> s.geom_equals(s2, align=False)\n",
      " |      0     True\n",
      " |      1     True\n",
      " |      2    False\n",
      " |      3    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method works in a row-wise manner. It does not check if an element\n",
      " |      of one GeoSeries is equal to *any* element of the other one.\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.geom_equals_exact\n",
      " |\n",
      " |  geom_equals_exact(self, other, tolerance, align=True)\n",
      " |      Return True for all geometries that equal aligned *other* to a given\n",
      " |      tolerance, else False.\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : GeoSeries or geometric object\n",
      " |          The GeoSeries (elementwise) or geometric object to compare to.\n",
      " |      tolerance : float\n",
      " |          Decimal place precision used when testing for approximate equality.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series (bool)\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Point(0, 1.1),\n",
      " |      ...         Point(0, 1.0),\n",
      " |      ...         Point(0, 1.2),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |\n",
      " |      >>> s\n",
      " |      0    POINT (0.00000 1.10000)\n",
      " |      1    POINT (0.00000 1.00000)\n",
      " |      2    POINT (0.00000 1.20000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |\n",
      " |      >>> s.geom_equals_exact(Point(0, 1), tolerance=0.1)\n",
      " |      0    False\n",
      " |      1     True\n",
      " |      2    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      >>> s.geom_equals_exact(Point(0, 1), tolerance=0.15)\n",
      " |      0     True\n",
      " |      1     True\n",
      " |      2    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method works in a row-wise manner. It does not check if an element\n",
      " |      of one GeoSeries is equal to *any* element of the other one.\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.geom_equals\n",
      " |\n",
      " |  get_coordinates(self, include_z=False, ignore_index=False, index_parts=False)\n",
      " |      Gets coordinates from a :class:`GeoSeries` as a :class:`~pandas.DataFrame` of\n",
      " |      floats.\n",
      " |\n",
      " |      The shape of the returned :class:`~pandas.DataFrame` is (N, 2), with N being the\n",
      " |      number of coordinate pairs. With the default of ``include_z=False``,\n",
      " |      three-dimensional data is ignored. When specifying ``include_z=True``, the shape\n",
      " |      of the returned :class:`~pandas.DataFrame` is (N, 3).\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      include_z : bool, default False\n",
      " |          Include Z coordinates\n",
      " |      ignore_index : bool, default False\n",
      " |          If True, the resulting index will be labelled 0, 1, …, n - 1, ignoring\n",
      " |          ``index_parts``.\n",
      " |      index_parts : bool, default False\n",
      " |         If True, the resulting index will be a :class:`~pandas.MultiIndex` (original\n",
      " |         index with an additional level indicating the ordering of the coordinate\n",
      " |         pairs: a new zero-based index for each geometry in the original GeoSeries).\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      pandas.DataFrame\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Point, LineString, Polygon\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Point(1, 1),\n",
      " |      ...         LineString([(1, -1), (1, 0)]),\n",
      " |      ...         Polygon([(3, -1), (4, 0), (3, 1)]),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0                              POINT (1.00000 1.00000)\n",
      " |      1       LINESTRING (1.00000 -1.00000, 1.00000 0.00000)\n",
      " |      2    POLYGON ((3.00000 -1.00000, 4.00000 0.00000, 3...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.get_coordinates()\n",
      " |           x    y\n",
      " |      0  1.0  1.0\n",
      " |      1  1.0 -1.0\n",
      " |      1  1.0  0.0\n",
      " |      2  3.0 -1.0\n",
      " |      2  4.0  0.0\n",
      " |      2  3.0  1.0\n",
      " |      2  3.0 -1.0\n",
      " |\n",
      " |      >>> s.get_coordinates(ignore_index=True)\n",
      " |           x    y\n",
      " |      0  1.0  1.0\n",
      " |      1  1.0 -1.0\n",
      " |      2  1.0  0.0\n",
      " |      3  3.0 -1.0\n",
      " |      4  4.0  0.0\n",
      " |      5  3.0  1.0\n",
      " |      6  3.0 -1.0\n",
      " |\n",
      " |      >>> s.get_coordinates(index_parts=True)\n",
      " |             x    y\n",
      " |      0 0  1.0  1.0\n",
      " |      1 0  1.0 -1.0\n",
      " |        1  1.0  0.0\n",
      " |      2 0  3.0 -1.0\n",
      " |        1  4.0  0.0\n",
      " |        2  3.0  1.0\n",
      " |        3  3.0 -1.0\n",
      " |\n",
      " |  hausdorff_distance(self, other, align=True, densify=None)\n",
      " |      Returns a ``Series`` containing the Hausdorff distance to aligned `other`.\n",
      " |\n",
      " |      The Hausdorff distance is the largest distance consisting of any point in `self`\n",
      " |      with the nearest point in `other`.\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : GeoSeries or geometric object\n",
      " |          The Geoseries (elementwise) or geometric object to find the\n",
      " |          distance to.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |      densify : float (default None)\n",
      " |          A value between 0 and 1, that splits each subsegment of a line string\n",
      " |          into equal length segments, making the approximation less coarse.\n",
      " |          A densify value of 0.5 will add a point halfway between each pair of\n",
      " |          points. A densify value of 0.25 will add a point a quarter of the way\n",
      " |          between each pair of points.\n",
      " |\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series (float)\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 0), (1, 1)]),\n",
      " |      ...         Polygon([(0, 0), (-1, 0), (-1, 1)]),\n",
      " |      ...         LineString([(1, 1), (0, 0)]),\n",
      " |      ...         Point(0, 0),\n",
      " |      ...     ],\n",
      " |      ... )\n",
      " |      >>> s2 = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0.5, 0.5), (1.5, 0.5), (1.5, 1.5), (0.5, 1.5)]),\n",
      " |      ...         Point(3, 1),\n",
      " |      ...         LineString([(1, 0), (2, 0)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ...     index=range(1, 5),\n",
      " |      ... )\n",
      " |\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 1.00000 0.00000, 1....\n",
      " |      1    POLYGON ((0.00000 0.00000, -1.00000 0.00000, -...\n",
      " |      2        LINESTRING (1.00000 1.00000, 0.00000 0.00000)\n",
      " |      3                              POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s2\n",
      " |      1    POLYGON ((0.50000 0.50000, 1.50000 0.50000, 1....\n",
      " |      2                              POINT (3.00000 1.00000)\n",
      " |      3        LINESTRING (1.00000 0.00000, 2.00000 0.00000)\n",
      " |      4                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can check the hausdorff distance of each geometry of GeoSeries\n",
      " |      to a single geometry:\n",
      " |\n",
      " |      >>> point = Point(-1, 0)\n",
      " |      >>> s.hausdorff_distance(point)\n",
      " |      0    2.236068\n",
      " |      1    1.000000\n",
      " |      2    2.236068\n",
      " |      3    1.000000\n",
      " |      dtype: float64\n",
      " |\n",
      " |      We can also check two GeoSeries against each other, row by row.\n",
      " |      The GeoSeries above have different indices. We can either align both GeoSeries\n",
      " |      based on index values and use elements with the same index using\n",
      " |      ``align=True`` or ignore index and use elements based on their matching\n",
      " |      order using ``align=False``:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-02.svg\n",
      " |\n",
      " |      >>> s.hausdorff_distance(s2, align=True)\n",
      " |      0         NaN\n",
      " |      1    2.121320\n",
      " |      2    3.162278\n",
      " |      3    2.000000\n",
      " |      4         NaN\n",
      " |      dtype: float64\n",
      " |\n",
      " |      >>> s.hausdorff_distance(s2, align=False)\n",
      " |      0    0.707107\n",
      " |      1    4.123106\n",
      " |      2    1.414214\n",
      " |      3    1.000000\n",
      " |      dtype: float64\n",
      " |\n",
      " |      We can also set a densify value, which is a float between 0 and 1 and\n",
      " |      signifies the fraction of the distance between each pair of points that will\n",
      " |      be used as the distance between the points when densifying.\n",
      " |\n",
      " |      >>> l1 = geopandas.GeoSeries([LineString([(130, 0), (0, 0), (0, 150)])])\n",
      " |      >>> l2 = geopandas.GeoSeries([LineString([(10, 10), (10, 150), (130, 10)])])\n",
      " |      >>> l1.hausdorff_distance(l2)\n",
      " |      0    14.142136\n",
      " |      dtype: float64\n",
      " |      >>> l1.hausdorff_distance(l2, densify=0.25)\n",
      " |      0    70.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |  hilbert_distance(self, total_bounds=None, level=16)\n",
      " |      Calculate the distance along a Hilbert curve.\n",
      " |\n",
      " |      The distances are calculated for the midpoints of the geometries in the\n",
      " |      GeoDataFrame, and using the total bounds of the GeoDataFrame.\n",
      " |\n",
      " |      The Hilbert distance can be used to spatially sort GeoPandas\n",
      " |      objects, by mapping two dimensional geometries along the Hilbert curve.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      total_bounds : 4-element array, optional\n",
      " |          The spatial extent in which the curve is constructed (used to\n",
      " |          rescale the geometry midpoints). By default, the total bounds\n",
      " |          of the full GeoDataFrame or GeoSeries will be computed. If known,\n",
      " |          you can pass the total bounds to avoid this extra computation.\n",
      " |      level : int (1 - 16), default 16\n",
      " |          Determines the precision of the curve (points on the curve will\n",
      " |          have coordinates in the range [0, 2^level - 1]).\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series\n",
      " |          Series containing distance along the curve for geometry\n",
      " |\n",
      " |  interpolate(self, distance, normalized=False)\n",
      " |      Return a point at the specified distance along each geometry\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      distance : float or Series of floats\n",
      " |          Distance(s) along the geometries at which a point should be\n",
      " |          returned. If np.array or pd.Series are used then it must have\n",
      " |          same length as the GeoSeries.\n",
      " |      normalized : boolean\n",
      " |          If normalized is True, distance will be interpreted as a fraction\n",
      " |          of the geometric object's length.\n",
      " |\n",
      " |  intersection(self, other, align=True)\n",
      " |      Returns a ``GeoSeries`` of the intersection of points in each\n",
      " |      aligned geometry with `other`.\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_geo-intersection.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |         :align: center\n",
      " |\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Geoseries or geometric object\n",
      " |          The Geoseries (elementwise) or geometric object to find the\n",
      " |          intersection with.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      GeoSeries\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         LineString([(0, 0), (2, 2)]),\n",
      " |      ...         LineString([(2, 0), (0, 2)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ... )\n",
      " |      >>> s2 = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         LineString([(1, 0), (1, 3)]),\n",
      " |      ...         LineString([(2, 0), (0, 2)]),\n",
      " |      ...         Point(1, 1),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ...     index=range(1, 6),\n",
      " |      ... )\n",
      " |\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      1    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      2        LINESTRING (0.00000 0.00000, 2.00000 2.00000)\n",
      " |      3        LINESTRING (2.00000 0.00000, 0.00000 2.00000)\n",
      " |      4                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s2\n",
      " |      1    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 0....\n",
      " |      2        LINESTRING (1.00000 0.00000, 1.00000 3.00000)\n",
      " |      3        LINESTRING (2.00000 0.00000, 0.00000 2.00000)\n",
      " |      4                              POINT (1.00000 1.00000)\n",
      " |      5                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can also do intersection of each geometry and a single\n",
      " |      shapely geometry:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-03.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      >>> s.intersection(Polygon([(0, 0), (1, 1), (0, 1)]))\n",
      " |      0    POLYGON ((0.00000 0.00000, 0.00000 1.00000, 1....\n",
      " |      1    POLYGON ((0.00000 0.00000, 0.00000 1.00000, 1....\n",
      " |      2        LINESTRING (0.00000 0.00000, 1.00000 1.00000)\n",
      " |      3                              POINT (1.00000 1.00000)\n",
      " |      4                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can also check two GeoSeries against each other, row by row.\n",
      " |      The GeoSeries above have different indices. We can either align both GeoSeries\n",
      " |      based on index values and compare elements with the same index using\n",
      " |      ``align=True`` or ignore index and compare elements based on their matching\n",
      " |      order using ``align=False``:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-02.svg\n",
      " |\n",
      " |      >>> s.intersection(s2, align=True)\n",
      " |      0                                                 None\n",
      " |      1    POLYGON ((0.00000 0.00000, 0.00000 1.00000, 1....\n",
      " |      2                              POINT (1.00000 1.00000)\n",
      " |      3        LINESTRING (2.00000 0.00000, 0.00000 2.00000)\n",
      " |      4                                          POINT EMPTY\n",
      " |      5                                                 None\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.intersection(s2, align=False)\n",
      " |      0    POLYGON ((0.00000 0.00000, 0.00000 1.00000, 1....\n",
      " |      1        LINESTRING (1.00000 1.00000, 1.00000 2.00000)\n",
      " |      2                              POINT (1.00000 1.00000)\n",
      " |      3                              POINT (1.00000 1.00000)\n",
      " |      4                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      GeoSeries.difference\n",
      " |      GeoSeries.symmetric_difference\n",
      " |      GeoSeries.union\n",
      " |\n",
      " |  intersects(self, other, align=True)\n",
      " |      Returns a ``Series`` of ``dtype('bool')`` with value ``True`` for\n",
      " |      each aligned geometry that intersects `other`.\n",
      " |\n",
      " |      An object is said to intersect `other` if its `boundary` and `interior`\n",
      " |      intersects in any way with those of the other.\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : GeoSeries or geometric object\n",
      " |          The GeoSeries (elementwise) or geometric object to test if is\n",
      " |          intersected.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series (bool)\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         LineString([(0, 0), (2, 2)]),\n",
      " |      ...         LineString([(2, 0), (0, 2)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ... )\n",
      " |      >>> s2 = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         LineString([(1, 0), (1, 3)]),\n",
      " |      ...         LineString([(2, 0), (0, 2)]),\n",
      " |      ...         Point(1, 1),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ...     index=range(1, 5),\n",
      " |      ... )\n",
      " |\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      1        LINESTRING (0.00000 0.00000, 2.00000 2.00000)\n",
      " |      2        LINESTRING (2.00000 0.00000, 0.00000 2.00000)\n",
      " |      3                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s2\n",
      " |      1    LINESTRING (1.00000 0.00000, 1.00000 3.00000)\n",
      " |      2    LINESTRING (2.00000 0.00000, 0.00000 2.00000)\n",
      " |      3                          POINT (1.00000 1.00000)\n",
      " |      4                          POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can check if each geometry of GeoSeries crosses a single\n",
      " |      geometry:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-03.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      >>> line = LineString([(-1, 1), (3, 1)])\n",
      " |      >>> s.intersects(line)\n",
      " |      0    True\n",
      " |      1    True\n",
      " |      2    True\n",
      " |      3    True\n",
      " |      dtype: bool\n",
      " |\n",
      " |      We can also check two GeoSeries against each other, row by row.\n",
      " |      The GeoSeries above have different indices. We can either align both GeoSeries\n",
      " |      based on index values and compare elements with the same index using\n",
      " |      ``align=True`` or ignore index and compare elements based on their matching\n",
      " |      order using ``align=False``:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-02.svg\n",
      " |\n",
      " |      >>> s.intersects(s2, align=True)\n",
      " |      0    False\n",
      " |      1     True\n",
      " |      2     True\n",
      " |      3    False\n",
      " |      4    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      >>> s.intersects(s2, align=False)\n",
      " |      0    True\n",
      " |      1    True\n",
      " |      2    True\n",
      " |      3    True\n",
      " |      dtype: bool\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method works in a row-wise manner. It does not check if an element\n",
      " |      of one GeoSeries ``crosses`` *any* element of the other one.\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.disjoint\n",
      " |      GeoSeries.crosses\n",
      " |      GeoSeries.touches\n",
      " |      GeoSeries.intersection\n",
      " |\n",
      " |  make_valid(self)\n",
      " |      Repairs invalid geometries.\n",
      " |\n",
      " |      Returns a ``GeoSeries`` with valid geometries.\n",
      " |      If the input geometry is already valid, then it will be preserved.\n",
      " |      In many cases, in order to create a valid geometry, the input\n",
      " |      geometry must be split into multiple parts or multiple geometries.\n",
      " |      If the geometry must be split into multiple parts of the same type\n",
      " |      to be made valid, then a multi-part geometry will be returned\n",
      " |      (e.g. a MultiPolygon).\n",
      " |      If the geometry must be split into multiple parts of different types\n",
      " |      to be made valid, then a GeometryCollection will be returned.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import MultiPolygon, Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (0, 2), (1, 1), (2, 2), (2, 0), (1, 1), (0, 0)]),\n",
      " |      ...         Polygon([(0, 2), (0, 1), (2, 0), (0, 0), (0, 2)]),\n",
      " |      ...         LineString([(0, 0), (1, 1), (1, 0)]),\n",
      " |      ...     ],\n",
      " |      ...     crs='EPSG:3857',\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.000 0.000, 0.000 2.000, 1.000 1.00...\n",
      " |      1    POLYGON ((0.000 2.000, 0.000 1.000, 2.000 0.00...\n",
      " |      2    LINESTRING (0.000 0.000, 1.000 1.000, 1.000 0....\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.make_valid()\n",
      " |      0    MULTIPOLYGON (((1.000 1.000, 0.000 0.000, 0.00...\n",
      " |      1    GEOMETRYCOLLECTION (POLYGON ((2.000 0.000, 0.0...\n",
      " |      2    LINESTRING (0.000 0.000, 1.000 1.000, 1.000 0....\n",
      " |      dtype: geometry\n",
      " |\n",
      " |  minimum_bounding_circle(self)\n",
      " |      Returns a ``GeoSeries`` of geometries representing the minimum bounding\n",
      " |      circle that encloses each geometry.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1), (0, 0)]),\n",
      " |      ...         LineString([(0, 0), (1, 1), (1, 0)]),\n",
      " |      ...         Point(0, 0),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 0....\n",
      " |      1    LINESTRING (0.00000 0.00000, 1.00000 1.00000, ...\n",
      " |      2                              POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.minimum_bounding_circle()\n",
      " |      0    POLYGON ((1.20711 0.50000, 1.19352 0.36205, 1....\n",
      " |      1    POLYGON ((1.20711 0.50000, 1.19352 0.36205, 1....\n",
      " |      2                              POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.convex_hull : convex hull geometry\n",
      " |\n",
      " |  minimum_bounding_radius(self)\n",
      " |      Returns a `Series` of the radii of the minimum bounding circles\n",
      " |      that enclose each geometry.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Point, LineString, Polygon\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1), (0, 0)]),\n",
      " |      ...         LineString([(0, 0), (1, 1), (1, 0)]),\n",
      " |      ...         Point(0,0),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 0....\n",
      " |      1    LINESTRING (0.00000 0.00000, 1.00000 1.00000, ...\n",
      " |      2                              POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |      >>> s.minimum_bounding_radius()\n",
      " |      0    0.707107\n",
      " |      1    0.707107\n",
      " |      2    0.000000\n",
      " |      dtype: float64\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.minumum_bounding_circle : minimum bounding circle (geometry)\n",
      " |\n",
      " |  minimum_rotated_rectangle(self)\n",
      " |      Returns a ``GeoSeries`` of the general minimum bounding rectangle\n",
      " |      that contains the object.\n",
      " |\n",
      " |      Unlike envelope this rectangle is not constrained to be parallel\n",
      " |      to the coordinate axes. If the convex hull of the object is a\n",
      " |      degenerate (line or point) this degenerate is returned.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point, MultiPoint\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         LineString([(0, 0), (1, 1), (1, 0)]),\n",
      " |      ...         MultiPoint([(0, 0), (1, 1)]),\n",
      " |      ...         Point(0, 0),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 0....\n",
      " |      1    LINESTRING (0.00000 0.00000, 1.00000 1.00000, ...\n",
      " |      2        MULTIPOINT (0.00000 0.00000, 1.00000 1.00000)\n",
      " |      3                              POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.minimum_rotated_rectangle()\n",
      " |      0    POLYGON ((1.00000 1.00000, 0.50000 1.50000, -0...\n",
      " |      1    POLYGON ((0.00000 0.00000, 0.50000 -0.50000, 1...\n",
      " |      2        LINESTRING (0.00000 0.00000, 1.00000 1.00000)\n",
      " |      3                              POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.envelope : bounding rectangle\n",
      " |\n",
      " |  normalize(self)\n",
      " |      Returns a ``GeoSeries`` of normalized\n",
      " |      geometries to normal form (or canonical form).\n",
      " |\n",
      " |      This method orders the coordinates, rings of a polygon and parts of\n",
      " |      multi geometries consistently. Typically useful for testing purposes\n",
      " |      (for example in combination with `equals_exact`).\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         LineString([(0, 0), (1, 1), (1, 0)]),\n",
      " |      ...         Point(0, 0),\n",
      " |      ...     ],\n",
      " |      ...     crs='EPSG:3857'\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.000 0.000, 1.000 1.000, 0.000 1.00...\n",
      " |      1    LINESTRING (0.000 0.000, 1.000 1.000, 1.000 0....\n",
      " |      2                                  POINT (0.000 0.000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.normalize()\n",
      " |      0    POLYGON ((0.000 0.000, 0.000 1.000, 1.000 1.00...\n",
      " |      1    LINESTRING (0.000 0.000, 1.000 1.000, 1.000 0....\n",
      " |      2                                  POINT (0.000 0.000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |  offset_curve(self, distance, quad_segs=8, join_style='round', mitre_limit=5.0)\n",
      " |      Returns a ``LineString`` or ``MultiLineString`` geometry at a\n",
      " |      distance from the object on its right or its left side.\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      distance : float | array-like\n",
      " |          Specifies the offset distance from the input geometry. Negative\n",
      " |          for right side offset, positive for left side offset.\n",
      " |      quad_segs : int (optional, default 8)\n",
      " |          Specifies the number of linear segments in a quarter circle in the\n",
      " |          approximation of circular arcs.\n",
      " |      join_style : {'round', 'bevel', 'mitre'}, (optional, default 'round')\n",
      " |          Specifies the shape of outside corners. 'round' results in\n",
      " |          rounded shapes. 'bevel' results in a beveled edge that touches the\n",
      " |          original vertex. 'mitre' results in a single vertex that is beveled\n",
      " |          depending on the ``mitre_limit`` parameter.\n",
      " |      mitre_limit : float (optional, default 5.0)\n",
      " |          Crops of 'mitre'-style joins if the point is displaced from the\n",
      " |          buffered vertex by more than this limit.\n",
      " |\n",
      " |      See http://shapely.readthedocs.io/en/latest/manual.html#object.offset_curve\n",
      " |      for details.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely.geometry import LineString\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         LineString([(0, 0), (0, 1), (1, 1)]),\n",
      " |      ...     ],\n",
      " |      ...     crs=3857\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    LINESTRING (0.000 0.000, 0.000 1.000, 1.000 1....\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.offset_curve(1)\n",
      " |      0    LINESTRING (-1.000 0.000, -1.000 1.000, -0.981...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |  overlaps(self, other, align=True)\n",
      " |      Returns True for all aligned geometries that overlap *other*, else False.\n",
      " |\n",
      " |      Geometries overlaps if they have more than one but not all\n",
      " |      points in common, have the same dimension, and the intersection of the\n",
      " |      interiors of the geometries has the same dimension as the geometries\n",
      " |      themselves.\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : GeoSeries or geometric object\n",
      " |          The GeoSeries (elementwise) or geometric object to test if\n",
      " |          overlaps.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series (bool)\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon, LineString, MultiPoint, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         LineString([(0, 0), (2, 2)]),\n",
      " |      ...         MultiPoint([(0, 0), (0, 1)]),\n",
      " |      ...     ],\n",
      " |      ... )\n",
      " |      >>> s2 = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (2, 0), (0, 2)]),\n",
      " |      ...         LineString([(0, 1), (1, 1)]),\n",
      " |      ...         LineString([(1, 1), (3, 3)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ...     index=range(1, 5),\n",
      " |      ... )\n",
      " |\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      1    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      2        LINESTRING (0.00000 0.00000, 2.00000 2.00000)\n",
      " |      3        MULTIPOINT (0.00000 0.00000, 0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s2\n",
      " |      1    POLYGON ((0.00000 0.00000, 2.00000 0.00000, 0....\n",
      " |      2        LINESTRING (0.00000 1.00000, 1.00000 1.00000)\n",
      " |      3        LINESTRING (1.00000 1.00000, 3.00000 3.00000)\n",
      " |      4                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can check if each geometry of GeoSeries overlaps a single\n",
      " |      geometry:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-03.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      >>> polygon = Polygon([(0, 0), (1, 0), (1, 1), (0, 1)])\n",
      " |      >>> s.overlaps(polygon)\n",
      " |      0     True\n",
      " |      1     True\n",
      " |      2    False\n",
      " |      3    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      We can also check two GeoSeries against each other, row by row.\n",
      " |      The GeoSeries above have different indices. We can either align both GeoSeries\n",
      " |      based on index values and compare elements with the same index using\n",
      " |      ``align=True`` or ignore index and compare elements based on their matching\n",
      " |      order using ``align=False``:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-02.svg\n",
      " |\n",
      " |      >>> s.overlaps(s2)\n",
      " |      0    False\n",
      " |      1     True\n",
      " |      2    False\n",
      " |      3    False\n",
      " |      4    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      >>> s.overlaps(s2, align=False)\n",
      " |      0     True\n",
      " |      1    False\n",
      " |      2     True\n",
      " |      3    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method works in a row-wise manner. It does not check if an element\n",
      " |      of one GeoSeries ``overlaps`` *any* element of the other one.\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.crosses\n",
      " |      GeoSeries.intersects\n",
      " |\n",
      " |  project(self, other, normalized=False, align=True)\n",
      " |      Return the distance along each geometry nearest to *other*\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      The project method is the inverse of interpolate.\n",
      " |\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : BaseGeometry or GeoSeries\n",
      " |          The *other* geometry to computed projected point from.\n",
      " |      normalized : boolean\n",
      " |          If normalized is True, return the distance normalized to\n",
      " |          the length of the object.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         LineString([(0, 0), (2, 0), (0, 2)]),\n",
      " |      ...         LineString([(0, 0), (2, 2)]),\n",
      " |      ...         LineString([(2, 0), (0, 2)]),\n",
      " |      ...     ],\n",
      " |      ... )\n",
      " |      >>> s2 = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Point(1, 0),\n",
      " |      ...         Point(1, 0),\n",
      " |      ...         Point(2, 1),\n",
      " |      ...     ],\n",
      " |      ...     index=range(1, 4),\n",
      " |      ... )\n",
      " |\n",
      " |      >>> s\n",
      " |      0    LINESTRING (0.00000 0.00000, 2.00000 0.00000, ...\n",
      " |      1        LINESTRING (0.00000 0.00000, 2.00000 2.00000)\n",
      " |      2        LINESTRING (2.00000 0.00000, 0.00000 2.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s2\n",
      " |      1    POINT (1.00000 0.00000)\n",
      " |      2    POINT (1.00000 0.00000)\n",
      " |      3    POINT (2.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can project each geometry on a single\n",
      " |      shapely geometry:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-03.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      >>> s.project(Point(1, 0))\n",
      " |      0    1.000000\n",
      " |      1    0.707107\n",
      " |      2    0.707107\n",
      " |      dtype: float64\n",
      " |\n",
      " |      We can also check two GeoSeries against each other, row by row.\n",
      " |      The GeoSeries above have different indices. We can either align both GeoSeries\n",
      " |      based on index values and project elements with the same index using\n",
      " |      ``align=True`` or ignore index and project elements based on their matching\n",
      " |      order using ``align=False``:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-02.svg\n",
      " |\n",
      " |      >>> s.project(s2, align=True)\n",
      " |      0         NaN\n",
      " |      1    0.707107\n",
      " |      2    0.707107\n",
      " |      3         NaN\n",
      " |      dtype: float64\n",
      " |\n",
      " |      >>> s.project(s2, align=False)\n",
      " |      0    1.000000\n",
      " |      1    0.707107\n",
      " |      2    0.707107\n",
      " |      dtype: float64\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.interpolate\n",
      " |\n",
      " |  relate(self, other, align=True)\n",
      " |      Returns the DE-9IM intersection matrices for the geometries\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : BaseGeometry or GeoSeries\n",
      " |          The other geometry to computed\n",
      " |          the DE-9IM intersection matrices from.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      spatial_relations: Series of strings\n",
      " |          The DE-9IM intersection matrices which describe\n",
      " |          the spatial relations of the other geometry.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         LineString([(0, 0), (2, 2)]),\n",
      " |      ...         LineString([(2, 0), (0, 2)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ... )\n",
      " |      >>> s2 = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         LineString([(1, 0), (1, 3)]),\n",
      " |      ...         LineString([(2, 0), (0, 2)]),\n",
      " |      ...         Point(1, 1),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ...     index=range(1, 6),\n",
      " |      ... )\n",
      " |\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      1    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      2        LINESTRING (0.00000 0.00000, 2.00000 2.00000)\n",
      " |      3        LINESTRING (2.00000 0.00000, 0.00000 2.00000)\n",
      " |      4                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s2\n",
      " |      1    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 0....\n",
      " |      2        LINESTRING (1.00000 0.00000, 1.00000 3.00000)\n",
      " |      3        LINESTRING (2.00000 0.00000, 0.00000 2.00000)\n",
      " |      4                              POINT (1.00000 1.00000)\n",
      " |      5                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can relate each geometry and a single\n",
      " |      shapely geometry:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-03.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      >>> s.relate(Polygon([(0, 0), (1, 1), (0, 1)]))\n",
      " |      0    212F11FF2\n",
      " |      1    212F11FF2\n",
      " |      2    F11F00212\n",
      " |      3    F01FF0212\n",
      " |      4    F0FFFF212\n",
      " |      dtype: object\n",
      " |\n",
      " |      We can also check two GeoSeries against each other, row by row.\n",
      " |      The GeoSeries above have different indices. We can either align both GeoSeries\n",
      " |      based on index values and compare elements with the same index using\n",
      " |      ``align=True`` or ignore index and compare elements based on their matching\n",
      " |      order using ``align=False``:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-02.svg\n",
      " |\n",
      " |      >>> s.relate(s2, align=True)\n",
      " |      0         None\n",
      " |      1    212F11FF2\n",
      " |      2    0F1FF0102\n",
      " |      3    1FFF0FFF2\n",
      " |      4    FF0FFF0F2\n",
      " |      5         None\n",
      " |      dtype: object\n",
      " |\n",
      " |      >>> s.relate(s2, align=False)\n",
      " |      0    212F11FF2\n",
      " |      1    1F20F1102\n",
      " |      2    0F1FF0102\n",
      " |      3    0F1FF0FF2\n",
      " |      4    0FFFFFFF2\n",
      " |      dtype: object\n",
      " |\n",
      " |  remove_repeated_points(self, tolerance=0.0)\n",
      " |      Returns a ``GeoSeries`` containing a copy of the input geometry\n",
      " |      with repeated points removed.\n",
      " |\n",
      " |      From the start of the coordinate sequence, each next point within the\n",
      " |      tolerance is removed.\n",
      " |\n",
      " |      Removing repeated points with a non-zero tolerance may result in an invalid\n",
      " |      geometry being returned.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tolerance : float, default 0.0\n",
      " |          Remove all points within this distance of each other. Use 0.0\n",
      " |          to remove only exactly repeated points (the default).\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely import LineString, Polygon\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...        LineString([(0, 0), (0, 0), (1, 0)]),\n",
      " |      ...        Polygon([(0, 0), (0, 0.5), (0, 1), (0.5, 1), (0,0)]),\n",
      " |      ...     ],\n",
      " |      ...     crs=3857\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    LINESTRING (0.000 0.000, 0.000 0.000, 1.000 0....\n",
      " |      1    POLYGON ((0.000 0.000, 0.000 0.500, 0.000 1.00...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.remove_repeated_points(tolerance=0.0)\n",
      " |      0                LINESTRING (0.000 0.000, 1.000 0.000)\n",
      " |      1    POLYGON ((0.000 0.000, 0.000 0.500, 0.000 1.00...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |  representative_point(self)\n",
      " |      Returns a ``GeoSeries`` of (cheaply computed) points that are\n",
      " |      guaranteed to be within each geometry.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         LineString([(0, 0), (1, 1), (1, 0)]),\n",
      " |      ...         Point(0, 0),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 0....\n",
      " |      1    LINESTRING (0.00000 0.00000, 1.00000 1.00000, ...\n",
      " |      2                              POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.representative_point()\n",
      " |      0    POINT (0.25000 0.50000)\n",
      " |      1    POINT (1.00000 1.00000)\n",
      " |      2    POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.centroid : geometric centroid\n",
      " |\n",
      " |  reverse(self)\n",
      " |      Returns a ``GeoSeries`` with the order of coordinates reversed.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         LineString([(0, 0), (1, 1), (1, 0)]),\n",
      " |      ...         Point(0, 0),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 0....\n",
      " |      1    LINESTRING (0.00000 0.00000, 1.00000 1.00000, ...\n",
      " |      2                              POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.reverse()\n",
      " |      0    POLYGON ((0.00000 0.00000, 0.00000 1.00000, 1....\n",
      " |      1    LINESTRING (1.00000 0.00000, 1.00000 1.00000, ...\n",
      " |      2    POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.normalize : normalize order of coordinates\n",
      " |\n",
      " |  rotate(self, angle, origin='center', use_radians=False)\n",
      " |      Returns a ``GeoSeries`` with rotated geometries.\n",
      " |\n",
      " |      See http://shapely.readthedocs.io/en/latest/manual.html#shapely.affinity.rotate\n",
      " |      for details.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      angle : float\n",
      " |          The angle of rotation can be specified in either degrees (default)\n",
      " |          or radians by setting use_radians=True. Positive angles are\n",
      " |          counter-clockwise and negative are clockwise rotations.\n",
      " |      origin : string, Point, or tuple (x, y)\n",
      " |          The point of origin can be a keyword 'center' for the bounding box\n",
      " |          center (default), 'centroid' for the geometry's centroid, a Point\n",
      " |          object or a coordinate tuple (x, y).\n",
      " |      use_radians : boolean\n",
      " |          Whether to interpret the angle of rotation as degrees or radians\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Point, LineString, Polygon\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Point(1, 1),\n",
      " |      ...         LineString([(1, -1), (1, 0)]),\n",
      " |      ...         Polygon([(3, -1), (4, 0), (3, 1)]),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0                              POINT (1.00000 1.00000)\n",
      " |      1       LINESTRING (1.00000 -1.00000, 1.00000 0.00000)\n",
      " |      2    POLYGON ((3.00000 -1.00000, 4.00000 0.00000, 3...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.rotate(90)\n",
      " |      0                              POINT (1.00000 1.00000)\n",
      " |      1      LINESTRING (1.50000 -0.50000, 0.50000 -0.50000)\n",
      " |      2    POLYGON ((4.50000 -0.50000, 3.50000 0.50000, 2...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.rotate(90, origin=(0, 0))\n",
      " |      0                             POINT (-1.00000 1.00000)\n",
      " |      1        LINESTRING (1.00000 1.00000, 0.00000 1.00000)\n",
      " |      2    POLYGON ((1.00000 3.00000, 0.00000 4.00000, -1...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |  sample_points(self, size, method='uniform', seed=None, rng=None, **kwargs)\n",
      " |      Sample points from each geometry.\n",
      " |\n",
      " |      Generate a MultiPoint per each geometry containing points sampled from the\n",
      " |      geometry. You can either sample randomly from a uniform distribution or use an\n",
      " |      advanced sampling algorithm from the ``pointpats`` package.\n",
      " |\n",
      " |      For polygons, this samples within the area of the polygon. For lines,\n",
      " |      this samples along the length of the linestring. For multi-part\n",
      " |      geometries, the weights of each part are selected according to their relevant\n",
      " |      attribute (area for Polygons, length for LineStrings), and then points are\n",
      " |      sampled from each part.\n",
      " |\n",
      " |      Any other geometry type (e.g. Point, GeometryCollection) is ignored, and an\n",
      " |      empty MultiPoint geometry is returned.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      size : int | array-like\n",
      " |          The size of the sample requested. Indicates the number of samples to draw\n",
      " |          from each geometry.  If an array of the same length as a GeoSeries is\n",
      " |          passed, it denotes the size of a sample per geometry.\n",
      " |      method : str, default \"uniform\"\n",
      " |          The sampling method. ``uniform`` samples uniformly at random from a\n",
      " |          geometry using ``numpy.random.uniform``. Other allowed strings\n",
      " |          (e.g. ``\"cluster_poisson\"``) denote sampling function name from the\n",
      " |          ``pointpats.random`` module (see\n",
      " |          http://pysal.org/pointpats/api.html#random-distributions). Pointpats methods\n",
      " |          are implemented for (Multi)Polygons only and will return an empty MultiPoint\n",
      " |          for other geometry types.\n",
      " |      rng : {None, int, array_like[ints], SeedSequence, BitGenerator, Generator}, optional\n",
      " |          A random generator or seed to initialize the numpy BitGenerator. If None, then fresh,\n",
      " |          unpredictable entropy will be pulled from the OS.\n",
      " |      **kwargs : dict\n",
      " |          Options for the pointpats sampling algorithms.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      GeoSeries\n",
      " |          Points sampled within (or along) each geometry.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(1, -1), (1, 0), (0, 0)]),\n",
      " |      ...         Polygon([(3, -1), (4, 0), (3, 1)]),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |\n",
      " |      >>> s.sample_points(size=10)  # doctest: +SKIP\n",
      " |      0    MULTIPOINT (0.04783 -0.04244, 0.24196 -0.09052...\n",
      " |      1    MULTIPOINT (3.00672 -0.52390, 3.01776 0.30065,...\n",
      " |      Name: sampled_points, dtype: geometry\n",
      " |\n",
      " |  scale(self, xfact=1.0, yfact=1.0, zfact=1.0, origin='center')\n",
      " |      Returns a ``GeoSeries`` with scaled geometries.\n",
      " |\n",
      " |      The geometries can be scaled by different factors along each\n",
      " |      dimension. Negative scale factors will mirror or reflect coordinates.\n",
      " |\n",
      " |      See http://shapely.readthedocs.io/en/latest/manual.html#shapely.affinity.scale\n",
      " |      for details.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      xfact, yfact, zfact : float, float, float\n",
      " |          Scaling factors for the x, y, and z dimensions respectively.\n",
      " |      origin : string, Point, or tuple\n",
      " |          The point of origin can be a keyword 'center' for the 2D bounding\n",
      " |          box center (default), 'centroid' for the geometry's 2D centroid, a\n",
      " |          Point object or a coordinate tuple (x, y, z).\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Point, LineString, Polygon\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Point(1, 1),\n",
      " |      ...         LineString([(1, -1), (1, 0)]),\n",
      " |      ...         Polygon([(3, -1), (4, 0), (3, 1)]),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0                              POINT (1.00000 1.00000)\n",
      " |      1       LINESTRING (1.00000 -1.00000, 1.00000 0.00000)\n",
      " |      2    POLYGON ((3.00000 -1.00000, 4.00000 0.00000, 3...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.scale(2, 3)\n",
      " |      0                              POINT (1.00000 1.00000)\n",
      " |      1       LINESTRING (1.00000 -2.00000, 1.00000 1.00000)\n",
      " |      2    POLYGON ((2.50000 -3.00000, 4.50000 0.00000, 2...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.scale(2, 3, origin=(0, 0))\n",
      " |      0                              POINT (2.00000 3.00000)\n",
      " |      1       LINESTRING (2.00000 -3.00000, 2.00000 0.00000)\n",
      " |      2    POLYGON ((6.00000 -3.00000, 8.00000 0.00000, 6...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |  segmentize(self, max_segment_length)\n",
      " |      Returns a ``GeoSeries`` with vertices added to line segments based on\n",
      " |      maximum segment length.\n",
      " |\n",
      " |      Additional vertices will be added to every line segment in an input geometry so\n",
      " |      that segments are no longer than the provided maximum segment length. New\n",
      " |      vertices will evenly subdivide each segment. Only linear components of input\n",
      " |      geometries are densified; other geometries are returned unmodified.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      max_segment_length : float | array-like\n",
      " |          Additional vertices will be added so that all line segments are no longer\n",
      " |          than this value. Must be greater than 0.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      GeoSeries\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon, LineString\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         LineString([(0, 0), (0, 10)]),\n",
      " |      ...         Polygon([(0, 0), (10, 0), (10, 10), (0, 10), (0, 0)]),\n",
      " |      ...     ],\n",
      " |      ...     crs=3857\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0               LINESTRING (0.000 0.000, 0.000 10.000)\n",
      " |      1    POLYGON ((0.000 0.000, 10.000 0.000, 10.000 10...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.segmentize(max_segment_length=5)\n",
      " |      0    LINESTRING (0.000 0.000, 0.000 5.000, 0.000 10...\n",
      " |      1    POLYGON ((0.000 0.000, 5.000 0.000, 10.000 0.0...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |  shortest_line(self, other, align=True)\n",
      " |      Returns the shortest two-point line between two geometries.\n",
      " |\n",
      " |      The resulting line consists of two points, representing the nearest points\n",
      " |      between the geometry pair. The line always starts in the first geometry a\n",
      " |      and ends in he second geometry b. The endpoints of the line will not\n",
      " |      necessarily be existing vertices of the input geometries a and b, but\n",
      " |      can also be a point along a line segment.\n",
      " |\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |      :align: center\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Geoseries or geometric object\n",
      " |          The Geoseries (elementwise) or geometric object to find the\n",
      " |          shortest line with.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      GeoSeries\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         LineString([(0, 0), (2, 2)]),\n",
      " |      ...         LineString([(2, 0), (0, 2)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ...     crs=5514\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.000 0.000, 2.000 2.000, 0.000 2.00...\n",
      " |      1    POLYGON ((0.000 0.000, 2.000 2.000, 0.000 2.00...\n",
      " |      2                LINESTRING (0.000 0.000, 2.000 2.000)\n",
      " |      3                LINESTRING (2.000 0.000, 0.000 2.000)\n",
      " |      4                                  POINT (0.000 1.000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can also do intersection of each geometry and a single\n",
      " |      shapely geometry:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-03.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      >>> p = Point(3, 3)\n",
      " |      >>> s.shortest_line(p)\n",
      " |      0    LINESTRING (2.000 2.000, 3.000 3.000)\n",
      " |      1    LINESTRING (2.000 2.000, 3.000 3.000)\n",
      " |      2    LINESTRING (2.000 2.000, 3.000 3.000)\n",
      " |      3    LINESTRING (1.000 1.000, 3.000 3.000)\n",
      " |      4    LINESTRING (0.000 1.000, 3.000 3.000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can also check two GeoSeries against each other, row by row.\n",
      " |      The GeoSeries above have different indices than the one below. We can either\n",
      " |      align both GeoSeries based on index values and compare elements with the same\n",
      " |      index using ``align=True`` or ignore index and compare elements based on their\n",
      " |      matching order using ``align=False``:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-02.svg\n",
      " |\n",
      " |      >>> s2 = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0.5, 0.5), (1.5, 0.5), (1.5, 1.5), (0.5, 1.5)]),\n",
      " |      ...         Point(3, 1),\n",
      " |      ...         LineString([(1, 0), (2, 0)]),\n",
      " |      ...         Point(10, 15),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ...     index=range(1, 6),\n",
      " |      ...     crs=5514,\n",
      " |      ... )\n",
      " |      >>> s.shortest_line(s2, align=True)\n",
      " |      0                                       None\n",
      " |      1      LINESTRING (0.500 0.500, 0.500 0.500)\n",
      " |      2      LINESTRING (2.000 2.000, 3.000 1.000)\n",
      " |      3      LINESTRING (2.000 0.000, 2.000 0.000)\n",
      " |      4    LINESTRING (0.000 1.000, 10.000 15.000)\n",
      " |      5                                       None\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.shortest_line(s2, align=False)\n",
      " |      0      LINESTRING (0.500 0.500, 0.500 0.500)\n",
      " |      1      LINESTRING (2.000 2.000, 3.000 1.000)\n",
      " |      2      LINESTRING (0.500 0.500, 1.000 0.000)\n",
      " |      3    LINESTRING (0.000 2.000, 10.000 15.000)\n",
      " |      4      LINESTRING (0.000 1.000, 0.000 1.000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |  simplify(self, *args, **kwargs)\n",
      " |      Returns a ``GeoSeries`` containing a simplified representation of\n",
      " |      each geometry.\n",
      " |\n",
      " |      The algorithm (Douglas-Peucker) recursively splits the original line\n",
      " |      into smaller parts and connects these parts’ endpoints\n",
      " |      by a straight line. Then, it removes all points whose distance\n",
      " |      to the straight line is smaller than `tolerance`. It does not\n",
      " |      move any points and it always preserves endpoints of\n",
      " |      the original line or polygon.\n",
      " |      See http://shapely.readthedocs.io/en/latest/manual.html#object.simplify\n",
      " |      for details\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tolerance : float\n",
      " |          All parts of a simplified geometry will be no more than\n",
      " |          `tolerance` distance from the original. It has the same units\n",
      " |          as the coordinate reference system of the GeoSeries.\n",
      " |          For example, using `tolerance=100` in a projected CRS with meters\n",
      " |          as units means a distance of 100 meters in reality.\n",
      " |      preserve_topology: bool (default True)\n",
      " |          False uses a quicker algorithm, but may produce self-intersecting\n",
      " |          or otherwise invalid geometries.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Invalid geometric objects may result from simplification that does not\n",
      " |      preserve topology and simplification may be sensitive to the order of\n",
      " |      coordinates: two geometries differing only in order of coordinates may be\n",
      " |      simplified differently.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Point, LineString\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [Point(0, 0).buffer(1), LineString([(0, 0), (1, 10), (0, 20)])]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    POLYGON ((1.00000 0.00000, 0.99518 -0.09802, 0...\n",
      " |      1    LINESTRING (0.00000 0.00000, 1.00000 10.00000,...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.simplify(1)\n",
      " |      0    POLYGON ((1.00000 0.00000, 0.00000 -1.00000, -...\n",
      " |      1       LINESTRING (0.00000 0.00000, 0.00000 20.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |  skew(self, xs=0.0, ys=0.0, origin='center', use_radians=False)\n",
      " |      Returns a ``GeoSeries`` with skewed geometries.\n",
      " |\n",
      " |      The geometries are sheared by angles along the x and y dimensions.\n",
      " |\n",
      " |      See http://shapely.readthedocs.io/en/latest/manual.html#shapely.affinity.skew\n",
      " |      for details.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      xs, ys : float, float\n",
      " |          The shear angle(s) for the x and y axes respectively. These can be\n",
      " |          specified in either degrees (default) or radians by setting\n",
      " |          use_radians=True.\n",
      " |      origin : string, Point, or tuple (x, y)\n",
      " |          The point of origin can be a keyword 'center' for the bounding box\n",
      " |          center (default), 'centroid' for the geometry's centroid, a Point\n",
      " |          object or a coordinate tuple (x, y).\n",
      " |      use_radians : boolean\n",
      " |          Whether to interpret the shear angle(s) as degrees or radians\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Point, LineString, Polygon\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Point(1, 1),\n",
      " |      ...         LineString([(1, -1), (1, 0)]),\n",
      " |      ...         Polygon([(3, -1), (4, 0), (3, 1)]),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0                              POINT (1.00000 1.00000)\n",
      " |      1       LINESTRING (1.00000 -1.00000, 1.00000 0.00000)\n",
      " |      2    POLYGON ((3.00000 -1.00000, 4.00000 0.00000, 3...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.skew(45, 30)\n",
      " |      0                              POINT (1.00000 1.00000)\n",
      " |      1       LINESTRING (0.50000 -1.00000, 1.50000 0.00000)\n",
      " |      2    POLYGON ((2.00000 -1.28868, 4.00000 0.28868, 4...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.skew(45, 30, origin=(0, 0))\n",
      " |      0                              POINT (2.00000 1.57735)\n",
      " |      1       LINESTRING (0.00000 -0.42265, 1.00000 0.57735)\n",
      " |      2    POLYGON ((2.00000 0.73205, 4.00000 2.30940, 4....\n",
      " |      dtype: geometry\n",
      " |\n",
      " |  symmetric_difference(self, other, align=True)\n",
      " |      Returns a ``GeoSeries`` of the symmetric difference of points in\n",
      " |      each aligned geometry with `other`.\n",
      " |\n",
      " |      For each geometry, the symmetric difference consists of points in the\n",
      " |      geometry not in `other`, and points in `other` not in the geometry.\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_geo-symm_diff.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |         :align: center\n",
      " |\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Geoseries or geometric object\n",
      " |          The Geoseries (elementwise) or geometric object to find the\n",
      " |          symmetric difference to.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      GeoSeries\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         LineString([(0, 0), (2, 2)]),\n",
      " |      ...         LineString([(2, 0), (0, 2)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ... )\n",
      " |      >>> s2 = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         LineString([(1, 0), (1, 3)]),\n",
      " |      ...         LineString([(2, 0), (0, 2)]),\n",
      " |      ...         Point(1, 1),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ...     index=range(1, 6),\n",
      " |      ... )\n",
      " |\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      1    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      2        LINESTRING (0.00000 0.00000, 2.00000 2.00000)\n",
      " |      3        LINESTRING (2.00000 0.00000, 0.00000 2.00000)\n",
      " |      4                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s2\n",
      " |      1    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 0....\n",
      " |      2        LINESTRING (1.00000 0.00000, 1.00000 3.00000)\n",
      " |      3        LINESTRING (2.00000 0.00000, 0.00000 2.00000)\n",
      " |      4                              POINT (1.00000 1.00000)\n",
      " |      5                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can do symmetric difference of each geometry and a single\n",
      " |      shapely geometry:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-03.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      >>> s.symmetric_difference(Polygon([(0, 0), (1, 1), (0, 1)]))\n",
      " |      0    POLYGON ((0.00000 2.00000, 2.00000 2.00000, 1....\n",
      " |      1    POLYGON ((0.00000 2.00000, 2.00000 2.00000, 1....\n",
      " |      2    GEOMETRYCOLLECTION (POLYGON ((0.00000 0.00000,...\n",
      " |      3    GEOMETRYCOLLECTION (POLYGON ((0.00000 0.00000,...\n",
      " |      4    POLYGON ((0.00000 1.00000, 1.00000 1.00000, 0....\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can also check two GeoSeries against each other, row by row.\n",
      " |      The GeoSeries above have different indices. We can either align both GeoSeries\n",
      " |      based on index values and compare elements with the same index using\n",
      " |      ``align=True`` or ignore index and compare elements based on their matching\n",
      " |      order using ``align=False``:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-02.svg\n",
      " |\n",
      " |      >>> s.symmetric_difference(s2, align=True)\n",
      " |      0                                                 None\n",
      " |      1    POLYGON ((0.00000 2.00000, 2.00000 2.00000, 1....\n",
      " |      2    MULTILINESTRING ((0.00000 0.00000, 1.00000 1.0...\n",
      " |      3                                   LINESTRING Z EMPTY\n",
      " |      4        MULTIPOINT (0.00000 1.00000, 1.00000 1.00000)\n",
      " |      5                                                 None\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.symmetric_difference(s2, align=False)\n",
      " |      0    POLYGON ((0.00000 2.00000, 2.00000 2.00000, 1....\n",
      " |      1    GEOMETRYCOLLECTION (POLYGON ((0.00000 0.00000,...\n",
      " |      2    MULTILINESTRING ((0.00000 0.00000, 1.00000 1.0...\n",
      " |      3        LINESTRING (2.00000 0.00000, 0.00000 2.00000)\n",
      " |      4                                          POINT EMPTY\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      GeoSeries.difference\n",
      " |      GeoSeries.union\n",
      " |      GeoSeries.intersection\n",
      " |\n",
      " |  touches(self, other, align=True)\n",
      " |      Returns a ``Series`` of ``dtype('bool')`` with value ``True`` for\n",
      " |      each aligned geometry that touches `other`.\n",
      " |\n",
      " |      An object is said to touch `other` if it has at least one point in\n",
      " |      common with `other` and its interior does not intersect with any part\n",
      " |      of the other. Overlapping features therefore do not touch.\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : GeoSeries or geometric object\n",
      " |          The GeoSeries (elementwise) or geometric object to test if is\n",
      " |          touched.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series (bool)\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon, LineString, MultiPoint, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         LineString([(0, 0), (2, 2)]),\n",
      " |      ...         MultiPoint([(0, 0), (0, 1)]),\n",
      " |      ...     ],\n",
      " |      ... )\n",
      " |      >>> s2 = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (-2, 0), (0, -2)]),\n",
      " |      ...         LineString([(0, 1), (1, 1)]),\n",
      " |      ...         LineString([(1, 1), (3, 0)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ...     index=range(1, 5),\n",
      " |      ... )\n",
      " |\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      1    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      2        LINESTRING (0.00000 0.00000, 2.00000 2.00000)\n",
      " |      3        MULTIPOINT (0.00000 0.00000, 0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s2\n",
      " |      1    POLYGON ((0.00000 0.00000, -2.00000 0.00000, 0...\n",
      " |      2        LINESTRING (0.00000 1.00000, 1.00000 1.00000)\n",
      " |      3        LINESTRING (1.00000 1.00000, 3.00000 0.00000)\n",
      " |      4                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can check if each geometry of GeoSeries touches a single\n",
      " |      geometry:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-03.svg\n",
      " |         :align: center\n",
      " |\n",
      " |\n",
      " |      >>> line = LineString([(0, 0), (-1, -2)])\n",
      " |      >>> s.touches(line)\n",
      " |      0    True\n",
      " |      1    True\n",
      " |      2    True\n",
      " |      3    True\n",
      " |      dtype: bool\n",
      " |\n",
      " |      We can also check two GeoSeries against each other, row by row.\n",
      " |      The GeoSeries above have different indices. We can either align both GeoSeries\n",
      " |      based on index values and compare elements with the same index using\n",
      " |      ``align=True`` or ignore index and compare elements based on their matching\n",
      " |      order using ``align=False``:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-02.svg\n",
      " |\n",
      " |      >>> s.touches(s2, align=True)\n",
      " |      0    False\n",
      " |      1     True\n",
      " |      2     True\n",
      " |      3    False\n",
      " |      4    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      >>> s.touches(s2, align=False)\n",
      " |      0     True\n",
      " |      1    False\n",
      " |      2     True\n",
      " |      3    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method works in a row-wise manner. It does not check if an element\n",
      " |      of one GeoSeries ``touches`` *any* element of the other one.\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.overlaps\n",
      " |      GeoSeries.intersects\n",
      " |\n",
      " |  translate(self, xoff=0.0, yoff=0.0, zoff=0.0)\n",
      " |      Returns a ``GeoSeries`` with translated geometries.\n",
      " |\n",
      " |      See http://shapely.readthedocs.io/en/latest/manual.html#shapely.affinity.translate\n",
      " |      for details.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      xoff, yoff, zoff : float, float, float\n",
      " |          Amount of offset along each dimension.\n",
      " |          xoff, yoff, and zoff for translation along the x, y, and z\n",
      " |          dimensions respectively.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Point, LineString, Polygon\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Point(1, 1),\n",
      " |      ...         LineString([(1, -1), (1, 0)]),\n",
      " |      ...         Polygon([(3, -1), (4, 0), (3, 1)]),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0                              POINT (1.00000 1.00000)\n",
      " |      1       LINESTRING (1.00000 -1.00000, 1.00000 0.00000)\n",
      " |      2    POLYGON ((3.00000 -1.00000, 4.00000 0.00000, 3...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.translate(2, 3)\n",
      " |      0                              POINT (3.00000 4.00000)\n",
      " |      1        LINESTRING (3.00000 2.00000, 3.00000 3.00000)\n",
      " |      2    POLYGON ((5.00000 2.00000, 6.00000 3.00000, 5....\n",
      " |      dtype: geometry\n",
      " |\n",
      " |  union(self, other, align=True)\n",
      " |      Returns a ``GeoSeries`` of the union of points in each aligned geometry with\n",
      " |      `other`.\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_geo-union.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |         :align: center\n",
      " |\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Geoseries or geometric object\n",
      " |          The Geoseries (elementwise) or geometric object to find the union\n",
      " |          with.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      GeoSeries\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         LineString([(0, 0), (2, 2)]),\n",
      " |      ...         LineString([(2, 0), (0, 2)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ... )\n",
      " |      >>> s2 = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         LineString([(1, 0), (1, 3)]),\n",
      " |      ...         LineString([(2, 0), (0, 2)]),\n",
      " |      ...         Point(1, 1),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ...     index=range(1, 6),\n",
      " |      ... )\n",
      " |\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      1    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      2        LINESTRING (0.00000 0.00000, 2.00000 2.00000)\n",
      " |      3        LINESTRING (2.00000 0.00000, 0.00000 2.00000)\n",
      " |      4                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s2\n",
      " |      1    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 0....\n",
      " |      2        LINESTRING (1.00000 0.00000, 1.00000 3.00000)\n",
      " |      3        LINESTRING (2.00000 0.00000, 0.00000 2.00000)\n",
      " |      4                              POINT (1.00000 1.00000)\n",
      " |      5                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can do union of each geometry and a single\n",
      " |      shapely geometry:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-03.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      >>> s.union(Polygon([(0, 0), (1, 1), (0, 1)]))\n",
      " |      0    POLYGON ((0.00000 0.00000, 0.00000 1.00000, 0....\n",
      " |      1    POLYGON ((0.00000 0.00000, 0.00000 1.00000, 0....\n",
      " |      2    GEOMETRYCOLLECTION (POLYGON ((0.00000 0.00000,...\n",
      " |      3    GEOMETRYCOLLECTION (POLYGON ((0.00000 0.00000,...\n",
      " |      4    POLYGON ((0.00000 1.00000, 1.00000 1.00000, 0....\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can also check two GeoSeries against each other, row by row.\n",
      " |      The GeoSeries above have different indices. We can either align both GeoSeries\n",
      " |      based on index values and compare elements with the same index using\n",
      " |      ``align=True`` or ignore index and compare elements based on their matching\n",
      " |      order using ``align=False``:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-02.svg\n",
      " |\n",
      " |      >>> s.union(s2, align=True)\n",
      " |      0                                                 None\n",
      " |      1    POLYGON ((0.00000 0.00000, 0.00000 1.00000, 0....\n",
      " |      2    MULTILINESTRING ((0.00000 0.00000, 1.00000 1.0...\n",
      " |      3        LINESTRING (2.00000 0.00000, 0.00000 2.00000)\n",
      " |      4        MULTIPOINT (0.00000 1.00000, 1.00000 1.00000)\n",
      " |      5                                                 None\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.union(s2, align=False)\n",
      " |      0    POLYGON ((0.00000 0.00000, 0.00000 1.00000, 0....\n",
      " |      1    GEOMETRYCOLLECTION (POLYGON ((0.00000 0.00000,...\n",
      " |      2    MULTILINESTRING ((0.00000 0.00000, 1.00000 1.0...\n",
      " |      3        LINESTRING (2.00000 0.00000, 0.00000 2.00000)\n",
      " |      4                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      GeoSeries.symmetric_difference\n",
      " |      GeoSeries.difference\n",
      " |      GeoSeries.intersection\n",
      " |\n",
      " |  within(self, other, align=True)\n",
      " |      Returns a ``Series`` of ``dtype('bool')`` with value ``True`` for\n",
      " |      each aligned geometry that is within `other`.\n",
      " |\n",
      " |      An object is said to be within `other` if at least one of its points is located\n",
      " |      in the `interior` and no points are located in the `exterior` of the other.\n",
      " |      If either object is empty, this operation returns ``False``.\n",
      " |\n",
      " |      This is the inverse of :meth:`contains` in the sense that the\n",
      " |      expression ``a.within(b) == b.contains(a)`` always evaluates to\n",
      " |      ``True``.\n",
      " |\n",
      " |      The operation works on a 1-to-1 row-wise manner:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-01.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : GeoSeries or geometric object\n",
      " |          The GeoSeries (elementwise) or geometric object to test if each\n",
      " |          geometry is within.\n",
      " |      align : bool (default True)\n",
      " |          If True, automatically aligns GeoSeries based on their indices.\n",
      " |          If False, the order of elements is preserved.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series (bool)\n",
      " |\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (2, 2), (0, 2)]),\n",
      " |      ...         Polygon([(0, 0), (1, 2), (0, 2)]),\n",
      " |      ...         LineString([(0, 0), (0, 2)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ... )\n",
      " |      >>> s2 = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         LineString([(0, 0), (0, 2)]),\n",
      " |      ...         LineString([(0, 0), (0, 1)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...     ],\n",
      " |      ...     index=range(1, 5),\n",
      " |      ... )\n",
      " |\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      1    POLYGON ((0.00000 0.00000, 1.00000 2.00000, 0....\n",
      " |      2        LINESTRING (0.00000 0.00000, 0.00000 2.00000)\n",
      " |      3                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s2\n",
      " |      1    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 0....\n",
      " |      2        LINESTRING (0.00000 0.00000, 0.00000 2.00000)\n",
      " |      3        LINESTRING (0.00000 0.00000, 0.00000 1.00000)\n",
      " |      4                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      We can check if each geometry of GeoSeries is within a single\n",
      " |      geometry:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-03.svg\n",
      " |         :align: center\n",
      " |\n",
      " |      >>> polygon = Polygon([(0, 0), (2, 2), (0, 2)])\n",
      " |      >>> s.within(polygon)\n",
      " |      0     True\n",
      " |      1     True\n",
      " |      2    False\n",
      " |      3    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      We can also check two GeoSeries against each other, row by row.\n",
      " |      The GeoSeries above have different indices. We can either align both GeoSeries\n",
      " |      based on index values and compare elements with the same index using\n",
      " |      ``align=True`` or ignore index and compare elements based on their matching\n",
      " |      order using ``align=False``:\n",
      " |\n",
      " |      .. image:: ../../../_static/binary_op-02.svg\n",
      " |\n",
      " |      >>> s2.within(s)\n",
      " |      0    False\n",
      " |      1    False\n",
      " |      2     True\n",
      " |      3    False\n",
      " |      4    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      >>> s2.within(s, align=False)\n",
      " |      1     True\n",
      " |      2    False\n",
      " |      3     True\n",
      " |      4     True\n",
      " |      dtype: bool\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method works in a row-wise manner. It does not check if an element\n",
      " |      of one GeoSeries is ``within`` *any* element of the other one.\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.contains\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from geopandas.base.GeoPandasBase:\n",
      " |\n",
      " |  area\n",
      " |      Returns a ``Series`` containing the area of each geometry in the\n",
      " |      ``GeoSeries`` expressed in the units of the CRS.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         Polygon([(10, 0), (10, 5), (0, 0)]),\n",
      " |      ...         Polygon([(0, 0), (2, 2), (2, 0)]),\n",
      " |      ...         LineString([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         Point(0, 1)\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 0....\n",
      " |      1    POLYGON ((10.00000 0.00000, 10.00000 5.00000, ...\n",
      " |      2    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 2....\n",
      " |      3    LINESTRING (0.00000 0.00000, 1.00000 1.00000, ...\n",
      " |      4                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.area\n",
      " |      0     0.5\n",
      " |      1    25.0\n",
      " |      2     2.0\n",
      " |      3     0.0\n",
      " |      4     0.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.length : measure length\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Area may be invalid for a geographic CRS using degrees as units;\n",
      " |      use :meth:`GeoSeries.to_crs` to project geometries to a planar\n",
      " |      CRS before using this function.\n",
      " |\n",
      " |      Every operation in GeoPandas is planar, i.e. the potential third\n",
      " |      dimension is not taken into account.\n",
      " |\n",
      " |  boundary\n",
      " |      Returns a ``GeoSeries`` of lower dimensional objects representing\n",
      " |      each geometry's set-theoretic `boundary`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         LineString([(0, 0), (1, 1), (1, 0)]),\n",
      " |      ...         Point(0, 0),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 0....\n",
      " |      1    LINESTRING (0.00000 0.00000, 1.00000 1.00000, ...\n",
      " |      2                              POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.boundary\n",
      " |      0    LINESTRING (0.00000 0.00000, 1.00000 1.00000, ...\n",
      " |      1        MULTIPOINT (0.00000 0.00000, 1.00000 0.00000)\n",
      " |      2                             GEOMETRYCOLLECTION EMPTY\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.exterior : outer boundary (without interior rings)\n",
      " |\n",
      " |  bounds\n",
      " |      Returns a ``DataFrame`` with columns ``minx``, ``miny``, ``maxx``,\n",
      " |      ``maxy`` values containing the bounds for each geometry.\n",
      " |\n",
      " |      See ``GeoSeries.total_bounds`` for the limits of the entire series.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Point, Polygon, LineString\n",
      " |      >>> d = {'geometry': [Point(2, 1), Polygon([(0, 0), (1, 1), (1, 0)]),\n",
      " |      ... LineString([(0, 1), (1, 2)])]}\n",
      " |      >>> gdf = geopandas.GeoDataFrame(d, crs=\"EPSG:4326\")\n",
      " |      >>> gdf.bounds\n",
      " |         minx  miny  maxx  maxy\n",
      " |      0   2.0   1.0   2.0   1.0\n",
      " |      1   0.0   0.0   1.0   1.0\n",
      " |      2   0.0   1.0   1.0   2.0\n",
      " |\n",
      " |      You can assign the bounds to the ``GeoDataFrame`` as:\n",
      " |\n",
      " |      >>> import pandas as pd\n",
      " |      >>> gdf = pd.concat([gdf, gdf.bounds], axis=1)\n",
      " |      >>> gdf\n",
      " |                                                  geometry  minx  miny  maxx  maxy\n",
      " |      0                            POINT (2.00000 1.00000)   2.0   1.0   2.0   1.0\n",
      " |      1  POLYGON ((0.00000 0.00000, 1.00000 1.00000, 1....   0.0   0.0   1.0   1.0\n",
      " |      2      LINESTRING (0.00000 1.00000, 1.00000 2.00000)   0.0   1.0   1.0   2.0\n",
      " |\n",
      " |  cascaded_union\n",
      " |      Deprecated: use `unary_union` instead\n",
      " |\n",
      " |  centroid\n",
      " |      Returns a ``GeoSeries`` of points representing the centroid of each\n",
      " |      geometry.\n",
      " |\n",
      " |      Note that centroid does not have to be on or within original geometry.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         LineString([(0, 0), (1, 1), (1, 0)]),\n",
      " |      ...         Point(0, 0),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 0....\n",
      " |      1    LINESTRING (0.00000 0.00000, 1.00000 1.00000, ...\n",
      " |      2                              POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.centroid\n",
      " |      0    POINT (0.33333 0.66667)\n",
      " |      1    POINT (0.70711 0.50000)\n",
      " |      2    POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.representative_point : point guaranteed to be within each geometry\n",
      " |\n",
      " |  convex_hull\n",
      " |      Returns a ``GeoSeries`` of geometries representing the convex hull\n",
      " |      of each geometry.\n",
      " |\n",
      " |      The convex hull of a geometry is the smallest convex `Polygon`\n",
      " |      containing all the points in each geometry, unless the number of points\n",
      " |      in the geometric object is less than three. For two points, the convex\n",
      " |      hull collapses to a `LineString`; for 1, a `Point`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point, MultiPoint\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         LineString([(0, 0), (1, 1), (1, 0)]),\n",
      " |      ...         MultiPoint([(0, 0), (1, 1), (0, 1), (1, 0), (0.5, 0.5)]),\n",
      " |      ...         MultiPoint([(0, 0), (1, 1)]),\n",
      " |      ...         Point(0, 0),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 0....\n",
      " |      1    LINESTRING (0.00000 0.00000, 1.00000 1.00000, ...\n",
      " |      2    MULTIPOINT (0.00000 0.00000, 1.00000 1.00000, ...\n",
      " |      3        MULTIPOINT (0.00000 0.00000, 1.00000 1.00000)\n",
      " |      4                              POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.convex_hull\n",
      " |      0    POLYGON ((0.00000 0.00000, 0.00000 1.00000, 1....\n",
      " |      1    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 1....\n",
      " |      2    POLYGON ((0.00000 0.00000, 0.00000 1.00000, 1....\n",
      " |      3        LINESTRING (0.00000 0.00000, 1.00000 1.00000)\n",
      " |      4                              POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.concave_hull : concave hull geometry\n",
      " |      GeoSeries.envelope : bounding rectangle geometry\n",
      " |\n",
      " |  cx\n",
      " |      Coordinate based indexer to select by intersection with bounding box.\n",
      " |\n",
      " |      Format of input should be ``.cx[xmin:xmax, ymin:ymax]``. Any of\n",
      " |      ``xmin``, ``xmax``, ``ymin``, and ``ymax`` can be provided, but input\n",
      " |      must include a comma separating x and y slices. That is, ``.cx[:, :]``\n",
      " |      will return the full series/frame, but ``.cx[:]`` is not implemented.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import LineString, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [Point(0, 0), Point(1, 2), Point(3, 3), LineString([(0, 0), (3, 3)])]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0                          POINT (0.00000 0.00000)\n",
      " |      1                          POINT (1.00000 2.00000)\n",
      " |      2                          POINT (3.00000 3.00000)\n",
      " |      3    LINESTRING (0.00000 0.00000, 3.00000 3.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.cx[0:1, 0:1]\n",
      " |      0                          POINT (0.00000 0.00000)\n",
      " |      3    LINESTRING (0.00000 0.00000, 3.00000 3.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.cx[:, 1:]\n",
      " |      1                          POINT (1.00000 2.00000)\n",
      " |      2                          POINT (3.00000 3.00000)\n",
      " |      3    LINESTRING (0.00000 0.00000, 3.00000 3.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |  envelope\n",
      " |      Returns a ``GeoSeries`` of geometries representing the envelope of\n",
      " |      each geometry.\n",
      " |\n",
      " |      The envelope of a geometry is the bounding rectangle. That is, the\n",
      " |      point or smallest rectangular polygon (with sides parallel to the\n",
      " |      coordinate axes) that contains the geometry.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely.geometry import Polygon, LineString, Point, MultiPoint\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         LineString([(0, 0), (1, 1), (1, 0)]),\n",
      " |      ...         MultiPoint([(0, 0), (1, 1)]),\n",
      " |      ...         Point(0, 0),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 0....\n",
      " |      1    LINESTRING (0.00000 0.00000, 1.00000 1.00000, ...\n",
      " |      2        MULTIPOINT (0.00000 0.00000, 1.00000 1.00000)\n",
      " |      3                              POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.envelope\n",
      " |      0    POLYGON ((0.00000 0.00000, 1.00000 0.00000, 1....\n",
      " |      1    POLYGON ((0.00000 0.00000, 1.00000 0.00000, 1....\n",
      " |      2    POLYGON ((0.00000 0.00000, 1.00000 0.00000, 1....\n",
      " |      3                              POINT (0.00000 0.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.convex_hull : convex hull geometry\n",
      " |\n",
      " |  exterior\n",
      " |      Returns a ``GeoSeries`` of LinearRings representing the outer\n",
      " |      boundary of each polygon in the GeoSeries.\n",
      " |\n",
      " |      Applies to GeoSeries containing only Polygons. Returns ``None``` for\n",
      " |      other geometry types.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely.geometry import Polygon, Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         Polygon([(1, 0), (2, 1), (0, 0)]),\n",
      " |      ...         Point(0, 1)\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 0....\n",
      " |      1    POLYGON ((1.00000 0.00000, 2.00000 1.00000, 0....\n",
      " |      2                              POINT (0.00000 1.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.exterior\n",
      " |      0    LINEARRING (0.00000 0.00000, 1.00000 1.00000, ...\n",
      " |      1    LINEARRING (1.00000 0.00000, 2.00000 1.00000, ...\n",
      " |      2                                                 None\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.boundary : complete set-theoretic boundary\n",
      " |      GeoSeries.interiors : list of inner rings of each polygon\n",
      " |\n",
      " |  geom_type\n",
      " |      Returns a ``Series`` of strings specifying the `Geometry Type` of each\n",
      " |      object.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Point, Polygon, LineString\n",
      " |      >>> d = {'geometry': [Point(2, 1), Polygon([(0, 0), (1, 1), (1, 0)]),\n",
      " |      ... LineString([(0, 0), (1, 1)])]}\n",
      " |      >>> gdf = geopandas.GeoDataFrame(d, crs=\"EPSG:4326\")\n",
      " |      >>> gdf.geom_type\n",
      " |      0         Point\n",
      " |      1       Polygon\n",
      " |      2    LineString\n",
      " |      dtype: object\n",
      " |\n",
      " |  has_sindex\n",
      " |      Check the existence of the spatial index without generating it.\n",
      " |\n",
      " |      Use the `.sindex` attribute on a GeoDataFrame or GeoSeries\n",
      " |      to generate a spatial index if it does not yet exist,\n",
      " |      which may take considerable time based on the underlying index\n",
      " |      implementation.\n",
      " |\n",
      " |      Note that the underlying spatial index may not be fully\n",
      " |      initialized until the first use.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely.geometry import Point\n",
      " |      >>> d = {'geometry': [Point(1, 2), Point(2, 1)]}\n",
      " |      >>> gdf = geopandas.GeoDataFrame(d)\n",
      " |      >>> gdf.has_sindex\n",
      " |      False\n",
      " |      >>> index = gdf.sindex\n",
      " |      >>> gdf.has_sindex\n",
      " |      True\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          `True` if the spatial index has been generated or\n",
      " |          `False` if not.\n",
      " |\n",
      " |  has_z\n",
      " |      Returns a ``Series`` of ``dtype('bool')`` with value ``True`` for\n",
      " |      features that have a z-component.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Every operation in GeoPandas is planar, i.e. the potential third\n",
      " |      dimension is not taken into account.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Point\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Point(0, 1),\n",
      " |      ...         Point(0, 1, 2),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0              POINT (0.00000 1.00000)\n",
      " |      1    POINT Z (0.00000 1.00000 2.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.has_z\n",
      " |      0    False\n",
      " |      1     True\n",
      " |      dtype: bool\n",
      " |\n",
      " |  interiors\n",
      " |      Returns a ``Series`` of List representing the\n",
      " |      inner rings of each polygon in the GeoSeries.\n",
      " |\n",
      " |      Applies to GeoSeries containing only Polygons.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      inner_rings: Series of List\n",
      " |          Inner rings of each polygon in the GeoSeries.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely.geometry import Polygon\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon(\n",
      " |      ...             [(0, 0), (0, 5), (5, 5), (5, 0)],\n",
      " |      ...             [[(1, 1), (2, 1), (1, 2)], [(1, 4), (2, 4), (2, 3)]],\n",
      " |      ...         ),\n",
      " |      ...         Polygon([(1, 0), (2, 1), (0, 0)]),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 0.00000 5.00000, 5....\n",
      " |      1    POLYGON ((1.00000 0.00000, 2.00000 1.00000, 0....\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.interiors\n",
      " |      0    [LINEARRING (1 1, 2 1, 1 2, 1 1), LINEARRING (...\n",
      " |      1                                                   []\n",
      " |      dtype: object\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.exterior : outer boundary\n",
      " |\n",
      " |  is_empty\n",
      " |      Returns a ``Series`` of ``dtype('bool')`` with value ``True`` for\n",
      " |      empty geometries.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      An example of a GeoDataFrame with one empty point, one point and one missing\n",
      " |      value:\n",
      " |\n",
      " |      >>> from shapely.geometry import Point\n",
      " |      >>> d = {'geometry': [Point(), Point(2, 1), None]}\n",
      " |      >>> gdf = geopandas.GeoDataFrame(d, crs=\"EPSG:4326\")\n",
      " |      >>> gdf\n",
      " |                         geometry\n",
      " |      0               POINT EMPTY\n",
      " |      1   POINT (2.00000 1.00000)\n",
      " |      2                      None\n",
      " |      >>> gdf.is_empty\n",
      " |      0     True\n",
      " |      1    False\n",
      " |      2    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      GeoSeries.isna : detect missing values\n",
      " |\n",
      " |  is_ring\n",
      " |      Returns a ``Series`` of ``dtype('bool')`` with value ``True`` for\n",
      " |      features that are closed.\n",
      " |\n",
      " |      When constructing a LinearRing, the sequence of coordinates may be\n",
      " |      explicitly closed by passing identical values in the first and last indices.\n",
      " |      Otherwise, the sequence will be implicitly closed by copying the first tuple\n",
      " |      to the last index.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import LineString, LinearRing\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         LineString([(0, 0), (1, 1), (1, -1)]),\n",
      " |      ...         LineString([(0, 0), (1, 1), (1, -1), (0, 0)]),\n",
      " |      ...         LinearRing([(0, 0), (1, 1), (1, -1)]),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    LINESTRING (0.00000 0.00000, 1.00000 1.00000, ...\n",
      " |      1    LINESTRING (0.00000 0.00000, 1.00000 1.00000, ...\n",
      " |      2    LINEARRING (0.00000 0.00000, 1.00000 1.00000, ...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.is_ring\n",
      " |      0    False\n",
      " |      1     True\n",
      " |      2     True\n",
      " |      dtype: bool\n",
      " |\n",
      " |  is_simple\n",
      " |      Returns a ``Series`` of ``dtype('bool')`` with value ``True`` for\n",
      " |      geometries that do not cross themselves.\n",
      " |\n",
      " |      This is meaningful only for `LineStrings` and `LinearRings`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import LineString\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         LineString([(0, 0), (1, 1), (1, -1), (0, 1)]),\n",
      " |      ...         LineString([(0, 0), (1, 1), (1, -1)]),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    LINESTRING (0.00000 0.00000, 1.00000 1.00000, ...\n",
      " |      1    LINESTRING (0.00000 0.00000, 1.00000 1.00000, ...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.is_simple\n",
      " |      0    False\n",
      " |      1     True\n",
      " |      dtype: bool\n",
      " |\n",
      " |  is_valid\n",
      " |      Returns a ``Series`` of ``dtype('bool')`` with value ``True`` for\n",
      " |      geometries that are valid.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      An example with one invalid polygon (a bowtie geometry crossing itself)\n",
      " |      and one missing geometry:\n",
      " |\n",
      " |      >>> from shapely.geometry import Polygon\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         Polygon([(0,0), (1, 1), (1, 0), (0, 1)]),  # bowtie geometry\n",
      " |      ...         Polygon([(0, 0), (2, 2), (2, 0)]),\n",
      " |      ...         None\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 0....\n",
      " |      1    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 1....\n",
      " |      2    POLYGON ((0.00000 0.00000, 2.00000 2.00000, 2....\n",
      " |      3                                                 None\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.is_valid\n",
      " |      0     True\n",
      " |      1    False\n",
      " |      2     True\n",
      " |      3    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |  length\n",
      " |      Returns a ``Series`` containing the length of each geometry\n",
      " |      expressed in the units of the CRS.\n",
      " |\n",
      " |      In the case of a (Multi)Polygon it measures the length\n",
      " |      of its exterior (i.e. perimeter).\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely.geometry import Polygon, LineString, MultiLineString, Point, GeometryCollection\n",
      " |      >>> s = geopandas.GeoSeries(\n",
      " |      ...     [\n",
      " |      ...         LineString([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         LineString([(10, 0), (10, 5), (0, 0)]),\n",
      " |      ...         MultiLineString([((0, 0), (1, 0)), ((-1, 0), (1, 0))]),\n",
      " |      ...         Polygon([(0, 0), (1, 1), (0, 1)]),\n",
      " |      ...         Point(0, 1),\n",
      " |      ...         GeometryCollection([Point(1, 0), LineString([(10, 0), (10, 5), (0, 0)])])\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> s\n",
      " |      0    LINESTRING (0.00000 0.00000, 1.00000 1.00000, ...\n",
      " |      1    LINESTRING (10.00000 0.00000, 10.00000 5.00000...\n",
      " |      2    MULTILINESTRING ((0.00000 0.00000, 1.00000 0.0...\n",
      " |      3    POLYGON ((0.00000 0.00000, 1.00000 1.00000, 0....\n",
      " |      4                              POINT (0.00000 1.00000)\n",
      " |      5    GEOMETRYCOLLECTION (POINT (1.00000 0.00000), L...\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.length\n",
      " |      0     2.414214\n",
      " |      1    16.180340\n",
      " |      2     3.000000\n",
      " |      3     3.414214\n",
      " |      4     0.000000\n",
      " |      5    16.180340\n",
      " |      dtype: float64\n",
      " |\n",
      " |      See also\n",
      " |      --------\n",
      " |      GeoSeries.area : measure area of a polygon\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Length may be invalid for a geographic CRS using degrees as units;\n",
      " |      use :meth:`GeoSeries.to_crs` to project geometries to a planar\n",
      " |      CRS before using this function.\n",
      " |\n",
      " |      Every operation in GeoPandas is planar, i.e. the potential third\n",
      " |      dimension is not taken into account.\n",
      " |\n",
      " |  sindex\n",
      " |      Generate the spatial index\n",
      " |\n",
      " |      Creates R-tree spatial index based on ``pygeos.STRtree`` or\n",
      " |      ``rtree.index.Index``.\n",
      " |\n",
      " |      Note that the  spatial index may not be fully\n",
      " |      initialized until the first use.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import box\n",
      " |      >>> s = geopandas.GeoSeries(geopandas.points_from_xy(range(5), range(5)))\n",
      " |      >>> s\n",
      " |      0    POINT (0.00000 0.00000)\n",
      " |      1    POINT (1.00000 1.00000)\n",
      " |      2    POINT (2.00000 2.00000)\n",
      " |      3    POINT (3.00000 3.00000)\n",
      " |      4    POINT (4.00000 4.00000)\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      Query the spatial index with a single geometry based on the bounding box:\n",
      " |\n",
      " |      >>> s.sindex.query(box(1, 1, 3, 3))\n",
      " |      array([1, 2, 3])\n",
      " |\n",
      " |      Query the spatial index with a single geometry based on the predicate:\n",
      " |\n",
      " |      >>> s.sindex.query(box(1, 1, 3, 3), predicate=\"contains\")\n",
      " |      array([2])\n",
      " |\n",
      " |      Query the spatial index with an array of geometries based on the bounding\n",
      " |      box:\n",
      " |\n",
      " |      >>> s2 = geopandas.GeoSeries([box(1, 1, 3, 3), box(4, 4, 5, 5)])\n",
      " |      >>> s2\n",
      " |      0    POLYGON ((3.00000 1.00000, 3.00000 3.00000, 1....\n",
      " |      1    POLYGON ((5.00000 4.00000, 5.00000 5.00000, 4....\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> s.sindex.query(s2)\n",
      " |      array([[0, 0, 0, 1],\n",
      " |             [1, 2, 3, 4]])\n",
      " |\n",
      " |      Query the spatial index with an array of geometries based on the predicate:\n",
      " |\n",
      " |      >>> s.sindex.query(s2, predicate=\"contains\")\n",
      " |      array([[0],\n",
      " |             [2]])\n",
      " |\n",
      " |  total_bounds\n",
      " |      Returns a tuple containing ``minx``, ``miny``, ``maxx``, ``maxy``\n",
      " |      values for the bounds of the series as a whole.\n",
      " |\n",
      " |      See ``GeoSeries.bounds`` for the bounds of the geometries contained in\n",
      " |      the series.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from shapely.geometry import Point, Polygon, LineString\n",
      " |      >>> d = {'geometry': [Point(3, -1), Polygon([(0, 0), (1, 1), (1, 0)]),\n",
      " |      ... LineString([(0, 1), (1, 2)])]}\n",
      " |      >>> gdf = geopandas.GeoDataFrame(d, crs=\"EPSG:4326\")\n",
      " |      >>> gdf.total_bounds\n",
      " |      array([ 0., -1.,  3.,  2.])\n",
      " |\n",
      " |  type\n",
      " |      Return the geometry type of each geometry in the GeoSeries\n",
      " |\n",
      " |  unary_union\n",
      " |      Returns a geometry containing the union of all geometries in the\n",
      " |      ``GeoSeries``.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> from shapely.geometry import box\n",
      " |      >>> s = geopandas.GeoSeries([box(0,0,1,1), box(0,0,2,2)])\n",
      " |      >>> s\n",
      " |      0    POLYGON ((1.00000 0.00000, 1.00000 1.00000, 0....\n",
      " |      1    POLYGON ((2.00000 0.00000, 2.00000 2.00000, 0....\n",
      " |      dtype: geometry\n",
      " |\n",
      " |      >>> union = s.unary_union\n",
      " |      >>> print(union)\n",
      " |      POLYGON ((0 1, 0 2, 2 2, 2 0, 1 0, 0 0, 0 1))\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from geopandas.base.GeoPandasBase:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pandas.core.frame.DataFrame:\n",
      " |\n",
      " |  __arrow_c_stream__(self, requested_schema=None)\n",
      " |      Export the pandas DataFrame as an Arrow C stream PyCapsule.\n",
      " |\n",
      " |      This relies on pyarrow to convert the pandas DataFrame to the Arrow\n",
      " |      format (and follows the default behaviour of ``pyarrow.Table.from_pandas``\n",
      " |      in its handling of the index, i.e. store the index as a column except\n",
      " |      for RangeIndex).\n",
      " |      This conversion is not necessarily zero-copy.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      requested_schema : PyCapsule, default None\n",
      " |          The schema to which the dataframe should be casted, passed as a\n",
      " |          PyCapsule containing a C ArrowSchema representation of the\n",
      " |          requested schema.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      PyCapsule\n",
      " |\n",
      " |  __dataframe__(self, nan_as_null: 'bool' = False, allow_copy: 'bool' = True) -> 'DataFrameXchg'\n",
      " |      Return the dataframe interchange object implementing the interchange protocol.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      nan_as_null : bool, default False\n",
      " |          `nan_as_null` is DEPRECATED and has no effect. Please avoid using\n",
      " |          it; it will be removed in a future release.\n",
      " |      allow_copy : bool, default True\n",
      " |          Whether to allow memory copying when exporting. If set to False\n",
      " |          it would cause non-zero-copy exports to fail.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame interchange object\n",
      " |          The object which consuming library can use to ingress the dataframe.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Details on the interchange protocol:\n",
      " |      https://data-apis.org/dataframe-protocol/latest/index.html\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df_not_necessarily_pandas = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
      " |      >>> interchange_object = df_not_necessarily_pandas.__dataframe__()\n",
      " |      >>> interchange_object.column_names()\n",
      " |      Index(['A', 'B'], dtype='object')\n",
      " |      >>> df_pandas = (pd.api.interchange.from_dataframe\n",
      " |      ...              (interchange_object.select_columns_by_name(['A'])))\n",
      " |      >>> df_pandas\n",
      " |           A\n",
      " |      0    1\n",
      " |      1    2\n",
      " |\n",
      " |      These methods (``column_names``, ``select_columns_by_name``) should work\n",
      " |      for any dataframe library which implements the interchange protocol.\n",
      " |\n",
      " |  __dataframe_consortium_standard__(self, *, api_version: 'str | None' = None) -> 'Any'\n",
      " |      Provide entry point to the Consortium DataFrame Standard API.\n",
      " |\n",
      " |      This is developed and maintained outside of pandas.\n",
      " |      Please report any issues to https://github.com/data-apis/dataframe-api-compat.\n",
      " |\n",
      " |  __divmod__(self, other) -> 'tuple[DataFrame, DataFrame]'\n",
      " |\n",
      " |  __len__(self) -> 'int'\n",
      " |      Returns length of info axis, but here we use the index.\n",
      " |\n",
      " |  __matmul__(self, other: 'AnyArrayLike | DataFrame') -> 'DataFrame | Series'\n",
      " |      Matrix multiplication using binary `@` operator.\n",
      " |\n",
      " |  __rdivmod__(self, other) -> 'tuple[DataFrame, DataFrame]'\n",
      " |\n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return a string representation for a particular DataFrame.\n",
      " |\n",
      " |  __rmatmul__(self, other) -> 'DataFrame'\n",
      " |      Matrix multiplication using binary `@` operator.\n",
      " |\n",
      " |  add(self, other, axis: 'Axis' = 'columns', level=None, fill_value=None) -> 'DataFrame'\n",
      " |      Get Addition of dataframe and other, element-wise (binary operator `add`).\n",
      " |\n",
      " |      Equivalent to ``dataframe + other``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `radd`.\n",
      " |\n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `floordiv`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, dict or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns.\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |\n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |\n",
      " |      >>> df + 1\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      >>> df.add(1)\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      Divide by constant with reverse version.\n",
      " |\n",
      " |      >>> df.div(10)\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |\n",
      " |      >>> df.rdiv(10)\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |\n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |\n",
      " |      >>> df - [1, 2]\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub([1, 2], axis='columns')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |\n",
      " |      Multiply a dictionary by axis.\n",
      " |\n",
      " |      >>> df.mul({'angles': 0, 'degrees': 2})\n",
      " |                  angles  degrees\n",
      " |      circle           0      720\n",
      " |      triangle         0      360\n",
      " |      rectangle        0      720\n",
      " |\n",
      " |      >>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n",
      " |                  angles  degrees\n",
      " |      circle           0        0\n",
      " |      triangle         6      360\n",
      " |      rectangle       12     1080\n",
      " |\n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |\n",
      " |      >>> df * other\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |\n",
      " |      >>> df.mul(other, fill_value=0)\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |\n",
      " |      Divide by a MultiIndex by level.\n",
      " |\n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |\n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |\n",
      " |  agg = aggregate(self, func=None, axis: 'Axis' = 0, *args, **kwargs)\n",
      " |\n",
      " |  aggregate(self, func=None, axis: 'Axis' = 0, *args, **kwargs)\n",
      " |      Aggregate using one or more operations over the specified axis.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function, str, list or dict\n",
      " |          Function to use for aggregating the data. If a function, must either\n",
      " |          work when passed a DataFrame or when passed to DataFrame.apply.\n",
      " |\n",
      " |          Accepted combinations are:\n",
      " |\n",
      " |          - function\n",
      " |          - string function name\n",
      " |          - list of functions and/or function names, e.g. ``[np.sum, 'mean']``\n",
      " |          - dict of axis labels -> functions, function names or list of such.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |              If 0 or 'index': apply function to each column.\n",
      " |              If 1 or 'columns': apply function to each row.\n",
      " |      *args\n",
      " |          Positional arguments to pass to `func`.\n",
      " |      **kwargs\n",
      " |          Keyword arguments to pass to `func`.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      scalar, Series or DataFrame\n",
      " |\n",
      " |          The return can be:\n",
      " |\n",
      " |          * scalar : when Series.agg is called with single function\n",
      " |          * Series : when DataFrame.agg is called with a single function\n",
      " |          * DataFrame : when DataFrame.agg is called with several functions\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.apply : Perform any type of operations.\n",
      " |      DataFrame.transform : Perform transformation type operations.\n",
      " |      pandas.DataFrame.groupby : Perform operations over groups.\n",
      " |      pandas.DataFrame.resample : Perform operations over resampled bins.\n",
      " |      pandas.DataFrame.rolling : Perform operations over rolling window.\n",
      " |      pandas.DataFrame.expanding : Perform operations over expanding window.\n",
      " |      pandas.core.window.ewm.ExponentialMovingWindow : Perform operation over exponential\n",
      " |          weighted window.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      The aggregation operations are always performed over an axis, either the\n",
      " |      index (default) or the column axis. This behavior is different from\n",
      " |      `numpy` aggregation functions (`mean`, `median`, `prod`, `sum`, `std`,\n",
      " |      `var`), where the default is to compute the aggregation of the flattened\n",
      " |      array, e.g., ``numpy.mean(arr_2d)`` as opposed to\n",
      " |      ``numpy.mean(arr_2d, axis=0)``.\n",
      " |\n",
      " |      `agg` is an alias for `aggregate`. Use the alias.\n",
      " |\n",
      " |      Functions that mutate the passed object can produce unexpected\n",
      " |      behavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\n",
      " |      for more details.\n",
      " |\n",
      " |      A passed user-defined-function will be passed a Series for evaluation.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([[1, 2, 3],\n",
      " |      ...                    [4, 5, 6],\n",
      " |      ...                    [7, 8, 9],\n",
      " |      ...                    [np.nan, np.nan, np.nan]],\n",
      " |      ...                   columns=['A', 'B', 'C'])\n",
      " |\n",
      " |      Aggregate these functions over the rows.\n",
      " |\n",
      " |      >>> df.agg(['sum', 'min'])\n",
      " |              A     B     C\n",
      " |      sum  12.0  15.0  18.0\n",
      " |      min   1.0   2.0   3.0\n",
      " |\n",
      " |      Different aggregations per column.\n",
      " |\n",
      " |      >>> df.agg({'A' : ['sum', 'min'], 'B' : ['min', 'max']})\n",
      " |              A    B\n",
      " |      sum  12.0  NaN\n",
      " |      min   1.0  2.0\n",
      " |      max   NaN  8.0\n",
      " |\n",
      " |      Aggregate different functions over the columns and rename the index of the resulting\n",
      " |      DataFrame.\n",
      " |\n",
      " |      >>> df.agg(x=('A', 'max'), y=('B', 'min'), z=('C', 'mean'))\n",
      " |           A    B    C\n",
      " |      x  7.0  NaN  NaN\n",
      " |      y  NaN  2.0  NaN\n",
      " |      z  NaN  NaN  6.0\n",
      " |\n",
      " |      Aggregate over the columns.\n",
      " |\n",
      " |      >>> df.agg(\"mean\", axis=\"columns\")\n",
      " |      0    2.0\n",
      " |      1    5.0\n",
      " |      2    8.0\n",
      " |      3    NaN\n",
      " |      dtype: float64\n",
      " |\n",
      " |  all(self, axis: 'Axis | None' = 0, bool_only: 'bool' = False, skipna: 'bool' = True, **kwargs) -> 'Series | bool'\n",
      " |      Return whether all elements are True, potentially over an axis.\n",
      " |\n",
      " |      Returns True unless there at least one element within a series or\n",
      " |      along a Dataframe axis that is False or equivalent (e.g. zero or\n",
      " |      empty).\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns', None}, default 0\n",
      " |          Indicate which axis or axes should be reduced. For `Series` this parameter\n",
      " |          is unused and defaults to 0.\n",
      " |\n",
      " |          * 0 / 'index' : reduce the index, return a Series whose index is the\n",
      " |            original column labels.\n",
      " |          * 1 / 'columns' : reduce the columns, return a Series whose index is the\n",
      " |            original index.\n",
      " |          * None : reduce all axes, return a scalar.\n",
      " |\n",
      " |      bool_only : bool, default False\n",
      " |          Include only boolean columns. Not implemented for Series.\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If the entire row/column is NA and skipna is\n",
      " |          True, then the result will be True, as for an empty row/column.\n",
      " |          If skipna is False, then NA are treated as True, because these are not\n",
      " |          equal to zero.\n",
      " |      **kwargs : any, default None\n",
      " |          Additional keywords have no effect but might be accepted for\n",
      " |          compatibility with NumPy.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          If level is specified, then, DataFrame is returned; otherwise, Series\n",
      " |          is returned.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.all : Return True if all elements are True.\n",
      " |      DataFrame.any : Return True if one (or more) elements are True.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      **Series**\n",
      " |\n",
      " |      >>> pd.Series([True, True]).all()\n",
      " |      True\n",
      " |      >>> pd.Series([True, False]).all()\n",
      " |      False\n",
      " |      >>> pd.Series([], dtype=\"float64\").all()\n",
      " |      True\n",
      " |      >>> pd.Series([np.nan]).all()\n",
      " |      True\n",
      " |      >>> pd.Series([np.nan]).all(skipna=False)\n",
      " |      True\n",
      " |\n",
      " |      **DataFrames**\n",
      " |\n",
      " |      Create a dataframe from a dictionary.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'col1': [True, True], 'col2': [True, False]})\n",
      " |      >>> df\n",
      " |         col1   col2\n",
      " |      0  True   True\n",
      " |      1  True  False\n",
      " |\n",
      " |      Default behaviour checks if values in each column all return True.\n",
      " |\n",
      " |      >>> df.all()\n",
      " |      col1     True\n",
      " |      col2    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      Specify ``axis='columns'`` to check if values in each row all return True.\n",
      " |\n",
      " |      >>> df.all(axis='columns')\n",
      " |      0     True\n",
      " |      1    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      Or ``axis=None`` for whether every value is True.\n",
      " |\n",
      " |      >>> df.all(axis=None)\n",
      " |      False\n",
      " |\n",
      " |  any(self, *, axis: 'Axis | None' = 0, bool_only: 'bool' = False, skipna: 'bool' = True, **kwargs) -> 'Series | bool'\n",
      " |      Return whether any element is True, potentially over an axis.\n",
      " |\n",
      " |      Returns False unless there is at least one element within a series or\n",
      " |      along a Dataframe axis that is True or equivalent (e.g. non-zero or\n",
      " |      non-empty).\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns', None}, default 0\n",
      " |          Indicate which axis or axes should be reduced. For `Series` this parameter\n",
      " |          is unused and defaults to 0.\n",
      " |\n",
      " |          * 0 / 'index' : reduce the index, return a Series whose index is the\n",
      " |            original column labels.\n",
      " |          * 1 / 'columns' : reduce the columns, return a Series whose index is the\n",
      " |            original index.\n",
      " |          * None : reduce all axes, return a scalar.\n",
      " |\n",
      " |      bool_only : bool, default False\n",
      " |          Include only boolean columns. Not implemented for Series.\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If the entire row/column is NA and skipna is\n",
      " |          True, then the result will be False, as for an empty row/column.\n",
      " |          If skipna is False, then NA are treated as True, because these are not\n",
      " |          equal to zero.\n",
      " |      **kwargs : any, default None\n",
      " |          Additional keywords have no effect but might be accepted for\n",
      " |          compatibility with NumPy.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          If level is specified, then, DataFrame is returned; otherwise, Series\n",
      " |          is returned.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.any : Numpy version of this method.\n",
      " |      Series.any : Return whether any element is True.\n",
      " |      Series.all : Return whether all elements are True.\n",
      " |      DataFrame.any : Return whether any element is True over requested axis.\n",
      " |      DataFrame.all : Return whether all elements are True over requested axis.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      **Series**\n",
      " |\n",
      " |      For Series input, the output is a scalar indicating whether any element\n",
      " |      is True.\n",
      " |\n",
      " |      >>> pd.Series([False, False]).any()\n",
      " |      False\n",
      " |      >>> pd.Series([True, False]).any()\n",
      " |      True\n",
      " |      >>> pd.Series([], dtype=\"float64\").any()\n",
      " |      False\n",
      " |      >>> pd.Series([np.nan]).any()\n",
      " |      False\n",
      " |      >>> pd.Series([np.nan]).any(skipna=False)\n",
      " |      True\n",
      " |\n",
      " |      **DataFrame**\n",
      " |\n",
      " |      Whether each column contains at least one True element (the default).\n",
      " |\n",
      " |      >>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [0, 2], \"C\": [0, 0]})\n",
      " |      >>> df\n",
      " |         A  B  C\n",
      " |      0  1  0  0\n",
      " |      1  2  2  0\n",
      " |\n",
      " |      >>> df.any()\n",
      " |      A     True\n",
      " |      B     True\n",
      " |      C    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      Aggregating over the columns.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({\"A\": [True, False], \"B\": [1, 2]})\n",
      " |      >>> df\n",
      " |             A  B\n",
      " |      0   True  1\n",
      " |      1  False  2\n",
      " |\n",
      " |      >>> df.any(axis='columns')\n",
      " |      0    True\n",
      " |      1    True\n",
      " |      dtype: bool\n",
      " |\n",
      " |      >>> df = pd.DataFrame({\"A\": [True, False], \"B\": [1, 0]})\n",
      " |      >>> df\n",
      " |             A  B\n",
      " |      0   True  1\n",
      " |      1  False  0\n",
      " |\n",
      " |      >>> df.any(axis='columns')\n",
      " |      0    True\n",
      " |      1    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      Aggregating over the entire DataFrame with ``axis=None``.\n",
      " |\n",
      " |      >>> df.any(axis=None)\n",
      " |      True\n",
      " |\n",
      " |      `any` for an empty DataFrame is an empty Series.\n",
      " |\n",
      " |      >>> pd.DataFrame([]).any()\n",
      " |      Series([], dtype: bool)\n",
      " |\n",
      " |  applymap(self, func: 'PythonFuncType', na_action: 'NaAction | None' = None, **kwargs) -> 'DataFrame'\n",
      " |      Apply a function to a Dataframe elementwise.\n",
      " |\n",
      " |      .. deprecated:: 2.1.0\n",
      " |\n",
      " |         DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      " |\n",
      " |      This method applies a function that accepts and returns a scalar\n",
      " |      to every element of a DataFrame.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : callable\n",
      " |          Python function, returns a single value from a single value.\n",
      " |      na_action : {None, 'ignore'}, default None\n",
      " |          If 'ignore', propagate NaN values, without passing them to func.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to pass as keywords arguments to\n",
      " |          `func`.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Transformed DataFrame.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.apply : Apply a function along input axis of DataFrame.\n",
      " |      DataFrame.map : Apply a function along input axis of DataFrame.\n",
      " |      DataFrame.replace: Replace values given in `to_replace` with `value`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([[1, 2.12], [3.356, 4.567]])\n",
      " |      >>> df\n",
      " |             0      1\n",
      " |      0  1.000  2.120\n",
      " |      1  3.356  4.567\n",
      " |\n",
      " |      >>> df.map(lambda x: len(str(x)))\n",
      " |         0  1\n",
      " |      0  3  4\n",
      " |      1  5  5\n",
      " |\n",
      " |  assign(self, **kwargs) -> 'DataFrame'\n",
      " |      Assign new columns to a DataFrame.\n",
      " |\n",
      " |      Returns a new object with all original columns in addition to new ones.\n",
      " |      Existing columns that are re-assigned will be overwritten.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs : dict of {str: callable or Series}\n",
      " |          The column names are keywords. If the values are\n",
      " |          callable, they are computed on the DataFrame and\n",
      " |          assigned to the new columns. The callable must not\n",
      " |          change input DataFrame (though pandas doesn't check it).\n",
      " |          If the values are not callable, (e.g. a Series, scalar, or array),\n",
      " |          they are simply assigned.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          A new DataFrame with the new columns in addition to\n",
      " |          all the existing columns.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Assigning multiple columns within the same ``assign`` is possible.\n",
      " |      Later items in '\\*\\*kwargs' may refer to newly created or modified\n",
      " |      columns in 'df'; items are computed and assigned into 'df' in order.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'temp_c': [17.0, 25.0]},\n",
      " |      ...                   index=['Portland', 'Berkeley'])\n",
      " |      >>> df\n",
      " |                temp_c\n",
      " |      Portland    17.0\n",
      " |      Berkeley    25.0\n",
      " |\n",
      " |      Where the value is a callable, evaluated on `df`:\n",
      " |\n",
      " |      >>> df.assign(temp_f=lambda x: x.temp_c * 9 / 5 + 32)\n",
      " |                temp_c  temp_f\n",
      " |      Portland    17.0    62.6\n",
      " |      Berkeley    25.0    77.0\n",
      " |\n",
      " |      Alternatively, the same behavior can be achieved by directly\n",
      " |      referencing an existing Series or sequence:\n",
      " |\n",
      " |      >>> df.assign(temp_f=df['temp_c'] * 9 / 5 + 32)\n",
      " |                temp_c  temp_f\n",
      " |      Portland    17.0    62.6\n",
      " |      Berkeley    25.0    77.0\n",
      " |\n",
      " |      You can create multiple columns within the same assign where one\n",
      " |      of the columns depends on another one defined within the same assign:\n",
      " |\n",
      " |      >>> df.assign(temp_f=lambda x: x['temp_c'] * 9 / 5 + 32,\n",
      " |      ...           temp_k=lambda x: (x['temp_f'] + 459.67) * 5 / 9)\n",
      " |                temp_c  temp_f  temp_k\n",
      " |      Portland    17.0    62.6  290.15\n",
      " |      Berkeley    25.0    77.0  298.15\n",
      " |\n",
      " |  boxplot = boxplot_frame(self: 'DataFrame', column=None, by=None, ax=None, fontsize: 'int | None' = None, rot: 'int' = 0, grid: 'bool' = True, figsize: 'tuple[float, float] | None' = None, layout=None, return_type=None, backend=None, **kwargs) from pandas.plotting._core\n",
      " |      Make a box plot from DataFrame columns.\n",
      " |\n",
      " |      Make a box-and-whisker plot from DataFrame columns, optionally grouped\n",
      " |      by some other columns. A box plot is a method for graphically depicting\n",
      " |      groups of numerical data through their quartiles.\n",
      " |      The box extends from the Q1 to Q3 quartile values of the data,\n",
      " |      with a line at the median (Q2). The whiskers extend from the edges\n",
      " |      of box to show the range of the data. By default, they extend no more than\n",
      " |      `1.5 * IQR (IQR = Q3 - Q1)` from the edges of the box, ending at the farthest\n",
      " |      data point within that interval. Outliers are plotted as separate dots.\n",
      " |\n",
      " |      For further details see\n",
      " |      Wikipedia's entry for `boxplot <https://en.wikipedia.org/wiki/Box_plot>`_.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      column : str or list of str, optional\n",
      " |          Column name or list of names, or vector.\n",
      " |          Can be any valid input to :meth:`pandas.DataFrame.groupby`.\n",
      " |      by : str or array-like, optional\n",
      " |          Column in the DataFrame to :meth:`pandas.DataFrame.groupby`.\n",
      " |          One box-plot will be done per value of columns in `by`.\n",
      " |      ax : object of class matplotlib.axes.Axes, optional\n",
      " |          The matplotlib axes to be used by boxplot.\n",
      " |      fontsize : float or str\n",
      " |          Tick label font size in points or as a string (e.g., `large`).\n",
      " |      rot : float, default 0\n",
      " |          The rotation angle of labels (in degrees)\n",
      " |          with respect to the screen coordinate system.\n",
      " |      grid : bool, default True\n",
      " |          Setting this to True will show the grid.\n",
      " |      figsize : A tuple (width, height) in inches\n",
      " |          The size of the figure to create in matplotlib.\n",
      " |      layout : tuple (rows, columns), optional\n",
      " |          For example, (3, 5) will display the subplots\n",
      " |          using 3 rows and 5 columns, starting from the top-left.\n",
      " |      return_type : {'axes', 'dict', 'both'} or None, default 'axes'\n",
      " |          The kind of object to return. The default is ``axes``.\n",
      " |\n",
      " |          * 'axes' returns the matplotlib axes the boxplot is drawn on.\n",
      " |          * 'dict' returns a dictionary whose values are the matplotlib\n",
      " |            Lines of the boxplot.\n",
      " |          * 'both' returns a namedtuple with the axes and dict.\n",
      " |          * when grouping with ``by``, a Series mapping columns to\n",
      " |            ``return_type`` is returned.\n",
      " |\n",
      " |            If ``return_type`` is `None`, a NumPy array\n",
      " |            of axes with the same shape as ``layout`` is returned.\n",
      " |      backend : str, default None\n",
      " |          Backend to use instead of the backend specified in the option\n",
      " |          ``plotting.backend``. For instance, 'matplotlib'. Alternatively, to\n",
      " |          specify the ``plotting.backend`` for the whole session, set\n",
      " |          ``pd.options.plotting.backend``.\n",
      " |\n",
      " |      **kwargs\n",
      " |          All other plotting keyword arguments to be passed to\n",
      " |          :func:`matplotlib.pyplot.boxplot`.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      result\n",
      " |          See Notes.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      pandas.Series.plot.hist: Make a histogram.\n",
      " |      matplotlib.pyplot.boxplot : Matplotlib equivalent plot.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      The return type depends on the `return_type` parameter:\n",
      " |\n",
      " |      * 'axes' : object of class matplotlib.axes.Axes\n",
      " |      * 'dict' : dict of matplotlib.lines.Line2D objects\n",
      " |      * 'both' : a namedtuple with structure (ax, lines)\n",
      " |\n",
      " |      For data grouped with ``by``, return a Series of the above or a numpy\n",
      " |      array:\n",
      " |\n",
      " |      * :class:`~pandas.Series`\n",
      " |      * :class:`~numpy.array` (for ``return_type = None``)\n",
      " |\n",
      " |      Use ``return_type='dict'`` when you want to tweak the appearance\n",
      " |      of the lines after plotting. In this case a dict containing the Lines\n",
      " |      making up the boxes, caps, fliers, medians, and whiskers is returned.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      Boxplots can be created for every column in the dataframe\n",
      " |      by ``df.boxplot()`` or indicating the columns to be used:\n",
      " |\n",
      " |      .. plot::\n",
      " |          :context: close-figs\n",
      " |\n",
      " |          >>> np.random.seed(1234)\n",
      " |          >>> df = pd.DataFrame(np.random.randn(10, 4),\n",
      " |          ...                   columns=['Col1', 'Col2', 'Col3', 'Col4'])\n",
      " |          >>> boxplot = df.boxplot(column=['Col1', 'Col2', 'Col3'])  # doctest: +SKIP\n",
      " |\n",
      " |      Boxplots of variables distributions grouped by the values of a third\n",
      " |      variable can be created using the option ``by``. For instance:\n",
      " |\n",
      " |      .. plot::\n",
      " |          :context: close-figs\n",
      " |\n",
      " |          >>> df = pd.DataFrame(np.random.randn(10, 2),\n",
      " |          ...                   columns=['Col1', 'Col2'])\n",
      " |          >>> df['X'] = pd.Series(['A', 'A', 'A', 'A', 'A',\n",
      " |          ...                      'B', 'B', 'B', 'B', 'B'])\n",
      " |          >>> boxplot = df.boxplot(by='X')\n",
      " |\n",
      " |      A list of strings (i.e. ``['X', 'Y']``) can be passed to boxplot\n",
      " |      in order to group the data by combination of the variables in the x-axis:\n",
      " |\n",
      " |      .. plot::\n",
      " |          :context: close-figs\n",
      " |\n",
      " |          >>> df = pd.DataFrame(np.random.randn(10, 3),\n",
      " |          ...                   columns=['Col1', 'Col2', 'Col3'])\n",
      " |          >>> df['X'] = pd.Series(['A', 'A', 'A', 'A', 'A',\n",
      " |          ...                      'B', 'B', 'B', 'B', 'B'])\n",
      " |          >>> df['Y'] = pd.Series(['A', 'B', 'A', 'B', 'A',\n",
      " |          ...                      'B', 'A', 'B', 'A', 'B'])\n",
      " |          >>> boxplot = df.boxplot(column=['Col1', 'Col2'], by=['X', 'Y'])\n",
      " |\n",
      " |      The layout of boxplot can be adjusted giving a tuple to ``layout``:\n",
      " |\n",
      " |      .. plot::\n",
      " |          :context: close-figs\n",
      " |\n",
      " |          >>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X',\n",
      " |          ...                      layout=(2, 1))\n",
      " |\n",
      " |      Additional formatting can be done to the boxplot, like suppressing the grid\n",
      " |      (``grid=False``), rotating the labels in the x-axis (i.e. ``rot=45``)\n",
      " |      or changing the fontsize (i.e. ``fontsize=15``):\n",
      " |\n",
      " |      .. plot::\n",
      " |          :context: close-figs\n",
      " |\n",
      " |          >>> boxplot = df.boxplot(grid=False, rot=45, fontsize=15)  # doctest: +SKIP\n",
      " |\n",
      " |      The parameter ``return_type`` can be used to select the type of element\n",
      " |      returned by `boxplot`.  When ``return_type='axes'`` is selected,\n",
      " |      the matplotlib axes on which the boxplot is drawn are returned:\n",
      " |\n",
      " |          >>> boxplot = df.boxplot(column=['Col1', 'Col2'], return_type='axes')\n",
      " |          >>> type(boxplot)\n",
      " |          <class 'matplotlib.axes._axes.Axes'>\n",
      " |\n",
      " |      When grouping with ``by``, a Series mapping columns to ``return_type``\n",
      " |      is returned:\n",
      " |\n",
      " |          >>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X',\n",
      " |          ...                      return_type='axes')\n",
      " |          >>> type(boxplot)\n",
      " |          <class 'pandas.core.series.Series'>\n",
      " |\n",
      " |      If ``return_type`` is `None`, a NumPy array of axes with the same shape\n",
      " |      as ``layout`` is returned:\n",
      " |\n",
      " |          >>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X',\n",
      " |          ...                      return_type=None)\n",
      " |          >>> type(boxplot)\n",
      " |          <class 'numpy.ndarray'>\n",
      " |\n",
      " |  combine(self, other: 'DataFrame', func: 'Callable[[Series, Series], Series | Hashable]', fill_value=None, overwrite: 'bool' = True) -> 'DataFrame'\n",
      " |      Perform column-wise combine with another DataFrame.\n",
      " |\n",
      " |      Combines a DataFrame with `other` DataFrame using `func`\n",
      " |      to element-wise combine columns. The row and column indexes of the\n",
      " |      resulting DataFrame will be the union of the two.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : DataFrame\n",
      " |          The DataFrame to merge column-wise.\n",
      " |      func : function\n",
      " |          Function that takes two series as inputs and return a Series or a\n",
      " |          scalar. Used to merge the two dataframes column by columns.\n",
      " |      fill_value : scalar value, default None\n",
      " |          The value to fill NaNs with prior to passing any column to the\n",
      " |          merge func.\n",
      " |      overwrite : bool, default True\n",
      " |          If True, columns in `self` that do not exist in `other` will be\n",
      " |          overwritten with NaNs.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Combination of the provided DataFrames.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.combine_first : Combine two DataFrame objects and default to\n",
      " |          non-null values in frame calling the method.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Combine using a simple function that chooses the smaller column.\n",
      " |\n",
      " |      >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})\n",
      " |      >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n",
      " |      >>> take_smaller = lambda s1, s2: s1 if s1.sum() < s2.sum() else s2\n",
      " |      >>> df1.combine(df2, take_smaller)\n",
      " |         A  B\n",
      " |      0  0  3\n",
      " |      1  0  3\n",
      " |\n",
      " |      Example using a true element-wise combine function.\n",
      " |\n",
      " |      >>> df1 = pd.DataFrame({'A': [5, 0], 'B': [2, 4]})\n",
      " |      >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n",
      " |      >>> df1.combine(df2, np.minimum)\n",
      " |         A  B\n",
      " |      0  1  2\n",
      " |      1  0  3\n",
      " |\n",
      " |      Using `fill_value` fills Nones prior to passing the column to the\n",
      " |      merge function.\n",
      " |\n",
      " |      >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})\n",
      " |      >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n",
      " |      >>> df1.combine(df2, take_smaller, fill_value=-5)\n",
      " |         A    B\n",
      " |      0  0 -5.0\n",
      " |      1  0  4.0\n",
      " |\n",
      " |      However, if the same element in both dataframes is None, that None\n",
      " |      is preserved\n",
      " |\n",
      " |      >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})\n",
      " |      >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [None, 3]})\n",
      " |      >>> df1.combine(df2, take_smaller, fill_value=-5)\n",
      " |          A    B\n",
      " |      0  0 -5.0\n",
      " |      1  0  3.0\n",
      " |\n",
      " |      Example that demonstrates the use of `overwrite` and behavior when\n",
      " |      the axis differ between the dataframes.\n",
      " |\n",
      " |      >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})\n",
      " |      >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [-10, 1], }, index=[1, 2])\n",
      " |      >>> df1.combine(df2, take_smaller)\n",
      " |           A    B     C\n",
      " |      0  NaN  NaN   NaN\n",
      " |      1  NaN  3.0 -10.0\n",
      " |      2  NaN  3.0   1.0\n",
      " |\n",
      " |      >>> df1.combine(df2, take_smaller, overwrite=False)\n",
      " |           A    B     C\n",
      " |      0  0.0  NaN   NaN\n",
      " |      1  0.0  3.0 -10.0\n",
      " |      2  NaN  3.0   1.0\n",
      " |\n",
      " |      Demonstrating the preference of the passed in dataframe.\n",
      " |\n",
      " |      >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1], }, index=[1, 2])\n",
      " |      >>> df2.combine(df1, take_smaller)\n",
      " |         A    B   C\n",
      " |      0  0.0  NaN NaN\n",
      " |      1  0.0  3.0 NaN\n",
      " |      2  NaN  3.0 NaN\n",
      " |\n",
      " |      >>> df2.combine(df1, take_smaller, overwrite=False)\n",
      " |           A    B   C\n",
      " |      0  0.0  NaN NaN\n",
      " |      1  0.0  3.0 1.0\n",
      " |      2  NaN  3.0 1.0\n",
      " |\n",
      " |  combine_first(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Update null elements with value in the same location in `other`.\n",
      " |\n",
      " |      Combine two DataFrame objects by filling null values in one DataFrame\n",
      " |      with non-null values from other DataFrame. The row and column indexes\n",
      " |      of the resulting DataFrame will be the union of the two. The resulting\n",
      " |      dataframe contains the 'first' dataframe values and overrides the\n",
      " |      second one values where both first.loc[index, col] and\n",
      " |      second.loc[index, col] are not missing values, upon calling\n",
      " |      first.combine_first(second).\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : DataFrame\n",
      " |          Provided DataFrame to use to fill null values.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          The result of combining the provided DataFrame with the other object.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.combine : Perform series-wise operation on two DataFrames\n",
      " |          using a given function.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = pd.DataFrame({'A': [None, 0], 'B': [None, 4]})\n",
      " |      >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n",
      " |      >>> df1.combine_first(df2)\n",
      " |           A    B\n",
      " |      0  1.0  3.0\n",
      " |      1  0.0  4.0\n",
      " |\n",
      " |      Null values still persist if the location of that null value\n",
      " |      does not exist in `other`\n",
      " |\n",
      " |      >>> df1 = pd.DataFrame({'A': [None, 0], 'B': [4, None]})\n",
      " |      >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1]}, index=[1, 2])\n",
      " |      >>> df1.combine_first(df2)\n",
      " |           A    B    C\n",
      " |      0  NaN  4.0  NaN\n",
      " |      1  0.0  3.0  1.0\n",
      " |      2  NaN  3.0  1.0\n",
      " |\n",
      " |  compare(self, other: 'DataFrame', align_axis: 'Axis' = 1, keep_shape: 'bool' = False, keep_equal: 'bool' = False, result_names: 'Suffixes' = ('self', 'other')) -> 'DataFrame'\n",
      " |      Compare to another DataFrame and show the differences.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : DataFrame\n",
      " |          Object to compare with.\n",
      " |\n",
      " |      align_axis : {0 or 'index', 1 or 'columns'}, default 1\n",
      " |          Determine which axis to align the comparison on.\n",
      " |\n",
      " |          * 0, or 'index' : Resulting differences are stacked vertically\n",
      " |              with rows drawn alternately from self and other.\n",
      " |          * 1, or 'columns' : Resulting differences are aligned horizontally\n",
      " |              with columns drawn alternately from self and other.\n",
      " |\n",
      " |      keep_shape : bool, default False\n",
      " |          If true, all rows and columns are kept.\n",
      " |          Otherwise, only the ones with different values are kept.\n",
      " |\n",
      " |      keep_equal : bool, default False\n",
      " |          If true, the result keeps values that are equal.\n",
      " |          Otherwise, equal values are shown as NaNs.\n",
      " |\n",
      " |      result_names : tuple, default ('self', 'other')\n",
      " |          Set the dataframes names in the comparison.\n",
      " |\n",
      " |          .. versionadded:: 1.5.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          DataFrame that shows the differences stacked side by side.\n",
      " |\n",
      " |          The resulting index will be a MultiIndex with 'self' and 'other'\n",
      " |          stacked alternately at the inner level.\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      ValueError\n",
      " |          When the two DataFrames don't have identical labels or shape.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.compare : Compare with another Series and show differences.\n",
      " |      DataFrame.equals : Test whether two objects contain the same elements.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Matching NaNs will not appear as a difference.\n",
      " |\n",
      " |      Can only compare identically-labeled\n",
      " |      (i.e. same shape, identical row and column labels) DataFrames\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame(\n",
      " |      ...     {\n",
      " |      ...         \"col1\": [\"a\", \"a\", \"b\", \"b\", \"a\"],\n",
      " |      ...         \"col2\": [1.0, 2.0, 3.0, np.nan, 5.0],\n",
      " |      ...         \"col3\": [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      " |      ...     },\n",
      " |      ...     columns=[\"col1\", \"col2\", \"col3\"],\n",
      " |      ... )\n",
      " |      >>> df\n",
      " |        col1  col2  col3\n",
      " |      0    a   1.0   1.0\n",
      " |      1    a   2.0   2.0\n",
      " |      2    b   3.0   3.0\n",
      " |      3    b   NaN   4.0\n",
      " |      4    a   5.0   5.0\n",
      " |\n",
      " |      >>> df2 = df.copy()\n",
      " |      >>> df2.loc[0, 'col1'] = 'c'\n",
      " |      >>> df2.loc[2, 'col3'] = 4.0\n",
      " |      >>> df2\n",
      " |        col1  col2  col3\n",
      " |      0    c   1.0   1.0\n",
      " |      1    a   2.0   2.0\n",
      " |      2    b   3.0   4.0\n",
      " |      3    b   NaN   4.0\n",
      " |      4    a   5.0   5.0\n",
      " |\n",
      " |      Align the differences on columns\n",
      " |\n",
      " |      >>> df.compare(df2)\n",
      " |        col1       col3\n",
      " |        self other self other\n",
      " |      0    a     c  NaN   NaN\n",
      " |      2  NaN   NaN  3.0   4.0\n",
      " |\n",
      " |      Assign result_names\n",
      " |\n",
      " |      >>> df.compare(df2, result_names=(\"left\", \"right\"))\n",
      " |        col1       col3\n",
      " |        left right left right\n",
      " |      0    a     c  NaN   NaN\n",
      " |      2  NaN   NaN  3.0   4.0\n",
      " |\n",
      " |      Stack the differences on rows\n",
      " |\n",
      " |      >>> df.compare(df2, align_axis=0)\n",
      " |              col1  col3\n",
      " |      0 self     a   NaN\n",
      " |        other    c   NaN\n",
      " |      2 self   NaN   3.0\n",
      " |        other  NaN   4.0\n",
      " |\n",
      " |      Keep the equal values\n",
      " |\n",
      " |      >>> df.compare(df2, keep_equal=True)\n",
      " |        col1       col3\n",
      " |        self other self other\n",
      " |      0    a     c  1.0   1.0\n",
      " |      2    b     b  3.0   4.0\n",
      " |\n",
      " |      Keep all original rows and columns\n",
      " |\n",
      " |      >>> df.compare(df2, keep_shape=True)\n",
      " |        col1       col2       col3\n",
      " |        self other self other self other\n",
      " |      0    a     c  NaN   NaN  NaN   NaN\n",
      " |      1  NaN   NaN  NaN   NaN  NaN   NaN\n",
      " |      2  NaN   NaN  NaN   NaN  3.0   4.0\n",
      " |      3  NaN   NaN  NaN   NaN  NaN   NaN\n",
      " |      4  NaN   NaN  NaN   NaN  NaN   NaN\n",
      " |\n",
      " |      Keep all original rows and columns and also all original values\n",
      " |\n",
      " |      >>> df.compare(df2, keep_shape=True, keep_equal=True)\n",
      " |        col1       col2       col3\n",
      " |        self other self other self other\n",
      " |      0    a     c  1.0   1.0  1.0   1.0\n",
      " |      1    a     a  2.0   2.0  2.0   2.0\n",
      " |      2    b     b  3.0   3.0  3.0   4.0\n",
      " |      3    b     b  NaN   NaN  4.0   4.0\n",
      " |      4    a     a  5.0   5.0  5.0   5.0\n",
      " |\n",
      " |  corr(self, method: 'CorrelationMethod' = 'pearson', min_periods: 'int' = 1, numeric_only: 'bool' = False) -> 'DataFrame'\n",
      " |      Compute pairwise correlation of columns, excluding NA/null values.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      method : {'pearson', 'kendall', 'spearman'} or callable\n",
      " |          Method of correlation:\n",
      " |\n",
      " |          * pearson : standard correlation coefficient\n",
      " |          * kendall : Kendall Tau correlation coefficient\n",
      " |          * spearman : Spearman rank correlation\n",
      " |          * callable: callable with input two 1d ndarrays\n",
      " |              and returning a float. Note that the returned matrix from corr\n",
      " |              will have 1 along the diagonals and will be symmetric\n",
      " |              regardless of the callable's behavior.\n",
      " |      min_periods : int, optional\n",
      " |          Minimum number of observations required per pair of columns\n",
      " |          to have a valid result. Currently only available for Pearson\n",
      " |          and Spearman correlation.\n",
      " |      numeric_only : bool, default False\n",
      " |          Include only `float`, `int` or `boolean` data.\n",
      " |\n",
      " |          .. versionadded:: 1.5.0\n",
      " |\n",
      " |          .. versionchanged:: 2.0.0\n",
      " |              The default value of ``numeric_only`` is now ``False``.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Correlation matrix.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.corrwith : Compute pairwise correlation with another\n",
      " |          DataFrame or Series.\n",
      " |      Series.corr : Compute the correlation between two Series.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Pearson, Kendall and Spearman correlation are currently computed using pairwise complete observations.\n",
      " |\n",
      " |      * `Pearson correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`_\n",
      " |      * `Kendall rank correlation coefficient <https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient>`_\n",
      " |      * `Spearman's rank correlation coefficient <https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient>`_\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> def histogram_intersection(a, b):\n",
      " |      ...     v = np.minimum(a, b).sum().round(decimals=1)\n",
      " |      ...     return v\n",
      " |      >>> df = pd.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)],\n",
      " |      ...                   columns=['dogs', 'cats'])\n",
      " |      >>> df.corr(method=histogram_intersection)\n",
      " |            dogs  cats\n",
      " |      dogs   1.0   0.3\n",
      " |      cats   0.3   1.0\n",
      " |\n",
      " |      >>> df = pd.DataFrame([(1, 1), (2, np.nan), (np.nan, 3), (4, 4)],\n",
      " |      ...                   columns=['dogs', 'cats'])\n",
      " |      >>> df.corr(min_periods=3)\n",
      " |            dogs  cats\n",
      " |      dogs   1.0   NaN\n",
      " |      cats   NaN   1.0\n",
      " |\n",
      " |  corrwith(self, other: 'DataFrame | Series', axis: 'Axis' = 0, drop: 'bool' = False, method: 'CorrelationMethod' = 'pearson', numeric_only: 'bool' = False) -> 'Series'\n",
      " |      Compute pairwise correlation.\n",
      " |\n",
      " |      Pairwise correlation is computed between rows or columns of\n",
      " |      DataFrame with rows or columns of Series or DataFrame. DataFrames\n",
      " |      are first aligned along both axes before computing the\n",
      " |      correlations.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : DataFrame, Series\n",
      " |          Object with which to compute correlations.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The axis to use. 0 or 'index' to compute row-wise, 1 or 'columns' for\n",
      " |          column-wise.\n",
      " |      drop : bool, default False\n",
      " |          Drop missing indices from result.\n",
      " |      method : {'pearson', 'kendall', 'spearman'} or callable\n",
      " |          Method of correlation:\n",
      " |\n",
      " |          * pearson : standard correlation coefficient\n",
      " |          * kendall : Kendall Tau correlation coefficient\n",
      " |          * spearman : Spearman rank correlation\n",
      " |          * callable: callable with input two 1d ndarrays\n",
      " |              and returning a float.\n",
      " |\n",
      " |      numeric_only : bool, default False\n",
      " |          Include only `float`, `int` or `boolean` data.\n",
      " |\n",
      " |          .. versionadded:: 1.5.0\n",
      " |\n",
      " |          .. versionchanged:: 2.0.0\n",
      " |              The default value of ``numeric_only`` is now ``False``.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series\n",
      " |          Pairwise correlations.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.corr : Compute pairwise correlation of columns.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> index = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
      " |      >>> columns = [\"one\", \"two\", \"three\", \"four\"]\n",
      " |      >>> df1 = pd.DataFrame(np.arange(20).reshape(5, 4), index=index, columns=columns)\n",
      " |      >>> df2 = pd.DataFrame(np.arange(16).reshape(4, 4), index=index[:4], columns=columns)\n",
      " |      >>> df1.corrwith(df2)\n",
      " |      one      1.0\n",
      " |      two      1.0\n",
      " |      three    1.0\n",
      " |      four     1.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |      >>> df2.corrwith(df1, axis=1)\n",
      " |      a    1.0\n",
      " |      b    1.0\n",
      " |      c    1.0\n",
      " |      d    1.0\n",
      " |      e    NaN\n",
      " |      dtype: float64\n",
      " |\n",
      " |  count(self, axis: 'Axis' = 0, numeric_only: 'bool' = False)\n",
      " |      Count non-NA cells for each column or row.\n",
      " |\n",
      " |      The values `None`, `NaN`, `NaT`, ``pandas.NA`` are considered NA.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          If 0 or 'index' counts are generated for each column.\n",
      " |          If 1 or 'columns' counts are generated for each row.\n",
      " |      numeric_only : bool, default False\n",
      " |          Include only `float`, `int` or `boolean` data.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series\n",
      " |          For each column/row the number of non-NA/null entries.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.count: Number of non-NA elements in a Series.\n",
      " |      DataFrame.value_counts: Count unique combinations of columns.\n",
      " |      DataFrame.shape: Number of DataFrame rows and columns (including NA\n",
      " |          elements).\n",
      " |      DataFrame.isna: Boolean same-sized DataFrame showing places of NA\n",
      " |          elements.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Constructing DataFrame from a dictionary:\n",
      " |\n",
      " |      >>> df = pd.DataFrame({\"Person\":\n",
      " |      ...                    [\"John\", \"Myla\", \"Lewis\", \"John\", \"Myla\"],\n",
      " |      ...                    \"Age\": [24., np.nan, 21., 33, 26],\n",
      " |      ...                    \"Single\": [False, True, True, True, False]})\n",
      " |      >>> df\n",
      " |         Person   Age  Single\n",
      " |      0    John  24.0   False\n",
      " |      1    Myla   NaN    True\n",
      " |      2   Lewis  21.0    True\n",
      " |      3    John  33.0    True\n",
      " |      4    Myla  26.0   False\n",
      " |\n",
      " |      Notice the uncounted NA values:\n",
      " |\n",
      " |      >>> df.count()\n",
      " |      Person    5\n",
      " |      Age       4\n",
      " |      Single    5\n",
      " |      dtype: int64\n",
      " |\n",
      " |      Counts for each **row**:\n",
      " |\n",
      " |      >>> df.count(axis='columns')\n",
      " |      0    3\n",
      " |      1    2\n",
      " |      2    3\n",
      " |      3    3\n",
      " |      4    3\n",
      " |      dtype: int64\n",
      " |\n",
      " |  cov(self, min_periods: 'int | None' = None, ddof: 'int | None' = 1, numeric_only: 'bool' = False) -> 'DataFrame'\n",
      " |      Compute pairwise covariance of columns, excluding NA/null values.\n",
      " |\n",
      " |      Compute the pairwise covariance among the series of a DataFrame.\n",
      " |      The returned data frame is the `covariance matrix\n",
      " |      <https://en.wikipedia.org/wiki/Covariance_matrix>`__ of the columns\n",
      " |      of the DataFrame.\n",
      " |\n",
      " |      Both NA and null values are automatically excluded from the\n",
      " |      calculation. (See the note below about bias from missing values.)\n",
      " |      A threshold can be set for the minimum number of\n",
      " |      observations for each value created. Comparisons with observations\n",
      " |      below this threshold will be returned as ``NaN``.\n",
      " |\n",
      " |      This method is generally used for the analysis of time series data to\n",
      " |      understand the relationship between different measures\n",
      " |      across time.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      min_periods : int, optional\n",
      " |          Minimum number of observations required per pair of columns\n",
      " |          to have a valid result.\n",
      " |\n",
      " |      ddof : int, default 1\n",
      " |          Delta degrees of freedom.  The divisor used in calculations\n",
      " |          is ``N - ddof``, where ``N`` represents the number of elements.\n",
      " |          This argument is applicable only when no ``nan`` is in the dataframe.\n",
      " |\n",
      " |      numeric_only : bool, default False\n",
      " |          Include only `float`, `int` or `boolean` data.\n",
      " |\n",
      " |          .. versionadded:: 1.5.0\n",
      " |\n",
      " |          .. versionchanged:: 2.0.0\n",
      " |              The default value of ``numeric_only`` is now ``False``.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          The covariance matrix of the series of the DataFrame.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.cov : Compute covariance with another Series.\n",
      " |      core.window.ewm.ExponentialMovingWindow.cov : Exponential weighted sample\n",
      " |          covariance.\n",
      " |      core.window.expanding.Expanding.cov : Expanding sample covariance.\n",
      " |      core.window.rolling.Rolling.cov : Rolling sample covariance.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Returns the covariance matrix of the DataFrame's time series.\n",
      " |      The covariance is normalized by N-ddof.\n",
      " |\n",
      " |      For DataFrames that have Series that are missing data (assuming that\n",
      " |      data is `missing at random\n",
      " |      <https://en.wikipedia.org/wiki/Missing_data#Missing_at_random>`__)\n",
      " |      the returned covariance matrix will be an unbiased estimate\n",
      " |      of the variance and covariance between the member Series.\n",
      " |\n",
      " |      However, for many applications this estimate may not be acceptable\n",
      " |      because the estimate covariance matrix is not guaranteed to be positive\n",
      " |      semi-definite. This could lead to estimate correlations having\n",
      " |      absolute values which are greater than one, and/or a non-invertible\n",
      " |      covariance matrix. See `Estimation of covariance matrices\n",
      " |      <https://en.wikipedia.org/w/index.php?title=Estimation_of_covariance_\n",
      " |      matrices>`__ for more details.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([(1, 2), (0, 3), (2, 0), (1, 1)],\n",
      " |      ...                   columns=['dogs', 'cats'])\n",
      " |      >>> df.cov()\n",
      " |                dogs      cats\n",
      " |      dogs  0.666667 -1.000000\n",
      " |      cats -1.000000  1.666667\n",
      " |\n",
      " |      >>> np.random.seed(42)\n",
      " |      >>> df = pd.DataFrame(np.random.randn(1000, 5),\n",
      " |      ...                   columns=['a', 'b', 'c', 'd', 'e'])\n",
      " |      >>> df.cov()\n",
      " |                a         b         c         d         e\n",
      " |      a  0.998438 -0.020161  0.059277 -0.008943  0.014144\n",
      " |      b -0.020161  1.059352 -0.008543 -0.024738  0.009826\n",
      " |      c  0.059277 -0.008543  1.010670 -0.001486 -0.000271\n",
      " |      d -0.008943 -0.024738 -0.001486  0.921297 -0.013692\n",
      " |      e  0.014144  0.009826 -0.000271 -0.013692  0.977795\n",
      " |\n",
      " |      **Minimum number of periods**\n",
      " |\n",
      " |      This method also supports an optional ``min_periods`` keyword\n",
      " |      that specifies the required minimum number of non-NA observations for\n",
      " |      each column pair in order to have a valid result:\n",
      " |\n",
      " |      >>> np.random.seed(42)\n",
      " |      >>> df = pd.DataFrame(np.random.randn(20, 3),\n",
      " |      ...                   columns=['a', 'b', 'c'])\n",
      " |      >>> df.loc[df.index[:5], 'a'] = np.nan\n",
      " |      >>> df.loc[df.index[5:10], 'b'] = np.nan\n",
      " |      >>> df.cov(min_periods=12)\n",
      " |                a         b         c\n",
      " |      a  0.316741       NaN -0.150812\n",
      " |      b       NaN  1.248003  0.191417\n",
      " |      c -0.150812  0.191417  0.895202\n",
      " |\n",
      " |  cummax(self, axis: 'Axis | None' = None, skipna: 'bool' = True, *args, **kwargs)\n",
      " |      Return cumulative maximum over a DataFrame or Series axis.\n",
      " |\n",
      " |      Returns a DataFrame or Series of the same size containing the cumulative\n",
      " |      maximum.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The index or the name of the axis. 0 is equivalent to None or 'index'.\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If an entire row/column is NA, the result\n",
      " |          will be NA.\n",
      " |      *args, **kwargs\n",
      " |          Additional keywords have no effect but might be accepted for\n",
      " |          compatibility with NumPy.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          Return cumulative maximum of Series or DataFrame.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      core.window.expanding.Expanding.max : Similar functionality\n",
      " |          but ignores ``NaN`` values.\n",
      " |      DataFrame.max : Return the maximum over\n",
      " |          DataFrame axis.\n",
      " |      DataFrame.cummax : Return cumulative maximum over DataFrame axis.\n",
      " |      DataFrame.cummin : Return cumulative minimum over DataFrame axis.\n",
      " |      DataFrame.cumsum : Return cumulative sum over DataFrame axis.\n",
      " |      DataFrame.cumprod : Return cumulative product over DataFrame axis.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      **Series**\n",
      " |\n",
      " |      >>> s = pd.Series([2, np.nan, 5, -1, 0])\n",
      " |      >>> s\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    5.0\n",
      " |      3   -1.0\n",
      " |      4    0.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |      By default, NA values are ignored.\n",
      " |\n",
      " |      >>> s.cummax()\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    5.0\n",
      " |      3    5.0\n",
      " |      4    5.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |      To include NA values in the operation, use ``skipna=False``\n",
      " |\n",
      " |      >>> s.cummax(skipna=False)\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    NaN\n",
      " |      3    NaN\n",
      " |      4    NaN\n",
      " |      dtype: float64\n",
      " |\n",
      " |      **DataFrame**\n",
      " |\n",
      " |      >>> df = pd.DataFrame([[2.0, 1.0],\n",
      " |      ...                    [3.0, np.nan],\n",
      " |      ...                    [1.0, 0.0]],\n",
      " |      ...                   columns=list('AB'))\n",
      " |      >>> df\n",
      " |           A    B\n",
      " |      0  2.0  1.0\n",
      " |      1  3.0  NaN\n",
      " |      2  1.0  0.0\n",
      " |\n",
      " |      By default, iterates over rows and finds the maximum\n",
      " |      in each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n",
      " |\n",
      " |      >>> df.cummax()\n",
      " |           A    B\n",
      " |      0  2.0  1.0\n",
      " |      1  3.0  NaN\n",
      " |      2  3.0  1.0\n",
      " |\n",
      " |      To iterate over columns and find the maximum in each row,\n",
      " |      use ``axis=1``\n",
      " |\n",
      " |      >>> df.cummax(axis=1)\n",
      " |           A    B\n",
      " |      0  2.0  2.0\n",
      " |      1  3.0  NaN\n",
      " |      2  1.0  1.0\n",
      " |\n",
      " |  cummin(self, axis: 'Axis | None' = None, skipna: 'bool' = True, *args, **kwargs)\n",
      " |      Return cumulative minimum over a DataFrame or Series axis.\n",
      " |\n",
      " |      Returns a DataFrame or Series of the same size containing the cumulative\n",
      " |      minimum.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The index or the name of the axis. 0 is equivalent to None or 'index'.\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If an entire row/column is NA, the result\n",
      " |          will be NA.\n",
      " |      *args, **kwargs\n",
      " |          Additional keywords have no effect but might be accepted for\n",
      " |          compatibility with NumPy.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          Return cumulative minimum of Series or DataFrame.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      core.window.expanding.Expanding.min : Similar functionality\n",
      " |          but ignores ``NaN`` values.\n",
      " |      DataFrame.min : Return the minimum over\n",
      " |          DataFrame axis.\n",
      " |      DataFrame.cummax : Return cumulative maximum over DataFrame axis.\n",
      " |      DataFrame.cummin : Return cumulative minimum over DataFrame axis.\n",
      " |      DataFrame.cumsum : Return cumulative sum over DataFrame axis.\n",
      " |      DataFrame.cumprod : Return cumulative product over DataFrame axis.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      **Series**\n",
      " |\n",
      " |      >>> s = pd.Series([2, np.nan, 5, -1, 0])\n",
      " |      >>> s\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    5.0\n",
      " |      3   -1.0\n",
      " |      4    0.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |      By default, NA values are ignored.\n",
      " |\n",
      " |      >>> s.cummin()\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    2.0\n",
      " |      3   -1.0\n",
      " |      4   -1.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |      To include NA values in the operation, use ``skipna=False``\n",
      " |\n",
      " |      >>> s.cummin(skipna=False)\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    NaN\n",
      " |      3    NaN\n",
      " |      4    NaN\n",
      " |      dtype: float64\n",
      " |\n",
      " |      **DataFrame**\n",
      " |\n",
      " |      >>> df = pd.DataFrame([[2.0, 1.0],\n",
      " |      ...                    [3.0, np.nan],\n",
      " |      ...                    [1.0, 0.0]],\n",
      " |      ...                   columns=list('AB'))\n",
      " |      >>> df\n",
      " |           A    B\n",
      " |      0  2.0  1.0\n",
      " |      1  3.0  NaN\n",
      " |      2  1.0  0.0\n",
      " |\n",
      " |      By default, iterates over rows and finds the minimum\n",
      " |      in each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n",
      " |\n",
      " |      >>> df.cummin()\n",
      " |           A    B\n",
      " |      0  2.0  1.0\n",
      " |      1  2.0  NaN\n",
      " |      2  1.0  0.0\n",
      " |\n",
      " |      To iterate over columns and find the minimum in each row,\n",
      " |      use ``axis=1``\n",
      " |\n",
      " |      >>> df.cummin(axis=1)\n",
      " |           A    B\n",
      " |      0  2.0  1.0\n",
      " |      1  3.0  NaN\n",
      " |      2  1.0  0.0\n",
      " |\n",
      " |  cumprod(self, axis: 'Axis | None' = None, skipna: 'bool' = True, *args, **kwargs)\n",
      " |      Return cumulative product over a DataFrame or Series axis.\n",
      " |\n",
      " |      Returns a DataFrame or Series of the same size containing the cumulative\n",
      " |      product.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The index or the name of the axis. 0 is equivalent to None or 'index'.\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If an entire row/column is NA, the result\n",
      " |          will be NA.\n",
      " |      *args, **kwargs\n",
      " |          Additional keywords have no effect but might be accepted for\n",
      " |          compatibility with NumPy.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          Return cumulative product of Series or DataFrame.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      core.window.expanding.Expanding.prod : Similar functionality\n",
      " |          but ignores ``NaN`` values.\n",
      " |      DataFrame.prod : Return the product over\n",
      " |          DataFrame axis.\n",
      " |      DataFrame.cummax : Return cumulative maximum over DataFrame axis.\n",
      " |      DataFrame.cummin : Return cumulative minimum over DataFrame axis.\n",
      " |      DataFrame.cumsum : Return cumulative sum over DataFrame axis.\n",
      " |      DataFrame.cumprod : Return cumulative product over DataFrame axis.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      **Series**\n",
      " |\n",
      " |      >>> s = pd.Series([2, np.nan, 5, -1, 0])\n",
      " |      >>> s\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    5.0\n",
      " |      3   -1.0\n",
      " |      4    0.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |      By default, NA values are ignored.\n",
      " |\n",
      " |      >>> s.cumprod()\n",
      " |      0     2.0\n",
      " |      1     NaN\n",
      " |      2    10.0\n",
      " |      3   -10.0\n",
      " |      4    -0.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |      To include NA values in the operation, use ``skipna=False``\n",
      " |\n",
      " |      >>> s.cumprod(skipna=False)\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    NaN\n",
      " |      3    NaN\n",
      " |      4    NaN\n",
      " |      dtype: float64\n",
      " |\n",
      " |      **DataFrame**\n",
      " |\n",
      " |      >>> df = pd.DataFrame([[2.0, 1.0],\n",
      " |      ...                    [3.0, np.nan],\n",
      " |      ...                    [1.0, 0.0]],\n",
      " |      ...                   columns=list('AB'))\n",
      " |      >>> df\n",
      " |           A    B\n",
      " |      0  2.0  1.0\n",
      " |      1  3.0  NaN\n",
      " |      2  1.0  0.0\n",
      " |\n",
      " |      By default, iterates over rows and finds the product\n",
      " |      in each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n",
      " |\n",
      " |      >>> df.cumprod()\n",
      " |           A    B\n",
      " |      0  2.0  1.0\n",
      " |      1  6.0  NaN\n",
      " |      2  6.0  0.0\n",
      " |\n",
      " |      To iterate over columns and find the product in each row,\n",
      " |      use ``axis=1``\n",
      " |\n",
      " |      >>> df.cumprod(axis=1)\n",
      " |           A    B\n",
      " |      0  2.0  2.0\n",
      " |      1  3.0  NaN\n",
      " |      2  1.0  0.0\n",
      " |\n",
      " |  cumsum(self, axis: 'Axis | None' = None, skipna: 'bool' = True, *args, **kwargs)\n",
      " |      Return cumulative sum over a DataFrame or Series axis.\n",
      " |\n",
      " |      Returns a DataFrame or Series of the same size containing the cumulative\n",
      " |      sum.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The index or the name of the axis. 0 is equivalent to None or 'index'.\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If an entire row/column is NA, the result\n",
      " |          will be NA.\n",
      " |      *args, **kwargs\n",
      " |          Additional keywords have no effect but might be accepted for\n",
      " |          compatibility with NumPy.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          Return cumulative sum of Series or DataFrame.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      core.window.expanding.Expanding.sum : Similar functionality\n",
      " |          but ignores ``NaN`` values.\n",
      " |      DataFrame.sum : Return the sum over\n",
      " |          DataFrame axis.\n",
      " |      DataFrame.cummax : Return cumulative maximum over DataFrame axis.\n",
      " |      DataFrame.cummin : Return cumulative minimum over DataFrame axis.\n",
      " |      DataFrame.cumsum : Return cumulative sum over DataFrame axis.\n",
      " |      DataFrame.cumprod : Return cumulative product over DataFrame axis.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      **Series**\n",
      " |\n",
      " |      >>> s = pd.Series([2, np.nan, 5, -1, 0])\n",
      " |      >>> s\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    5.0\n",
      " |      3   -1.0\n",
      " |      4    0.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |      By default, NA values are ignored.\n",
      " |\n",
      " |      >>> s.cumsum()\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    7.0\n",
      " |      3    6.0\n",
      " |      4    6.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |      To include NA values in the operation, use ``skipna=False``\n",
      " |\n",
      " |      >>> s.cumsum(skipna=False)\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    NaN\n",
      " |      3    NaN\n",
      " |      4    NaN\n",
      " |      dtype: float64\n",
      " |\n",
      " |      **DataFrame**\n",
      " |\n",
      " |      >>> df = pd.DataFrame([[2.0, 1.0],\n",
      " |      ...                    [3.0, np.nan],\n",
      " |      ...                    [1.0, 0.0]],\n",
      " |      ...                   columns=list('AB'))\n",
      " |      >>> df\n",
      " |           A    B\n",
      " |      0  2.0  1.0\n",
      " |      1  3.0  NaN\n",
      " |      2  1.0  0.0\n",
      " |\n",
      " |      By default, iterates over rows and finds the sum\n",
      " |      in each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n",
      " |\n",
      " |      >>> df.cumsum()\n",
      " |           A    B\n",
      " |      0  2.0  1.0\n",
      " |      1  5.0  NaN\n",
      " |      2  6.0  1.0\n",
      " |\n",
      " |      To iterate over columns and find the sum in each row,\n",
      " |      use ``axis=1``\n",
      " |\n",
      " |      >>> df.cumsum(axis=1)\n",
      " |           A    B\n",
      " |      0  2.0  3.0\n",
      " |      1  3.0  NaN\n",
      " |      2  1.0  1.0\n",
      " |\n",
      " |  diff(self, periods: 'int' = 1, axis: 'Axis' = 0) -> 'DataFrame'\n",
      " |      First discrete difference of element.\n",
      " |\n",
      " |      Calculates the difference of a DataFrame element compared with another\n",
      " |      element in the DataFrame (default is element in previous row).\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      periods : int, default 1\n",
      " |          Periods to shift for calculating difference, accepts negative\n",
      " |          values.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          Take difference over rows (0) or columns (1).\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          First differences of the Series.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.pct_change: Percent change over given number of periods.\n",
      " |      DataFrame.shift: Shift index by desired number of periods with an\n",
      " |          optional time freq.\n",
      " |      Series.diff: First discrete difference of object.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      For boolean dtypes, this uses :meth:`operator.xor` rather than\n",
      " |      :meth:`operator.sub`.\n",
      " |      The result is calculated according to current dtype in DataFrame,\n",
      " |      however dtype of the result is always float64.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      Difference with previous row\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6],\n",
      " |      ...                    'b': [1, 1, 2, 3, 5, 8],\n",
      " |      ...                    'c': [1, 4, 9, 16, 25, 36]})\n",
      " |      >>> df\n",
      " |         a  b   c\n",
      " |      0  1  1   1\n",
      " |      1  2  1   4\n",
      " |      2  3  2   9\n",
      " |      3  4  3  16\n",
      " |      4  5  5  25\n",
      " |      5  6  8  36\n",
      " |\n",
      " |      >>> df.diff()\n",
      " |           a    b     c\n",
      " |      0  NaN  NaN   NaN\n",
      " |      1  1.0  0.0   3.0\n",
      " |      2  1.0  1.0   5.0\n",
      " |      3  1.0  1.0   7.0\n",
      " |      4  1.0  2.0   9.0\n",
      " |      5  1.0  3.0  11.0\n",
      " |\n",
      " |      Difference with previous column\n",
      " |\n",
      " |      >>> df.diff(axis=1)\n",
      " |          a  b   c\n",
      " |      0 NaN  0   0\n",
      " |      1 NaN -1   3\n",
      " |      2 NaN -1   7\n",
      " |      3 NaN -1  13\n",
      " |      4 NaN  0  20\n",
      " |      5 NaN  2  28\n",
      " |\n",
      " |      Difference with 3rd previous row\n",
      " |\n",
      " |      >>> df.diff(periods=3)\n",
      " |           a    b     c\n",
      " |      0  NaN  NaN   NaN\n",
      " |      1  NaN  NaN   NaN\n",
      " |      2  NaN  NaN   NaN\n",
      " |      3  3.0  2.0  15.0\n",
      " |      4  3.0  4.0  21.0\n",
      " |      5  3.0  6.0  27.0\n",
      " |\n",
      " |      Difference with following row\n",
      " |\n",
      " |      >>> df.diff(periods=-1)\n",
      " |           a    b     c\n",
      " |      0 -1.0  0.0  -3.0\n",
      " |      1 -1.0 -1.0  -5.0\n",
      " |      2 -1.0 -1.0  -7.0\n",
      " |      3 -1.0 -2.0  -9.0\n",
      " |      4 -1.0 -3.0 -11.0\n",
      " |      5  NaN  NaN   NaN\n",
      " |\n",
      " |      Overflow in input dtype\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'a': [1, 0]}, dtype=np.uint8)\n",
      " |      >>> df.diff()\n",
      " |             a\n",
      " |      0    NaN\n",
      " |      1  255.0\n",
      " |\n",
      " |  div = truediv(self, other, axis: 'Axis' = 'columns', level=None, fill_value=None) -> 'DataFrame'\n",
      " |\n",
      " |  divide = truediv(self, other, axis: 'Axis' = 'columns', level=None, fill_value=None) -> 'DataFrame'\n",
      " |\n",
      " |  dot(self, other: 'AnyArrayLike | DataFrame') -> 'DataFrame | Series'\n",
      " |      Compute the matrix multiplication between the DataFrame and other.\n",
      " |\n",
      " |      This method computes the matrix product between the DataFrame and the\n",
      " |      values of an other Series, DataFrame or a numpy array.\n",
      " |\n",
      " |      It can also be called using ``self @ other``.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Series, DataFrame or array-like\n",
      " |          The other object to compute the matrix product with.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          If other is a Series, return the matrix product between self and\n",
      " |          other as a Series. If other is a DataFrame or a numpy.array, return\n",
      " |          the matrix product of self and other in a DataFrame of a np.array.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.dot: Similar method for Series.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      The dimensions of DataFrame and other must be compatible in order to\n",
      " |      compute the matrix multiplication. In addition, the column names of\n",
      " |      DataFrame and the index of other must contain the same values, as they\n",
      " |      will be aligned prior to the multiplication.\n",
      " |\n",
      " |      The dot method for Series computes the inner product, instead of the\n",
      " |      matrix product here.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Here we multiply a DataFrame with a Series.\n",
      " |\n",
      " |      >>> df = pd.DataFrame([[0, 1, -2, -1], [1, 1, 1, 1]])\n",
      " |      >>> s = pd.Series([1, 1, 2, 1])\n",
      " |      >>> df.dot(s)\n",
      " |      0    -4\n",
      " |      1     5\n",
      " |      dtype: int64\n",
      " |\n",
      " |      Here we multiply a DataFrame with another DataFrame.\n",
      " |\n",
      " |      >>> other = pd.DataFrame([[0, 1], [1, 2], [-1, -1], [2, 0]])\n",
      " |      >>> df.dot(other)\n",
      " |          0   1\n",
      " |      0   1   4\n",
      " |      1   2   2\n",
      " |\n",
      " |      Note that the dot method give the same result as @\n",
      " |\n",
      " |      >>> df @ other\n",
      " |          0   1\n",
      " |      0   1   4\n",
      " |      1   2   2\n",
      " |\n",
      " |      The dot method works also if other is an np.array.\n",
      " |\n",
      " |      >>> arr = np.array([[0, 1], [1, 2], [-1, -1], [2, 0]])\n",
      " |      >>> df.dot(arr)\n",
      " |          0   1\n",
      " |      0   1   4\n",
      " |      1   2   2\n",
      " |\n",
      " |      Note how shuffling of the objects does not change the result.\n",
      " |\n",
      " |      >>> s2 = s.reindex([1, 0, 2, 3])\n",
      " |      >>> df.dot(s2)\n",
      " |      0    -4\n",
      " |      1     5\n",
      " |      dtype: int64\n",
      " |\n",
      " |  drop(self, labels: 'IndexLabel | None' = None, *, axis: 'Axis' = 0, index: 'IndexLabel | None' = None, columns: 'IndexLabel | None' = None, level: 'Level | None' = None, inplace: 'bool' = False, errors: 'IgnoreRaise' = 'raise') -> 'DataFrame | None'\n",
      " |      Drop specified labels from rows or columns.\n",
      " |\n",
      " |      Remove rows or columns by specifying label names and corresponding\n",
      " |      axis, or by directly specifying index or column names. When using a\n",
      " |      multi-index, labels on different levels can be removed by specifying\n",
      " |      the level. See the :ref:`user guide <advanced.shown_levels>`\n",
      " |      for more information about the now unused levels.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      labels : single label or list-like\n",
      " |          Index or column labels to drop. A tuple will be used as a single\n",
      " |          label and not treated as a list-like.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          Whether to drop labels from the index (0 or 'index') or\n",
      " |          columns (1 or 'columns').\n",
      " |      index : single label or list-like\n",
      " |          Alternative to specifying axis (``labels, axis=0``\n",
      " |          is equivalent to ``index=labels``).\n",
      " |      columns : single label or list-like\n",
      " |          Alternative to specifying axis (``labels, axis=1``\n",
      " |          is equivalent to ``columns=labels``).\n",
      " |      level : int or level name, optional\n",
      " |          For MultiIndex, level from which the labels will be removed.\n",
      " |      inplace : bool, default False\n",
      " |          If False, return a copy. Otherwise, do operation\n",
      " |          in place and return None.\n",
      " |      errors : {'ignore', 'raise'}, default 'raise'\n",
      " |          If 'ignore', suppress error and only existing labels are\n",
      " |          dropped.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame or None\n",
      " |          Returns DataFrame or None DataFrame with the specified\n",
      " |          index or column labels removed or None if inplace=True.\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If any of the labels is not found in the selected axis.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.loc : Label-location based indexer for selection by label.\n",
      " |      DataFrame.dropna : Return DataFrame with labels on given axis omitted\n",
      " |          where (all or any) data are missing.\n",
      " |      DataFrame.drop_duplicates : Return DataFrame with duplicate rows\n",
      " |          removed, optionally only considering certain columns.\n",
      " |      Series.drop : Return Series with specified index labels removed.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame(np.arange(12).reshape(3, 4),\n",
      " |      ...                   columns=['A', 'B', 'C', 'D'])\n",
      " |      >>> df\n",
      " |         A  B   C   D\n",
      " |      0  0  1   2   3\n",
      " |      1  4  5   6   7\n",
      " |      2  8  9  10  11\n",
      " |\n",
      " |      Drop columns\n",
      " |\n",
      " |      >>> df.drop(['B', 'C'], axis=1)\n",
      " |         A   D\n",
      " |      0  0   3\n",
      " |      1  4   7\n",
      " |      2  8  11\n",
      " |\n",
      " |      >>> df.drop(columns=['B', 'C'])\n",
      " |         A   D\n",
      " |      0  0   3\n",
      " |      1  4   7\n",
      " |      2  8  11\n",
      " |\n",
      " |      Drop a row by index\n",
      " |\n",
      " |      >>> df.drop([0, 1])\n",
      " |         A  B   C   D\n",
      " |      2  8  9  10  11\n",
      " |\n",
      " |      Drop columns and/or rows of MultiIndex DataFrame\n",
      " |\n",
      " |      >>> midx = pd.MultiIndex(levels=[['llama', 'cow', 'falcon'],\n",
      " |      ...                              ['speed', 'weight', 'length']],\n",
      " |      ...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
      " |      ...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n",
      " |      >>> df = pd.DataFrame(index=midx, columns=['big', 'small'],\n",
      " |      ...                   data=[[45, 30], [200, 100], [1.5, 1], [30, 20],\n",
      " |      ...                         [250, 150], [1.5, 0.8], [320, 250],\n",
      " |      ...                         [1, 0.8], [0.3, 0.2]])\n",
      " |      >>> df\n",
      " |                      big     small\n",
      " |      llama   speed   45.0    30.0\n",
      " |              weight  200.0   100.0\n",
      " |              length  1.5     1.0\n",
      " |      cow     speed   30.0    20.0\n",
      " |              weight  250.0   150.0\n",
      " |              length  1.5     0.8\n",
      " |      falcon  speed   320.0   250.0\n",
      " |              weight  1.0     0.8\n",
      " |              length  0.3     0.2\n",
      " |\n",
      " |      Drop a specific index combination from the MultiIndex\n",
      " |      DataFrame, i.e., drop the combination ``'falcon'`` and\n",
      " |      ``'weight'``, which deletes only the corresponding row\n",
      " |\n",
      " |      >>> df.drop(index=('falcon', 'weight'))\n",
      " |                      big     small\n",
      " |      llama   speed   45.0    30.0\n",
      " |              weight  200.0   100.0\n",
      " |              length  1.5     1.0\n",
      " |      cow     speed   30.0    20.0\n",
      " |              weight  250.0   150.0\n",
      " |              length  1.5     0.8\n",
      " |      falcon  speed   320.0   250.0\n",
      " |              length  0.3     0.2\n",
      " |\n",
      " |      >>> df.drop(index='cow', columns='small')\n",
      " |                      big\n",
      " |      llama   speed   45.0\n",
      " |              weight  200.0\n",
      " |              length  1.5\n",
      " |      falcon  speed   320.0\n",
      " |              weight  1.0\n",
      " |              length  0.3\n",
      " |\n",
      " |      >>> df.drop(index='length', level=1)\n",
      " |                      big     small\n",
      " |      llama   speed   45.0    30.0\n",
      " |              weight  200.0   100.0\n",
      " |      cow     speed   30.0    20.0\n",
      " |              weight  250.0   150.0\n",
      " |      falcon  speed   320.0   250.0\n",
      " |              weight  1.0     0.8\n",
      " |\n",
      " |  drop_duplicates(self, subset: 'Hashable | Sequence[Hashable] | None' = None, *, keep: 'DropKeep' = 'first', inplace: 'bool' = False, ignore_index: 'bool' = False) -> 'DataFrame | None'\n",
      " |      Return DataFrame with duplicate rows removed.\n",
      " |\n",
      " |      Considering certain columns is optional. Indexes, including time indexes\n",
      " |      are ignored.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      subset : column label or sequence of labels, optional\n",
      " |          Only consider certain columns for identifying duplicates, by\n",
      " |          default use all of the columns.\n",
      " |      keep : {'first', 'last', ``False``}, default 'first'\n",
      " |          Determines which duplicates (if any) to keep.\n",
      " |\n",
      " |          - 'first' : Drop duplicates except for the first occurrence.\n",
      " |          - 'last' : Drop duplicates except for the last occurrence.\n",
      " |          - ``False`` : Drop all duplicates.\n",
      " |\n",
      " |      inplace : bool, default ``False``\n",
      " |          Whether to modify the DataFrame rather than creating a new one.\n",
      " |      ignore_index : bool, default ``False``\n",
      " |          If ``True``, the resulting axis will be labeled 0, 1, …, n - 1.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame or None\n",
      " |          DataFrame with duplicates removed or None if ``inplace=True``.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.value_counts: Count unique combinations of columns.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Consider dataset containing ramen rating.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({\n",
      " |      ...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n",
      " |      ...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n",
      " |      ...     'rating': [4, 4, 3.5, 15, 5]\n",
      " |      ... })\n",
      " |      >>> df\n",
      " |          brand style  rating\n",
      " |      0  Yum Yum   cup     4.0\n",
      " |      1  Yum Yum   cup     4.0\n",
      " |      2  Indomie   cup     3.5\n",
      " |      3  Indomie  pack    15.0\n",
      " |      4  Indomie  pack     5.0\n",
      " |\n",
      " |      By default, it removes duplicate rows based on all columns.\n",
      " |\n",
      " |      >>> df.drop_duplicates()\n",
      " |          brand style  rating\n",
      " |      0  Yum Yum   cup     4.0\n",
      " |      2  Indomie   cup     3.5\n",
      " |      3  Indomie  pack    15.0\n",
      " |      4  Indomie  pack     5.0\n",
      " |\n",
      " |      To remove duplicates on specific column(s), use ``subset``.\n",
      " |\n",
      " |      >>> df.drop_duplicates(subset=['brand'])\n",
      " |          brand style  rating\n",
      " |      0  Yum Yum   cup     4.0\n",
      " |      2  Indomie   cup     3.5\n",
      " |\n",
      " |      To remove duplicates and keep last occurrences, use ``keep``.\n",
      " |\n",
      " |      >>> df.drop_duplicates(subset=['brand', 'style'], keep='last')\n",
      " |          brand style  rating\n",
      " |      1  Yum Yum   cup     4.0\n",
      " |      2  Indomie   cup     3.5\n",
      " |      4  Indomie  pack     5.0\n",
      " |\n",
      " |  dropna(self, *, axis: 'Axis' = 0, how: 'AnyAll | lib.NoDefault' = <no_default>, thresh: 'int | lib.NoDefault' = <no_default>, subset: 'IndexLabel | None' = None, inplace: 'bool' = False, ignore_index: 'bool' = False) -> 'DataFrame | None'\n",
      " |      Remove missing values.\n",
      " |\n",
      " |      See the :ref:`User Guide <missing_data>` for more on which values are\n",
      " |      considered missing, and how to work with missing data.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          Determine if rows or columns which contain missing values are\n",
      " |          removed.\n",
      " |\n",
      " |          * 0, or 'index' : Drop rows which contain missing values.\n",
      " |          * 1, or 'columns' : Drop columns which contain missing value.\n",
      " |\n",
      " |          Only a single axis is allowed.\n",
      " |\n",
      " |      how : {'any', 'all'}, default 'any'\n",
      " |          Determine if row or column is removed from DataFrame, when we have\n",
      " |          at least one NA or all NA.\n",
      " |\n",
      " |          * 'any' : If any NA values are present, drop that row or column.\n",
      " |          * 'all' : If all values are NA, drop that row or column.\n",
      " |\n",
      " |      thresh : int, optional\n",
      " |          Require that many non-NA values. Cannot be combined with how.\n",
      " |      subset : column label or sequence of labels, optional\n",
      " |          Labels along other axis to consider, e.g. if you are dropping rows\n",
      " |          these would be a list of columns to include.\n",
      " |      inplace : bool, default False\n",
      " |          Whether to modify the DataFrame rather than creating a new one.\n",
      " |      ignore_index : bool, default ``False``\n",
      " |          If ``True``, the resulting axis will be labeled 0, 1, …, n - 1.\n",
      " |\n",
      " |          .. versionadded:: 2.0.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame or None\n",
      " |          DataFrame with NA entries dropped from it or None if ``inplace=True``.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.isna: Indicate missing values.\n",
      " |      DataFrame.notna : Indicate existing (non-missing) values.\n",
      " |      DataFrame.fillna : Replace missing values.\n",
      " |      Series.dropna : Drop missing values.\n",
      " |      Index.dropna : Drop missing indices.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({\"name\": ['Alfred', 'Batman', 'Catwoman'],\n",
      " |      ...                    \"toy\": [np.nan, 'Batmobile', 'Bullwhip'],\n",
      " |      ...                    \"born\": [pd.NaT, pd.Timestamp(\"1940-04-25\"),\n",
      " |      ...                             pd.NaT]})\n",
      " |      >>> df\n",
      " |             name        toy       born\n",
      " |      0    Alfred        NaN        NaT\n",
      " |      1    Batman  Batmobile 1940-04-25\n",
      " |      2  Catwoman   Bullwhip        NaT\n",
      " |\n",
      " |      Drop the rows where at least one element is missing.\n",
      " |\n",
      " |      >>> df.dropna()\n",
      " |           name        toy       born\n",
      " |      1  Batman  Batmobile 1940-04-25\n",
      " |\n",
      " |      Drop the columns where at least one element is missing.\n",
      " |\n",
      " |      >>> df.dropna(axis='columns')\n",
      " |             name\n",
      " |      0    Alfred\n",
      " |      1    Batman\n",
      " |      2  Catwoman\n",
      " |\n",
      " |      Drop the rows where all elements are missing.\n",
      " |\n",
      " |      >>> df.dropna(how='all')\n",
      " |             name        toy       born\n",
      " |      0    Alfred        NaN        NaT\n",
      " |      1    Batman  Batmobile 1940-04-25\n",
      " |      2  Catwoman   Bullwhip        NaT\n",
      " |\n",
      " |      Keep only the rows with at least 2 non-NA values.\n",
      " |\n",
      " |      >>> df.dropna(thresh=2)\n",
      " |             name        toy       born\n",
      " |      1    Batman  Batmobile 1940-04-25\n",
      " |      2  Catwoman   Bullwhip        NaT\n",
      " |\n",
      " |      Define in which columns to look for missing values.\n",
      " |\n",
      " |      >>> df.dropna(subset=['name', 'toy'])\n",
      " |             name        toy       born\n",
      " |      1    Batman  Batmobile 1940-04-25\n",
      " |      2  Catwoman   Bullwhip        NaT\n",
      " |\n",
      " |  duplicated(self, subset: 'Hashable | Sequence[Hashable] | None' = None, keep: 'DropKeep' = 'first') -> 'Series'\n",
      " |      Return boolean Series denoting duplicate rows.\n",
      " |\n",
      " |      Considering certain columns is optional.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      subset : column label or sequence of labels, optional\n",
      " |          Only consider certain columns for identifying duplicates, by\n",
      " |          default use all of the columns.\n",
      " |      keep : {'first', 'last', False}, default 'first'\n",
      " |          Determines which duplicates (if any) to mark.\n",
      " |\n",
      " |          - ``first`` : Mark duplicates as ``True`` except for the first occurrence.\n",
      " |          - ``last`` : Mark duplicates as ``True`` except for the last occurrence.\n",
      " |          - False : Mark all duplicates as ``True``.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series\n",
      " |          Boolean series for each duplicated rows.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Index.duplicated : Equivalent method on index.\n",
      " |      Series.duplicated : Equivalent method on Series.\n",
      " |      Series.drop_duplicates : Remove duplicate values from Series.\n",
      " |      DataFrame.drop_duplicates : Remove duplicate values from DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Consider dataset containing ramen rating.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({\n",
      " |      ...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n",
      " |      ...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n",
      " |      ...     'rating': [4, 4, 3.5, 15, 5]\n",
      " |      ... })\n",
      " |      >>> df\n",
      " |          brand style  rating\n",
      " |      0  Yum Yum   cup     4.0\n",
      " |      1  Yum Yum   cup     4.0\n",
      " |      2  Indomie   cup     3.5\n",
      " |      3  Indomie  pack    15.0\n",
      " |      4  Indomie  pack     5.0\n",
      " |\n",
      " |      By default, for each set of duplicated values, the first occurrence\n",
      " |      is set on False and all others on True.\n",
      " |\n",
      " |      >>> df.duplicated()\n",
      " |      0    False\n",
      " |      1     True\n",
      " |      2    False\n",
      " |      3    False\n",
      " |      4    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      By using 'last', the last occurrence of each set of duplicated values\n",
      " |      is set on False and all others on True.\n",
      " |\n",
      " |      >>> df.duplicated(keep='last')\n",
      " |      0     True\n",
      " |      1    False\n",
      " |      2    False\n",
      " |      3    False\n",
      " |      4    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      By setting ``keep`` on False, all duplicates are True.\n",
      " |\n",
      " |      >>> df.duplicated(keep=False)\n",
      " |      0     True\n",
      " |      1     True\n",
      " |      2    False\n",
      " |      3    False\n",
      " |      4    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |      To find duplicates on specific column(s), use ``subset``.\n",
      " |\n",
      " |      >>> df.duplicated(subset=['brand'])\n",
      " |      0    False\n",
      " |      1     True\n",
      " |      2    False\n",
      " |      3     True\n",
      " |      4     True\n",
      " |      dtype: bool\n",
      " |\n",
      " |  eq(self, other, axis: 'Axis' = 'columns', level=None) -> 'DataFrame'\n",
      " |      Get Equal to of dataframe and other, element-wise (binary operator `eq`).\n",
      " |\n",
      " |      Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n",
      " |      operators.\n",
      " |\n",
      " |      Equivalent to `==`, `!=`, `<=`, `<`, `>=`, `>` with support to choose axis\n",
      " |      (rows or columns) and level for comparison.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns').\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the passed\n",
      " |          MultiIndex level.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame of bool\n",
      " |          Result of the comparison.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.eq : Compare DataFrames for equality elementwise.\n",
      " |      DataFrame.ne : Compare DataFrames for inequality elementwise.\n",
      " |      DataFrame.le : Compare DataFrames for less than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.lt : Compare DataFrames for strictly less than\n",
      " |          inequality elementwise.\n",
      " |      DataFrame.ge : Compare DataFrames for greater than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.gt : Compare DataFrames for strictly greater than\n",
      " |          inequality elementwise.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      `NaN` values are considered different (i.e. `NaN` != `NaN`).\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'cost': [250, 150, 100],\n",
      " |      ...                    'revenue': [100, 250, 300]},\n",
      " |      ...                   index=['A', 'B', 'C'])\n",
      " |      >>> df\n",
      " |         cost  revenue\n",
      " |      A   250      100\n",
      " |      B   150      250\n",
      " |      C   100      300\n",
      " |\n",
      " |      Comparison with a scalar, using either the operator or method:\n",
      " |\n",
      " |      >>> df == 100\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |\n",
      " |      >>> df.eq(100)\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |\n",
      " |      When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n",
      " |      with the index of `other` and broadcast:\n",
      " |\n",
      " |      >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B   True    False\n",
      " |      C  False     True\n",
      " |\n",
      " |      Use the method to control the broadcast axis:\n",
      " |\n",
      " |      >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')\n",
      " |         cost  revenue\n",
      " |      A  True    False\n",
      " |      B  True     True\n",
      " |      C  True     True\n",
      " |      D  True     True\n",
      " |\n",
      " |      When comparing to an arbitrary sequence, the number of columns must\n",
      " |      match the number elements in `other`:\n",
      " |\n",
      " |      >>> df == [250, 100]\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B  False    False\n",
      " |      C  False    False\n",
      " |\n",
      " |      Use the method to control the axis:\n",
      " |\n",
      " |      >>> df.eq([250, 250, 100], axis='index')\n",
      " |          cost  revenue\n",
      " |      A   True    False\n",
      " |      B  False     True\n",
      " |      C   True    False\n",
      " |\n",
      " |      Compare to a DataFrame of different shape.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n",
      " |      ...                      index=['A', 'B', 'C', 'D'])\n",
      " |      >>> other\n",
      " |         revenue\n",
      " |      A      300\n",
      " |      B      250\n",
      " |      C      100\n",
      " |      D      150\n",
      " |\n",
      " |      >>> df.gt(other)\n",
      " |          cost  revenue\n",
      " |      A  False    False\n",
      " |      B  False    False\n",
      " |      C  False     True\n",
      " |      D  False    False\n",
      " |\n",
      " |      Compare to a MultiIndex by level.\n",
      " |\n",
      " |      >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n",
      " |      ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n",
      " |      ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n",
      " |      ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n",
      " |      >>> df_multindex\n",
      " |            cost  revenue\n",
      " |      Q1 A   250      100\n",
      " |         B   150      250\n",
      " |         C   100      300\n",
      " |      Q2 A   150      200\n",
      " |         B   300      175\n",
      " |         C   220      225\n",
      " |\n",
      " |      >>> df.le(df_multindex, level=1)\n",
      " |             cost  revenue\n",
      " |      Q1 A   True     True\n",
      " |         B   True     True\n",
      " |         C   True     True\n",
      " |      Q2 A  False     True\n",
      " |         B   True    False\n",
      " |         C   True    False\n",
      " |\n",
      " |  eval(self, expr: 'str', *, inplace: 'bool' = False, **kwargs) -> 'Any | None'\n",
      " |      Evaluate a string describing operations on DataFrame columns.\n",
      " |\n",
      " |      Operates on columns only, not specific rows or elements.  This allows\n",
      " |      `eval` to run arbitrary code, which can make you vulnerable to code\n",
      " |      injection if you pass user input to this function.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      expr : str\n",
      " |          The expression string to evaluate.\n",
      " |      inplace : bool, default False\n",
      " |          If the expression contains an assignment, whether to perform the\n",
      " |          operation inplace and mutate the existing DataFrame. Otherwise,\n",
      " |          a new DataFrame is returned.\n",
      " |      **kwargs\n",
      " |          See the documentation for :func:`eval` for complete details\n",
      " |          on the keyword arguments accepted by\n",
      " |          :meth:`~pandas.DataFrame.query`.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      ndarray, scalar, pandas object, or None\n",
      " |          The result of the evaluation or None if ``inplace=True``.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.query : Evaluates a boolean expression to query the columns\n",
      " |          of a frame.\n",
      " |      DataFrame.assign : Can evaluate an expression or function to create new\n",
      " |          values for a column.\n",
      " |      eval : Evaluate a Python expression as a string using various\n",
      " |          backends.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      For more details see the API documentation for :func:`~eval`.\n",
      " |      For detailed examples see :ref:`enhancing performance with eval\n",
      " |      <enhancingperf.eval>`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'A': range(1, 6), 'B': range(10, 0, -2)})\n",
      " |      >>> df\n",
      " |         A   B\n",
      " |      0  1  10\n",
      " |      1  2   8\n",
      " |      2  3   6\n",
      " |      3  4   4\n",
      " |      4  5   2\n",
      " |      >>> df.eval('A + B')\n",
      " |      0    11\n",
      " |      1    10\n",
      " |      2     9\n",
      " |      3     8\n",
      " |      4     7\n",
      " |      dtype: int64\n",
      " |\n",
      " |      Assignment is allowed though by default the original DataFrame is not\n",
      " |      modified.\n",
      " |\n",
      " |      >>> df.eval('C = A + B')\n",
      " |         A   B   C\n",
      " |      0  1  10  11\n",
      " |      1  2   8  10\n",
      " |      2  3   6   9\n",
      " |      3  4   4   8\n",
      " |      4  5   2   7\n",
      " |      >>> df\n",
      " |         A   B\n",
      " |      0  1  10\n",
      " |      1  2   8\n",
      " |      2  3   6\n",
      " |      3  4   4\n",
      " |      4  5   2\n",
      " |\n",
      " |      Multiple columns can be assigned to using multi-line expressions:\n",
      " |\n",
      " |      >>> df.eval(\n",
      " |      ...     '''\n",
      " |      ... C = A + B\n",
      " |      ... D = A - B\n",
      " |      ... '''\n",
      " |      ... )\n",
      " |         A   B   C  D\n",
      " |      0  1  10  11 -9\n",
      " |      1  2   8  10 -6\n",
      " |      2  3   6   9 -3\n",
      " |      3  4   4   8  0\n",
      " |      4  5   2   7  3\n",
      " |\n",
      " |  floordiv(self, other, axis: 'Axis' = 'columns', level=None, fill_value=None) -> 'DataFrame'\n",
      " |      Get Integer division of dataframe and other, element-wise (binary operator `floordiv`).\n",
      " |\n",
      " |      Equivalent to ``dataframe // other``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `rfloordiv`.\n",
      " |\n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `floordiv`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, dict or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns.\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |\n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |\n",
      " |      >>> df + 1\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      >>> df.add(1)\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      Divide by constant with reverse version.\n",
      " |\n",
      " |      >>> df.div(10)\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |\n",
      " |      >>> df.rdiv(10)\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |\n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |\n",
      " |      >>> df - [1, 2]\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub([1, 2], axis='columns')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |\n",
      " |      Multiply a dictionary by axis.\n",
      " |\n",
      " |      >>> df.mul({'angles': 0, 'degrees': 2})\n",
      " |                  angles  degrees\n",
      " |      circle           0      720\n",
      " |      triangle         0      360\n",
      " |      rectangle        0      720\n",
      " |\n",
      " |      >>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n",
      " |                  angles  degrees\n",
      " |      circle           0        0\n",
      " |      triangle         6      360\n",
      " |      rectangle       12     1080\n",
      " |\n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |\n",
      " |      >>> df * other\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |\n",
      " |      >>> df.mul(other, fill_value=0)\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |\n",
      " |      Divide by a MultiIndex by level.\n",
      " |\n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |\n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |\n",
      " |  ge(self, other, axis: 'Axis' = 'columns', level=None) -> 'DataFrame'\n",
      " |      Get Greater than or equal to of dataframe and other, element-wise (binary operator `ge`).\n",
      " |\n",
      " |      Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n",
      " |      operators.\n",
      " |\n",
      " |      Equivalent to `==`, `!=`, `<=`, `<`, `>=`, `>` with support to choose axis\n",
      " |      (rows or columns) and level for comparison.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns').\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the passed\n",
      " |          MultiIndex level.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame of bool\n",
      " |          Result of the comparison.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.eq : Compare DataFrames for equality elementwise.\n",
      " |      DataFrame.ne : Compare DataFrames for inequality elementwise.\n",
      " |      DataFrame.le : Compare DataFrames for less than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.lt : Compare DataFrames for strictly less than\n",
      " |          inequality elementwise.\n",
      " |      DataFrame.ge : Compare DataFrames for greater than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.gt : Compare DataFrames for strictly greater than\n",
      " |          inequality elementwise.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      `NaN` values are considered different (i.e. `NaN` != `NaN`).\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'cost': [250, 150, 100],\n",
      " |      ...                    'revenue': [100, 250, 300]},\n",
      " |      ...                   index=['A', 'B', 'C'])\n",
      " |      >>> df\n",
      " |         cost  revenue\n",
      " |      A   250      100\n",
      " |      B   150      250\n",
      " |      C   100      300\n",
      " |\n",
      " |      Comparison with a scalar, using either the operator or method:\n",
      " |\n",
      " |      >>> df == 100\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |\n",
      " |      >>> df.eq(100)\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |\n",
      " |      When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n",
      " |      with the index of `other` and broadcast:\n",
      " |\n",
      " |      >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B   True    False\n",
      " |      C  False     True\n",
      " |\n",
      " |      Use the method to control the broadcast axis:\n",
      " |\n",
      " |      >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')\n",
      " |         cost  revenue\n",
      " |      A  True    False\n",
      " |      B  True     True\n",
      " |      C  True     True\n",
      " |      D  True     True\n",
      " |\n",
      " |      When comparing to an arbitrary sequence, the number of columns must\n",
      " |      match the number elements in `other`:\n",
      " |\n",
      " |      >>> df == [250, 100]\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B  False    False\n",
      " |      C  False    False\n",
      " |\n",
      " |      Use the method to control the axis:\n",
      " |\n",
      " |      >>> df.eq([250, 250, 100], axis='index')\n",
      " |          cost  revenue\n",
      " |      A   True    False\n",
      " |      B  False     True\n",
      " |      C   True    False\n",
      " |\n",
      " |      Compare to a DataFrame of different shape.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n",
      " |      ...                      index=['A', 'B', 'C', 'D'])\n",
      " |      >>> other\n",
      " |         revenue\n",
      " |      A      300\n",
      " |      B      250\n",
      " |      C      100\n",
      " |      D      150\n",
      " |\n",
      " |      >>> df.gt(other)\n",
      " |          cost  revenue\n",
      " |      A  False    False\n",
      " |      B  False    False\n",
      " |      C  False     True\n",
      " |      D  False    False\n",
      " |\n",
      " |      Compare to a MultiIndex by level.\n",
      " |\n",
      " |      >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n",
      " |      ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n",
      " |      ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n",
      " |      ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n",
      " |      >>> df_multindex\n",
      " |            cost  revenue\n",
      " |      Q1 A   250      100\n",
      " |         B   150      250\n",
      " |         C   100      300\n",
      " |      Q2 A   150      200\n",
      " |         B   300      175\n",
      " |         C   220      225\n",
      " |\n",
      " |      >>> df.le(df_multindex, level=1)\n",
      " |             cost  revenue\n",
      " |      Q1 A   True     True\n",
      " |         B   True     True\n",
      " |         C   True     True\n",
      " |      Q2 A  False     True\n",
      " |         B   True    False\n",
      " |         C   True    False\n",
      " |\n",
      " |  groupby(self, by=None, axis: 'Axis | lib.NoDefault' = <no_default>, level: 'IndexLabel | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, observed: 'bool | lib.NoDefault' = <no_default>, dropna: 'bool' = True) -> 'DataFrameGroupBy'\n",
      " |      Group DataFrame using a mapper or by a Series of columns.\n",
      " |\n",
      " |      A groupby operation involves some combination of splitting the\n",
      " |      object, applying a function, and combining the results. This can be\n",
      " |      used to group large amounts of data and compute operations on these\n",
      " |      groups.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      by : mapping, function, label, pd.Grouper or list of such\n",
      " |          Used to determine the groups for the groupby.\n",
      " |          If ``by`` is a function, it's called on each value of the object's\n",
      " |          index. If a dict or Series is passed, the Series or dict VALUES\n",
      " |          will be used to determine the groups (the Series' values are first\n",
      " |          aligned; see ``.align()`` method). If a list or ndarray of length\n",
      " |          equal to the selected axis is passed (see the `groupby user guide\n",
      " |          <https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups>`_),\n",
      " |          the values are used as-is to determine the groups. A label or list\n",
      " |          of labels may be passed to group by the columns in ``self``.\n",
      " |          Notice that a tuple is interpreted as a (single) key.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          Split along rows (0) or columns (1). For `Series` this parameter\n",
      " |          is unused and defaults to 0.\n",
      " |\n",
      " |          .. deprecated:: 2.1.0\n",
      " |\n",
      " |              Will be removed and behave like axis=0 in a future version.\n",
      " |              For ``axis=1``, do ``frame.T.groupby(...)`` instead.\n",
      " |\n",
      " |      level : int, level name, or sequence of such, default None\n",
      " |          If the axis is a MultiIndex (hierarchical), group by a particular\n",
      " |          level or levels. Do not specify both ``by`` and ``level``.\n",
      " |      as_index : bool, default True\n",
      " |          Return object with group labels as the\n",
      " |          index. Only relevant for DataFrame input. as_index=False is\n",
      " |          effectively \"SQL-style\" grouped output. This argument has no effect\n",
      " |          on filtrations (see the `filtrations in the user guide\n",
      " |          <https://pandas.pydata.org/docs/dev/user_guide/groupby.html#filtration>`_),\n",
      " |          such as ``head()``, ``tail()``, ``nth()`` and in transformations\n",
      " |          (see the `transformations in the user guide\n",
      " |          <https://pandas.pydata.org/docs/dev/user_guide/groupby.html#transformation>`_).\n",
      " |      sort : bool, default True\n",
      " |          Sort group keys. Get better performance by turning this off.\n",
      " |          Note this does not influence the order of observations within each\n",
      " |          group. Groupby preserves the order of rows within each group. If False,\n",
      " |          the groups will appear in the same order as they did in the original DataFrame.\n",
      " |          This argument has no effect on filtrations (see the `filtrations in the user guide\n",
      " |          <https://pandas.pydata.org/docs/dev/user_guide/groupby.html#filtration>`_),\n",
      " |          such as ``head()``, ``tail()``, ``nth()`` and in transformations\n",
      " |          (see the `transformations in the user guide\n",
      " |          <https://pandas.pydata.org/docs/dev/user_guide/groupby.html#transformation>`_).\n",
      " |\n",
      " |          .. versionchanged:: 2.0.0\n",
      " |\n",
      " |              Specifying ``sort=False`` with an ordered categorical grouper will no\n",
      " |              longer sort the values.\n",
      " |\n",
      " |      group_keys : bool, default True\n",
      " |          When calling apply and the ``by`` argument produces a like-indexed\n",
      " |          (i.e. :ref:`a transform <groupby.transform>`) result, add group keys to\n",
      " |          index to identify pieces. By default group keys are not included\n",
      " |          when the result's index (and column) labels match the inputs, and\n",
      " |          are included otherwise.\n",
      " |\n",
      " |          .. versionchanged:: 1.5.0\n",
      " |\n",
      " |             Warns that ``group_keys`` will no longer be ignored when the\n",
      " |             result from ``apply`` is a like-indexed Series or DataFrame.\n",
      " |             Specify ``group_keys`` explicitly to include the group keys or\n",
      " |             not.\n",
      " |\n",
      " |          .. versionchanged:: 2.0.0\n",
      " |\n",
      " |             ``group_keys`` now defaults to ``True``.\n",
      " |\n",
      " |      observed : bool, default False\n",
      " |          This only applies if any of the groupers are Categoricals.\n",
      " |          If True: only show observed values for categorical groupers.\n",
      " |          If False: show all values for categorical groupers.\n",
      " |\n",
      " |          .. deprecated:: 2.1.0\n",
      " |\n",
      " |              The default value will change to True in a future version of pandas.\n",
      " |\n",
      " |      dropna : bool, default True\n",
      " |          If True, and if group keys contain NA values, NA values together\n",
      " |          with row/column will be dropped.\n",
      " |          If False, NA values will also be treated as the key in groups.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      pandas.api.typing.DataFrameGroupBy\n",
      " |          Returns a groupby object that contains information about the groups.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      resample : Convenience method for frequency conversion and resampling\n",
      " |          of time series.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      See the `user guide\n",
      " |      <https://pandas.pydata.org/pandas-docs/stable/groupby.html>`__ for more\n",
      " |      detailed usage and examples, including splitting an object into groups,\n",
      " |      iterating through groups, selecting a group, aggregation, and more.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n",
      " |      ...                               'Parrot', 'Parrot'],\n",
      " |      ...                    'Max Speed': [380., 370., 24., 26.]})\n",
      " |      >>> df\n",
      " |         Animal  Max Speed\n",
      " |      0  Falcon      380.0\n",
      " |      1  Falcon      370.0\n",
      " |      2  Parrot       24.0\n",
      " |      3  Parrot       26.0\n",
      " |      >>> df.groupby(['Animal']).mean()\n",
      " |              Max Speed\n",
      " |      Animal\n",
      " |      Falcon      375.0\n",
      " |      Parrot       25.0\n",
      " |\n",
      " |      **Hierarchical Indexes**\n",
      " |\n",
      " |      We can groupby different levels of a hierarchical index\n",
      " |      using the `level` parameter:\n",
      " |\n",
      " |      >>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n",
      " |      ...           ['Captive', 'Wild', 'Captive', 'Wild']]\n",
      " |      >>> index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n",
      " |      >>> df = pd.DataFrame({'Max Speed': [390., 350., 30., 20.]},\n",
      " |      ...                   index=index)\n",
      " |      >>> df\n",
      " |                      Max Speed\n",
      " |      Animal Type\n",
      " |      Falcon Captive      390.0\n",
      " |             Wild         350.0\n",
      " |      Parrot Captive       30.0\n",
      " |             Wild          20.0\n",
      " |      >>> df.groupby(level=0).mean()\n",
      " |              Max Speed\n",
      " |      Animal\n",
      " |      Falcon      370.0\n",
      " |      Parrot       25.0\n",
      " |      >>> df.groupby(level=\"Type\").mean()\n",
      " |               Max Speed\n",
      " |      Type\n",
      " |      Captive      210.0\n",
      " |      Wild         185.0\n",
      " |\n",
      " |      We can also choose to include NA in group keys or not by setting\n",
      " |      `dropna` parameter, the default setting is `True`.\n",
      " |\n",
      " |      >>> l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]\n",
      " |      >>> df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"])\n",
      " |\n",
      " |      >>> df.groupby(by=[\"b\"]).sum()\n",
      " |          a   c\n",
      " |      b\n",
      " |      1.0 2   3\n",
      " |      2.0 2   5\n",
      " |\n",
      " |      >>> df.groupby(by=[\"b\"], dropna=False).sum()\n",
      " |          a   c\n",
      " |      b\n",
      " |      1.0 2   3\n",
      " |      2.0 2   5\n",
      " |      NaN 1   4\n",
      " |\n",
      " |      >>> l = [[\"a\", 12, 12], [None, 12.3, 33.], [\"b\", 12.3, 123], [\"a\", 1, 1]]\n",
      " |      >>> df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"])\n",
      " |\n",
      " |      >>> df.groupby(by=\"a\").sum()\n",
      " |          b     c\n",
      " |      a\n",
      " |      a   13.0   13.0\n",
      " |      b   12.3  123.0\n",
      " |\n",
      " |      >>> df.groupby(by=\"a\", dropna=False).sum()\n",
      " |          b     c\n",
      " |      a\n",
      " |      a   13.0   13.0\n",
      " |      b   12.3  123.0\n",
      " |      NaN 12.3   33.0\n",
      " |\n",
      " |      When using ``.apply()``, use ``group_keys`` to include or exclude the\n",
      " |      group keys. The ``group_keys`` argument defaults to ``True`` (include).\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n",
      " |      ...                               'Parrot', 'Parrot'],\n",
      " |      ...                    'Max Speed': [380., 370., 24., 26.]})\n",
      " |      >>> df.groupby(\"Animal\", group_keys=True)[['Max Speed']].apply(lambda x: x)\n",
      " |                Max Speed\n",
      " |      Animal\n",
      " |      Falcon 0      380.0\n",
      " |             1      370.0\n",
      " |      Parrot 2       24.0\n",
      " |             3       26.0\n",
      " |\n",
      " |      >>> df.groupby(\"Animal\", group_keys=False)[['Max Speed']].apply(lambda x: x)\n",
      " |         Max Speed\n",
      " |      0      380.0\n",
      " |      1      370.0\n",
      " |      2       24.0\n",
      " |      3       26.0\n",
      " |\n",
      " |  gt(self, other, axis: 'Axis' = 'columns', level=None) -> 'DataFrame'\n",
      " |      Get Greater than of dataframe and other, element-wise (binary operator `gt`).\n",
      " |\n",
      " |      Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n",
      " |      operators.\n",
      " |\n",
      " |      Equivalent to `==`, `!=`, `<=`, `<`, `>=`, `>` with support to choose axis\n",
      " |      (rows or columns) and level for comparison.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns').\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the passed\n",
      " |          MultiIndex level.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame of bool\n",
      " |          Result of the comparison.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.eq : Compare DataFrames for equality elementwise.\n",
      " |      DataFrame.ne : Compare DataFrames for inequality elementwise.\n",
      " |      DataFrame.le : Compare DataFrames for less than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.lt : Compare DataFrames for strictly less than\n",
      " |          inequality elementwise.\n",
      " |      DataFrame.ge : Compare DataFrames for greater than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.gt : Compare DataFrames for strictly greater than\n",
      " |          inequality elementwise.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      `NaN` values are considered different (i.e. `NaN` != `NaN`).\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'cost': [250, 150, 100],\n",
      " |      ...                    'revenue': [100, 250, 300]},\n",
      " |      ...                   index=['A', 'B', 'C'])\n",
      " |      >>> df\n",
      " |         cost  revenue\n",
      " |      A   250      100\n",
      " |      B   150      250\n",
      " |      C   100      300\n",
      " |\n",
      " |      Comparison with a scalar, using either the operator or method:\n",
      " |\n",
      " |      >>> df == 100\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |\n",
      " |      >>> df.eq(100)\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |\n",
      " |      When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n",
      " |      with the index of `other` and broadcast:\n",
      " |\n",
      " |      >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B   True    False\n",
      " |      C  False     True\n",
      " |\n",
      " |      Use the method to control the broadcast axis:\n",
      " |\n",
      " |      >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')\n",
      " |         cost  revenue\n",
      " |      A  True    False\n",
      " |      B  True     True\n",
      " |      C  True     True\n",
      " |      D  True     True\n",
      " |\n",
      " |      When comparing to an arbitrary sequence, the number of columns must\n",
      " |      match the number elements in `other`:\n",
      " |\n",
      " |      >>> df == [250, 100]\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B  False    False\n",
      " |      C  False    False\n",
      " |\n",
      " |      Use the method to control the axis:\n",
      " |\n",
      " |      >>> df.eq([250, 250, 100], axis='index')\n",
      " |          cost  revenue\n",
      " |      A   True    False\n",
      " |      B  False     True\n",
      " |      C   True    False\n",
      " |\n",
      " |      Compare to a DataFrame of different shape.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n",
      " |      ...                      index=['A', 'B', 'C', 'D'])\n",
      " |      >>> other\n",
      " |         revenue\n",
      " |      A      300\n",
      " |      B      250\n",
      " |      C      100\n",
      " |      D      150\n",
      " |\n",
      " |      >>> df.gt(other)\n",
      " |          cost  revenue\n",
      " |      A  False    False\n",
      " |      B  False    False\n",
      " |      C  False     True\n",
      " |      D  False    False\n",
      " |\n",
      " |      Compare to a MultiIndex by level.\n",
      " |\n",
      " |      >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n",
      " |      ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n",
      " |      ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n",
      " |      ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n",
      " |      >>> df_multindex\n",
      " |            cost  revenue\n",
      " |      Q1 A   250      100\n",
      " |         B   150      250\n",
      " |         C   100      300\n",
      " |      Q2 A   150      200\n",
      " |         B   300      175\n",
      " |         C   220      225\n",
      " |\n",
      " |      >>> df.le(df_multindex, level=1)\n",
      " |             cost  revenue\n",
      " |      Q1 A   True     True\n",
      " |         B   True     True\n",
      " |         C   True     True\n",
      " |      Q2 A  False     True\n",
      " |         B   True    False\n",
      " |         C   True    False\n",
      " |\n",
      " |  hist = hist_frame(data: 'DataFrame', column: 'IndexLabel | None' = None, by=None, grid: 'bool' = True, xlabelsize: 'int | None' = None, xrot: 'float | None' = None, ylabelsize: 'int | None' = None, yrot: 'float | None' = None, ax=None, sharex: 'bool' = False, sharey: 'bool' = False, figsize: 'tuple[int, int] | None' = None, layout: 'tuple[int, int] | None' = None, bins: 'int | Sequence[int]' = 10, backend: 'str | None' = None, legend: 'bool' = False, **kwargs) from pandas.plotting._core\n",
      " |      Make a histogram of the DataFrame's columns.\n",
      " |\n",
      " |      A `histogram`_ is a representation of the distribution of data.\n",
      " |      This function calls :meth:`matplotlib.pyplot.hist`, on each series in\n",
      " |      the DataFrame, resulting in one histogram per column.\n",
      " |\n",
      " |      .. _histogram: https://en.wikipedia.org/wiki/Histogram\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : DataFrame\n",
      " |          The pandas object holding the data.\n",
      " |      column : str or sequence, optional\n",
      " |          If passed, will be used to limit data to a subset of columns.\n",
      " |      by : object, optional\n",
      " |          If passed, then used to form histograms for separate groups.\n",
      " |      grid : bool, default True\n",
      " |          Whether to show axis grid lines.\n",
      " |      xlabelsize : int, default None\n",
      " |          If specified changes the x-axis label size.\n",
      " |      xrot : float, default None\n",
      " |          Rotation of x axis labels. For example, a value of 90 displays the\n",
      " |          x labels rotated 90 degrees clockwise.\n",
      " |      ylabelsize : int, default None\n",
      " |          If specified changes the y-axis label size.\n",
      " |      yrot : float, default None\n",
      " |          Rotation of y axis labels. For example, a value of 90 displays the\n",
      " |          y labels rotated 90 degrees clockwise.\n",
      " |      ax : Matplotlib axes object, default None\n",
      " |          The axes to plot the histogram on.\n",
      " |      sharex : bool, default True if ax is None else False\n",
      " |          In case subplots=True, share x axis and set some x axis labels to\n",
      " |          invisible; defaults to True if ax is None otherwise False if an ax\n",
      " |          is passed in.\n",
      " |          Note that passing in both an ax and sharex=True will alter all x axis\n",
      " |          labels for all subplots in a figure.\n",
      " |      sharey : bool, default False\n",
      " |          In case subplots=True, share y axis and set some y axis labels to\n",
      " |          invisible.\n",
      " |      figsize : tuple, optional\n",
      " |          The size in inches of the figure to create. Uses the value in\n",
      " |          `matplotlib.rcParams` by default.\n",
      " |      layout : tuple, optional\n",
      " |          Tuple of (rows, columns) for the layout of the histograms.\n",
      " |      bins : int or sequence, default 10\n",
      " |          Number of histogram bins to be used. If an integer is given, bins + 1\n",
      " |          bin edges are calculated and returned. If bins is a sequence, gives\n",
      " |          bin edges, including left edge of first bin and right edge of last\n",
      " |          bin. In this case, bins is returned unmodified.\n",
      " |\n",
      " |      backend : str, default None\n",
      " |          Backend to use instead of the backend specified in the option\n",
      " |          ``plotting.backend``. For instance, 'matplotlib'. Alternatively, to\n",
      " |          specify the ``plotting.backend`` for the whole session, set\n",
      " |          ``pd.options.plotting.backend``.\n",
      " |\n",
      " |      legend : bool, default False\n",
      " |          Whether to show the legend.\n",
      " |\n",
      " |      **kwargs\n",
      " |          All other plotting keyword arguments to be passed to\n",
      " |          :meth:`matplotlib.pyplot.hist`.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      matplotlib.AxesSubplot or numpy.ndarray of them\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      matplotlib.pyplot.hist : Plot a histogram using matplotlib.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      This example draws a histogram based on the length and width of\n",
      " |      some animals, displayed in three bins\n",
      " |\n",
      " |      .. plot::\n",
      " |          :context: close-figs\n",
      " |\n",
      " |          >>> data = {'length': [1.5, 0.5, 1.2, 0.9, 3],\n",
      " |          ...         'width': [0.7, 0.2, 0.15, 0.2, 1.1]}\n",
      " |          >>> index = ['pig', 'rabbit', 'duck', 'chicken', 'horse']\n",
      " |          >>> df = pd.DataFrame(data, index=index)\n",
      " |          >>> hist = df.hist(bins=3)\n",
      " |\n",
      " |  idxmax(self, axis: 'Axis' = 0, skipna: 'bool' = True, numeric_only: 'bool' = False) -> 'Series'\n",
      " |      Return index of first occurrence of maximum over requested axis.\n",
      " |\n",
      " |      NA/null values are excluded.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If an entire row/column is NA, the result\n",
      " |          will be NA.\n",
      " |      numeric_only : bool, default False\n",
      " |          Include only `float`, `int` or `boolean` data.\n",
      " |\n",
      " |          .. versionadded:: 1.5.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series\n",
      " |          Indexes of maxima along the specified axis.\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      ValueError\n",
      " |          * If the row/column is empty\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.idxmax : Return index of the maximum element.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method is the DataFrame version of ``ndarray.argmax``.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Consider a dataset containing food consumption in Argentina.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48],\n",
      " |      ...                     'co2_emissions': [37.2, 19.66, 1712]},\n",
      " |      ...                   index=['Pork', 'Wheat Products', 'Beef'])\n",
      " |\n",
      " |      >>> df\n",
      " |                      consumption  co2_emissions\n",
      " |      Pork                  10.51         37.20\n",
      " |      Wheat Products       103.11         19.66\n",
      " |      Beef                  55.48       1712.00\n",
      " |\n",
      " |      By default, it returns the index for the maximum value in each column.\n",
      " |\n",
      " |      >>> df.idxmax()\n",
      " |      consumption     Wheat Products\n",
      " |      co2_emissions             Beef\n",
      " |      dtype: object\n",
      " |\n",
      " |      To return the index for the maximum value in each row, use ``axis=\"columns\"``.\n",
      " |\n",
      " |      >>> df.idxmax(axis=\"columns\")\n",
      " |      Pork              co2_emissions\n",
      " |      Wheat Products     consumption\n",
      " |      Beef              co2_emissions\n",
      " |      dtype: object\n",
      " |\n",
      " |  idxmin(self, axis: 'Axis' = 0, skipna: 'bool' = True, numeric_only: 'bool' = False) -> 'Series'\n",
      " |      Return index of first occurrence of minimum over requested axis.\n",
      " |\n",
      " |      NA/null values are excluded.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If an entire row/column is NA, the result\n",
      " |          will be NA.\n",
      " |      numeric_only : bool, default False\n",
      " |          Include only `float`, `int` or `boolean` data.\n",
      " |\n",
      " |          .. versionadded:: 1.5.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series\n",
      " |          Indexes of minima along the specified axis.\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      ValueError\n",
      " |          * If the row/column is empty\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.idxmin : Return index of the minimum element.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method is the DataFrame version of ``ndarray.argmin``.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Consider a dataset containing food consumption in Argentina.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48],\n",
      " |      ...                     'co2_emissions': [37.2, 19.66, 1712]},\n",
      " |      ...                   index=['Pork', 'Wheat Products', 'Beef'])\n",
      " |\n",
      " |      >>> df\n",
      " |                      consumption  co2_emissions\n",
      " |      Pork                  10.51         37.20\n",
      " |      Wheat Products       103.11         19.66\n",
      " |      Beef                  55.48       1712.00\n",
      " |\n",
      " |      By default, it returns the index for the minimum value in each column.\n",
      " |\n",
      " |      >>> df.idxmin()\n",
      " |      consumption                Pork\n",
      " |      co2_emissions    Wheat Products\n",
      " |      dtype: object\n",
      " |\n",
      " |      To return the index for the minimum value in each row, use ``axis=\"columns\"``.\n",
      " |\n",
      " |      >>> df.idxmin(axis=\"columns\")\n",
      " |      Pork                consumption\n",
      " |      Wheat Products    co2_emissions\n",
      " |      Beef                consumption\n",
      " |      dtype: object\n",
      " |\n",
      " |  info(self, verbose: 'bool | None' = None, buf: 'WriteBuffer[str] | None' = None, max_cols: 'int | None' = None, memory_usage: 'bool | str | None' = None, show_counts: 'bool | None' = None) -> 'None'\n",
      " |      Print a concise summary of a DataFrame.\n",
      " |\n",
      " |      This method prints information about a DataFrame including\n",
      " |      the index dtype and columns, non-null values and memory usage.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      verbose : bool, optional\n",
      " |          Whether to print the full summary. By default, the setting in\n",
      " |          ``pandas.options.display.max_info_columns`` is followed.\n",
      " |      buf : writable buffer, defaults to sys.stdout\n",
      " |          Where to send the output. By default, the output is printed to\n",
      " |          sys.stdout. Pass a writable buffer if you need to further process\n",
      " |          the output.\n",
      " |      max_cols : int, optional\n",
      " |          When to switch from the verbose to the truncated output. If the\n",
      " |          DataFrame has more than `max_cols` columns, the truncated output\n",
      " |          is used. By default, the setting in\n",
      " |          ``pandas.options.display.max_info_columns`` is used.\n",
      " |      memory_usage : bool, str, optional\n",
      " |          Specifies whether total memory usage of the DataFrame\n",
      " |          elements (including the index) should be displayed. By default,\n",
      " |          this follows the ``pandas.options.display.memory_usage`` setting.\n",
      " |\n",
      " |          True always show memory usage. False never shows memory usage.\n",
      " |          A value of 'deep' is equivalent to \"True with deep introspection\".\n",
      " |          Memory usage is shown in human-readable units (base-2\n",
      " |          representation). Without deep introspection a memory estimation is\n",
      " |          made based in column dtype and number of rows assuming values\n",
      " |          consume the same memory amount for corresponding dtypes. With deep\n",
      " |          memory introspection, a real memory usage calculation is performed\n",
      " |          at the cost of computational resources. See the\n",
      " |          :ref:`Frequently Asked Questions <df-memory-usage>` for more\n",
      " |          details.\n",
      " |      show_counts : bool, optional\n",
      " |          Whether to show the non-null counts. By default, this is shown\n",
      " |          only if the DataFrame is smaller than\n",
      " |          ``pandas.options.display.max_info_rows`` and\n",
      " |          ``pandas.options.display.max_info_columns``. A value of True always\n",
      " |          shows the counts, and False never shows the counts.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |          This method prints a summary of a DataFrame and returns None.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.describe: Generate descriptive statistics of DataFrame\n",
      " |          columns.\n",
      " |      DataFrame.memory_usage: Memory usage of DataFrame columns.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> int_values = [1, 2, 3, 4, 5]\n",
      " |      >>> text_values = ['alpha', 'beta', 'gamma', 'delta', 'epsilon']\n",
      " |      >>> float_values = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
      " |      >>> df = pd.DataFrame({\"int_col\": int_values, \"text_col\": text_values,\n",
      " |      ...                   \"float_col\": float_values})\n",
      " |      >>> df\n",
      " |          int_col text_col  float_col\n",
      " |      0        1    alpha       0.00\n",
      " |      1        2     beta       0.25\n",
      " |      2        3    gamma       0.50\n",
      " |      3        4    delta       0.75\n",
      " |      4        5  epsilon       1.00\n",
      " |\n",
      " |      Prints information of all columns:\n",
      " |\n",
      " |      >>> df.info(verbose=True)\n",
      " |      <class 'pandas.core.frame.DataFrame'>\n",
      " |      RangeIndex: 5 entries, 0 to 4\n",
      " |      Data columns (total 3 columns):\n",
      " |       #   Column     Non-Null Count  Dtype\n",
      " |      ---  ------     --------------  -----\n",
      " |       0   int_col    5 non-null      int64\n",
      " |       1   text_col   5 non-null      object\n",
      " |       2   float_col  5 non-null      float64\n",
      " |      dtypes: float64(1), int64(1), object(1)\n",
      " |      memory usage: 248.0+ bytes\n",
      " |\n",
      " |      Prints a summary of columns count and its dtypes but not per column\n",
      " |      information:\n",
      " |\n",
      " |      >>> df.info(verbose=False)\n",
      " |      <class 'pandas.core.frame.DataFrame'>\n",
      " |      RangeIndex: 5 entries, 0 to 4\n",
      " |      Columns: 3 entries, int_col to float_col\n",
      " |      dtypes: float64(1), int64(1), object(1)\n",
      " |      memory usage: 248.0+ bytes\n",
      " |\n",
      " |      Pipe output of DataFrame.info to buffer instead of sys.stdout, get\n",
      " |      buffer content and writes to a text file:\n",
      " |\n",
      " |      >>> import io\n",
      " |      >>> buffer = io.StringIO()\n",
      " |      >>> df.info(buf=buffer)\n",
      " |      >>> s = buffer.getvalue()\n",
      " |      >>> with open(\"df_info.txt\", \"w\",\n",
      " |      ...           encoding=\"utf-8\") as f:  # doctest: +SKIP\n",
      " |      ...     f.write(s)\n",
      " |      260\n",
      " |\n",
      " |      The `memory_usage` parameter allows deep introspection mode, specially\n",
      " |      useful for big DataFrames and fine-tune memory optimization:\n",
      " |\n",
      " |      >>> random_strings_array = np.random.choice(['a', 'b', 'c'], 10 ** 6)\n",
      " |      >>> df = pd.DataFrame({\n",
      " |      ...     'column_1': np.random.choice(['a', 'b', 'c'], 10 ** 6),\n",
      " |      ...     'column_2': np.random.choice(['a', 'b', 'c'], 10 ** 6),\n",
      " |      ...     'column_3': np.random.choice(['a', 'b', 'c'], 10 ** 6)\n",
      " |      ... })\n",
      " |      >>> df.info()\n",
      " |      <class 'pandas.core.frame.DataFrame'>\n",
      " |      RangeIndex: 1000000 entries, 0 to 999999\n",
      " |      Data columns (total 3 columns):\n",
      " |       #   Column    Non-Null Count    Dtype\n",
      " |      ---  ------    --------------    -----\n",
      " |       0   column_1  1000000 non-null  object\n",
      " |       1   column_2  1000000 non-null  object\n",
      " |       2   column_3  1000000 non-null  object\n",
      " |      dtypes: object(3)\n",
      " |      memory usage: 22.9+ MB\n",
      " |\n",
      " |      >>> df.info(memory_usage='deep')\n",
      " |      <class 'pandas.core.frame.DataFrame'>\n",
      " |      RangeIndex: 1000000 entries, 0 to 999999\n",
      " |      Data columns (total 3 columns):\n",
      " |       #   Column    Non-Null Count    Dtype\n",
      " |      ---  ------    --------------    -----\n",
      " |       0   column_1  1000000 non-null  object\n",
      " |       1   column_2  1000000 non-null  object\n",
      " |       2   column_3  1000000 non-null  object\n",
      " |      dtypes: object(3)\n",
      " |      memory usage: 165.9 MB\n",
      " |\n",
      " |  insert(self, loc: 'int', column: 'Hashable', value: 'Scalar | AnyArrayLike', allow_duplicates: 'bool | lib.NoDefault' = <no_default>) -> 'None'\n",
      " |      Insert column into DataFrame at specified location.\n",
      " |\n",
      " |      Raises a ValueError if `column` is already contained in the DataFrame,\n",
      " |      unless `allow_duplicates` is set to True.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      loc : int\n",
      " |          Insertion index. Must verify 0 <= loc <= len(columns).\n",
      " |      column : str, number, or hashable object\n",
      " |          Label of the inserted column.\n",
      " |      value : Scalar, Series, or array-like\n",
      " |          Content of the inserted column.\n",
      " |      allow_duplicates : bool, optional, default lib.no_default\n",
      " |          Allow duplicate column labels to be created.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Index.insert : Insert new item by index.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n",
      " |      >>> df\n",
      " |         col1  col2\n",
      " |      0     1     3\n",
      " |      1     2     4\n",
      " |      >>> df.insert(1, \"newcol\", [99, 99])\n",
      " |      >>> df\n",
      " |         col1  newcol  col2\n",
      " |      0     1      99     3\n",
      " |      1     2      99     4\n",
      " |      >>> df.insert(0, \"col1\", [100, 100], allow_duplicates=True)\n",
      " |      >>> df\n",
      " |         col1  col1  newcol  col2\n",
      " |      0   100     1      99     3\n",
      " |      1   100     2      99     4\n",
      " |\n",
      " |      Notice that pandas uses index alignment in case of `value` from type `Series`:\n",
      " |\n",
      " |      >>> df.insert(0, \"col0\", pd.Series([5, 6], index=[1, 2]))\n",
      " |      >>> df\n",
      " |         col0  col1  col1  newcol  col2\n",
      " |      0   NaN   100     1      99     3\n",
      " |      1   5.0   100     2      99     4\n",
      " |\n",
      " |  isetitem(self, loc, value) -> 'None'\n",
      " |      Set the given value in the column with position `loc`.\n",
      " |\n",
      " |      This is a positional analogue to ``__setitem__``.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      loc : int or sequence of ints\n",
      " |          Index position for the column.\n",
      " |      value : scalar or arraylike\n",
      " |          Value(s) for the column.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      ``frame.isetitem(loc, value)`` is an in-place method as it will\n",
      " |      modify the DataFrame in place (not returning a new object). In contrast to\n",
      " |      ``frame.iloc[:, i] = value`` which will try to update the existing values in\n",
      " |      place, ``frame.isetitem(loc, value)`` will not update the values of the column\n",
      " |      itself in place, it will instead insert a new array.\n",
      " |\n",
      " |      In cases where ``frame.columns`` is unique, this is equivalent to\n",
      " |      ``frame[frame.columns[i]] = value``.\n",
      " |\n",
      " |  isin(self, values: 'Series | DataFrame | Sequence | Mapping') -> 'DataFrame'\n",
      " |      Whether each element in the DataFrame is contained in values.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      values : iterable, Series, DataFrame or dict\n",
      " |          The result will only be true at a location if all the\n",
      " |          labels match. If `values` is a Series, that's the index. If\n",
      " |          `values` is a dict, the keys must be the column names,\n",
      " |          which must match. If `values` is a DataFrame,\n",
      " |          then both the index and column labels must match.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          DataFrame of booleans showing whether each element in the DataFrame\n",
      " |          is contained in values.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.eq: Equality test for DataFrame.\n",
      " |      Series.isin: Equivalent method on Series.\n",
      " |      Series.str.contains: Test if pattern or regex is contained within a\n",
      " |          string of a Series or Index.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'num_legs': [2, 4], 'num_wings': [2, 0]},\n",
      " |      ...                   index=['falcon', 'dog'])\n",
      " |      >>> df\n",
      " |              num_legs  num_wings\n",
      " |      falcon         2          2\n",
      " |      dog            4          0\n",
      " |\n",
      " |      When ``values`` is a list check whether every value in the DataFrame\n",
      " |      is present in the list (which animals have 0 or 2 legs or wings)\n",
      " |\n",
      " |      >>> df.isin([0, 2])\n",
      " |              num_legs  num_wings\n",
      " |      falcon      True       True\n",
      " |      dog        False       True\n",
      " |\n",
      " |      To check if ``values`` is *not* in the DataFrame, use the ``~`` operator:\n",
      " |\n",
      " |      >>> ~df.isin([0, 2])\n",
      " |              num_legs  num_wings\n",
      " |      falcon     False      False\n",
      " |      dog         True      False\n",
      " |\n",
      " |      When ``values`` is a dict, we can pass values to check for each\n",
      " |      column separately:\n",
      " |\n",
      " |      >>> df.isin({'num_wings': [0, 3]})\n",
      " |              num_legs  num_wings\n",
      " |      falcon     False      False\n",
      " |      dog        False       True\n",
      " |\n",
      " |      When ``values`` is a Series or DataFrame the index and column must\n",
      " |      match. Note that 'falcon' does not match based on the number of legs\n",
      " |      in other.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'num_legs': [8, 3], 'num_wings': [0, 2]},\n",
      " |      ...                      index=['spider', 'falcon'])\n",
      " |      >>> df.isin(other)\n",
      " |              num_legs  num_wings\n",
      " |      falcon     False       True\n",
      " |      dog        False      False\n",
      " |\n",
      " |  isna(self) -> 'DataFrame'\n",
      " |      Detect missing values.\n",
      " |\n",
      " |      Return a boolean same-sized object indicating if the values are NA.\n",
      " |      NA values, such as None or :attr:`numpy.NaN`, gets mapped to True\n",
      " |      values.\n",
      " |      Everything else gets mapped to False values. Characters such as empty\n",
      " |      strings ``''`` or :attr:`numpy.inf` are not considered NA values\n",
      " |      (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Mask of bool values for each element in DataFrame that\n",
      " |          indicates whether an element is an NA value.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.isnull : Alias of isna.\n",
      " |      DataFrame.notna : Boolean inverse of isna.\n",
      " |      DataFrame.dropna : Omit axes labels with missing values.\n",
      " |      isna : Top-level isna.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Show which entries in a DataFrame are NA.\n",
      " |\n",
      " |      >>> df = pd.DataFrame(dict(age=[5, 6, np.nan],\n",
      " |      ...                        born=[pd.NaT, pd.Timestamp('1939-05-27'),\n",
      " |      ...                              pd.Timestamp('1940-04-25')],\n",
      " |      ...                        name=['Alfred', 'Batman', ''],\n",
      " |      ...                        toy=[None, 'Batmobile', 'Joker']))\n",
      " |      >>> df\n",
      " |         age       born    name        toy\n",
      " |      0  5.0        NaT  Alfred       None\n",
      " |      1  6.0 1939-05-27  Batman  Batmobile\n",
      " |      2  NaN 1940-04-25              Joker\n",
      " |\n",
      " |      >>> df.isna()\n",
      " |           age   born   name    toy\n",
      " |      0  False   True  False   True\n",
      " |      1  False  False  False  False\n",
      " |      2   True  False  False  False\n",
      " |\n",
      " |      Show which entries in a Series are NA.\n",
      " |\n",
      " |      >>> ser = pd.Series([5, 6, np.nan])\n",
      " |      >>> ser\n",
      " |      0    5.0\n",
      " |      1    6.0\n",
      " |      2    NaN\n",
      " |      dtype: float64\n",
      " |\n",
      " |      >>> ser.isna()\n",
      " |      0    False\n",
      " |      1    False\n",
      " |      2     True\n",
      " |      dtype: bool\n",
      " |\n",
      " |  isnull(self) -> 'DataFrame'\n",
      " |      DataFrame.isnull is an alias for DataFrame.isna.\n",
      " |\n",
      " |      Detect missing values.\n",
      " |\n",
      " |      Return a boolean same-sized object indicating if the values are NA.\n",
      " |      NA values, such as None or :attr:`numpy.NaN`, gets mapped to True\n",
      " |      values.\n",
      " |      Everything else gets mapped to False values. Characters such as empty\n",
      " |      strings ``''`` or :attr:`numpy.inf` are not considered NA values\n",
      " |      (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Mask of bool values for each element in DataFrame that\n",
      " |          indicates whether an element is an NA value.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.isnull : Alias of isna.\n",
      " |      DataFrame.notna : Boolean inverse of isna.\n",
      " |      DataFrame.dropna : Omit axes labels with missing values.\n",
      " |      isna : Top-level isna.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Show which entries in a DataFrame are NA.\n",
      " |\n",
      " |      >>> df = pd.DataFrame(dict(age=[5, 6, np.nan],\n",
      " |      ...                        born=[pd.NaT, pd.Timestamp('1939-05-27'),\n",
      " |      ...                              pd.Timestamp('1940-04-25')],\n",
      " |      ...                        name=['Alfred', 'Batman', ''],\n",
      " |      ...                        toy=[None, 'Batmobile', 'Joker']))\n",
      " |      >>> df\n",
      " |         age       born    name        toy\n",
      " |      0  5.0        NaT  Alfred       None\n",
      " |      1  6.0 1939-05-27  Batman  Batmobile\n",
      " |      2  NaN 1940-04-25              Joker\n",
      " |\n",
      " |      >>> df.isna()\n",
      " |           age   born   name    toy\n",
      " |      0  False   True  False   True\n",
      " |      1  False  False  False  False\n",
      " |      2   True  False  False  False\n",
      " |\n",
      " |      Show which entries in a Series are NA.\n",
      " |\n",
      " |      >>> ser = pd.Series([5, 6, np.nan])\n",
      " |      >>> ser\n",
      " |      0    5.0\n",
      " |      1    6.0\n",
      " |      2    NaN\n",
      " |      dtype: float64\n",
      " |\n",
      " |      >>> ser.isna()\n",
      " |      0    False\n",
      " |      1    False\n",
      " |      2     True\n",
      " |      dtype: bool\n",
      " |\n",
      " |  items(self) -> 'Iterable[tuple[Hashable, Series]]'\n",
      " |      Iterate over (column name, Series) pairs.\n",
      " |\n",
      " |      Iterates over the DataFrame columns, returning a tuple with\n",
      " |      the column name and the content as a Series.\n",
      " |\n",
      " |      Yields\n",
      " |      ------\n",
      " |      label : object\n",
      " |          The column names for the DataFrame being iterated over.\n",
      " |      content : Series\n",
      " |          The column entries belonging to each label, as a Series.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.iterrows : Iterate over DataFrame rows as\n",
      " |          (index, Series) pairs.\n",
      " |      DataFrame.itertuples : Iterate over DataFrame rows as namedtuples\n",
      " |          of the values.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'species': ['bear', 'bear', 'marsupial'],\n",
      " |      ...                   'population': [1864, 22000, 80000]},\n",
      " |      ...                   index=['panda', 'polar', 'koala'])\n",
      " |      >>> df\n",
      " |              species   population\n",
      " |      panda   bear      1864\n",
      " |      polar   bear      22000\n",
      " |      koala   marsupial 80000\n",
      " |      >>> for label, content in df.items():\n",
      " |      ...     print(f'label: {label}')\n",
      " |      ...     print(f'content: {content}', sep='\\n')\n",
      " |      ...\n",
      " |      label: species\n",
      " |      content:\n",
      " |      panda         bear\n",
      " |      polar         bear\n",
      " |      koala    marsupial\n",
      " |      Name: species, dtype: object\n",
      " |      label: population\n",
      " |      content:\n",
      " |      panda     1864\n",
      " |      polar    22000\n",
      " |      koala    80000\n",
      " |      Name: population, dtype: int64\n",
      " |\n",
      " |  iterrows(self) -> 'Iterable[tuple[Hashable, Series]]'\n",
      " |      Iterate over DataFrame rows as (index, Series) pairs.\n",
      " |\n",
      " |      Yields\n",
      " |      ------\n",
      " |      index : label or tuple of label\n",
      " |          The index of the row. A tuple for a `MultiIndex`.\n",
      " |      data : Series\n",
      " |          The data of the row as a Series.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.itertuples : Iterate over DataFrame rows as namedtuples of the values.\n",
      " |      DataFrame.items : Iterate over (column name, Series) pairs.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      1. Because ``iterrows`` returns a Series for each row,\n",
      " |         it does **not** preserve dtypes across the rows (dtypes are\n",
      " |         preserved across columns for DataFrames).\n",
      " |\n",
      " |         To preserve dtypes while iterating over the rows, it is better\n",
      " |         to use :meth:`itertuples` which returns namedtuples of the values\n",
      " |         and which is generally faster than ``iterrows``.\n",
      " |\n",
      " |      2. You should **never modify** something you are iterating over.\n",
      " |         This is not guaranteed to work in all cases. Depending on the\n",
      " |         data types, the iterator returns a copy and not a view, and writing\n",
      " |         to it will have no effect.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      >>> df = pd.DataFrame([[1, 1.5]], columns=['int', 'float'])\n",
      " |      >>> row = next(df.iterrows())[1]\n",
      " |      >>> row\n",
      " |      int      1.0\n",
      " |      float    1.5\n",
      " |      Name: 0, dtype: float64\n",
      " |      >>> print(row['int'].dtype)\n",
      " |      float64\n",
      " |      >>> print(df['int'].dtype)\n",
      " |      int64\n",
      " |\n",
      " |  itertuples(self, index: 'bool' = True, name: 'str | None' = 'Pandas') -> 'Iterable[tuple[Any, ...]]'\n",
      " |      Iterate over DataFrame rows as namedtuples.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      index : bool, default True\n",
      " |          If True, return the index as the first element of the tuple.\n",
      " |      name : str or None, default \"Pandas\"\n",
      " |          The name of the returned namedtuples or None to return regular\n",
      " |          tuples.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      iterator\n",
      " |          An object to iterate over namedtuples for each row in the\n",
      " |          DataFrame with the first field possibly being the index and\n",
      " |          following fields being the column values.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.iterrows : Iterate over DataFrame rows as (index, Series)\n",
      " |          pairs.\n",
      " |      DataFrame.items : Iterate over (column name, Series) pairs.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      The column names will be renamed to positional names if they are\n",
      " |      invalid Python identifiers, repeated, or start with an underscore.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'num_legs': [4, 2], 'num_wings': [0, 2]},\n",
      " |      ...                   index=['dog', 'hawk'])\n",
      " |      >>> df\n",
      " |            num_legs  num_wings\n",
      " |      dog          4          0\n",
      " |      hawk         2          2\n",
      " |      >>> for row in df.itertuples():\n",
      " |      ...     print(row)\n",
      " |      ...\n",
      " |      Pandas(Index='dog', num_legs=4, num_wings=0)\n",
      " |      Pandas(Index='hawk', num_legs=2, num_wings=2)\n",
      " |\n",
      " |      By setting the `index` parameter to False we can remove the index\n",
      " |      as the first element of the tuple:\n",
      " |\n",
      " |      >>> for row in df.itertuples(index=False):\n",
      " |      ...     print(row)\n",
      " |      ...\n",
      " |      Pandas(num_legs=4, num_wings=0)\n",
      " |      Pandas(num_legs=2, num_wings=2)\n",
      " |\n",
      " |      With the `name` parameter set we set a custom name for the yielded\n",
      " |      namedtuples:\n",
      " |\n",
      " |      >>> for row in df.itertuples(name='Animal'):\n",
      " |      ...     print(row)\n",
      " |      ...\n",
      " |      Animal(Index='dog', num_legs=4, num_wings=0)\n",
      " |      Animal(Index='hawk', num_legs=2, num_wings=2)\n",
      " |\n",
      " |  join(self, other: 'DataFrame | Series | Iterable[DataFrame | Series]', on: 'IndexLabel | None' = None, how: 'MergeHow' = 'left', lsuffix: 'str' = '', rsuffix: 'str' = '', sort: 'bool' = False, validate: 'JoinValidate | None' = None) -> 'DataFrame'\n",
      " |      Join columns of another DataFrame.\n",
      " |\n",
      " |      Join columns with `other` DataFrame either on index or on a key\n",
      " |      column. Efficiently join multiple DataFrame objects by index at once by\n",
      " |      passing a list.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : DataFrame, Series, or a list containing any combination of them\n",
      " |          Index should be similar to one of the columns in this one. If a\n",
      " |          Series is passed, its name attribute must be set, and that will be\n",
      " |          used as the column name in the resulting joined DataFrame.\n",
      " |      on : str, list of str, or array-like, optional\n",
      " |          Column or index level name(s) in the caller to join on the index\n",
      " |          in `other`, otherwise joins index-on-index. If multiple\n",
      " |          values given, the `other` DataFrame must have a MultiIndex. Can\n",
      " |          pass an array as the join key if it is not already contained in\n",
      " |          the calling DataFrame. Like an Excel VLOOKUP operation.\n",
      " |      how : {'left', 'right', 'outer', 'inner', 'cross'}, default 'left'\n",
      " |          How to handle the operation of the two objects.\n",
      " |\n",
      " |          * left: use calling frame's index (or column if on is specified)\n",
      " |          * right: use `other`'s index.\n",
      " |          * outer: form union of calling frame's index (or column if on is\n",
      " |            specified) with `other`'s index, and sort it lexicographically.\n",
      " |          * inner: form intersection of calling frame's index (or column if\n",
      " |            on is specified) with `other`'s index, preserving the order\n",
      " |            of the calling's one.\n",
      " |          * cross: creates the cartesian product from both frames, preserves the order\n",
      " |            of the left keys.\n",
      " |      lsuffix : str, default ''\n",
      " |          Suffix to use from left frame's overlapping columns.\n",
      " |      rsuffix : str, default ''\n",
      " |          Suffix to use from right frame's overlapping columns.\n",
      " |      sort : bool, default False\n",
      " |          Order result DataFrame lexicographically by the join key. If False,\n",
      " |          the order of the join key depends on the join type (how keyword).\n",
      " |      validate : str, optional\n",
      " |          If specified, checks if join is of specified type.\n",
      " |\n",
      " |          * \"one_to_one\" or \"1:1\": check if join keys are unique in both left\n",
      " |            and right datasets.\n",
      " |          * \"one_to_many\" or \"1:m\": check if join keys are unique in left dataset.\n",
      " |          * \"many_to_one\" or \"m:1\": check if join keys are unique in right dataset.\n",
      " |          * \"many_to_many\" or \"m:m\": allowed, but does not result in checks.\n",
      " |\n",
      " |          .. versionadded:: 1.5.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          A dataframe containing columns from both the caller and `other`.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.merge : For column(s)-on-column(s) operations.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Parameters `on`, `lsuffix`, and `rsuffix` are not supported when\n",
      " |      passing a list of `DataFrame` objects.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'],\n",
      " |      ...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\n",
      " |\n",
      " |      >>> df\n",
      " |        key   A\n",
      " |      0  K0  A0\n",
      " |      1  K1  A1\n",
      " |      2  K2  A2\n",
      " |      3  K3  A3\n",
      " |      4  K4  A4\n",
      " |      5  K5  A5\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'key': ['K0', 'K1', 'K2'],\n",
      " |      ...                       'B': ['B0', 'B1', 'B2']})\n",
      " |\n",
      " |      >>> other\n",
      " |        key   B\n",
      " |      0  K0  B0\n",
      " |      1  K1  B1\n",
      " |      2  K2  B2\n",
      " |\n",
      " |      Join DataFrames using their indexes.\n",
      " |\n",
      " |      >>> df.join(other, lsuffix='_caller', rsuffix='_other')\n",
      " |        key_caller   A key_other    B\n",
      " |      0         K0  A0        K0   B0\n",
      " |      1         K1  A1        K1   B1\n",
      " |      2         K2  A2        K2   B2\n",
      " |      3         K3  A3       NaN  NaN\n",
      " |      4         K4  A4       NaN  NaN\n",
      " |      5         K5  A5       NaN  NaN\n",
      " |\n",
      " |      If we want to join using the key columns, we need to set key to be\n",
      " |      the index in both `df` and `other`. The joined DataFrame will have\n",
      " |      key as its index.\n",
      " |\n",
      " |      >>> df.set_index('key').join(other.set_index('key'))\n",
      " |            A    B\n",
      " |      key\n",
      " |      K0   A0   B0\n",
      " |      K1   A1   B1\n",
      " |      K2   A2   B2\n",
      " |      K3   A3  NaN\n",
      " |      K4   A4  NaN\n",
      " |      K5   A5  NaN\n",
      " |\n",
      " |      Another option to join using the key columns is to use the `on`\n",
      " |      parameter. DataFrame.join always uses `other`'s index but we can use\n",
      " |      any column in `df`. This method preserves the original DataFrame's\n",
      " |      index in the result.\n",
      " |\n",
      " |      >>> df.join(other.set_index('key'), on='key')\n",
      " |        key   A    B\n",
      " |      0  K0  A0   B0\n",
      " |      1  K1  A1   B1\n",
      " |      2  K2  A2   B2\n",
      " |      3  K3  A3  NaN\n",
      " |      4  K4  A4  NaN\n",
      " |      5  K5  A5  NaN\n",
      " |\n",
      " |      Using non-unique key values shows how they are matched.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'key': ['K0', 'K1', 'K1', 'K3', 'K0', 'K1'],\n",
      " |      ...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\n",
      " |\n",
      " |      >>> df\n",
      " |        key   A\n",
      " |      0  K0  A0\n",
      " |      1  K1  A1\n",
      " |      2  K1  A2\n",
      " |      3  K3  A3\n",
      " |      4  K0  A4\n",
      " |      5  K1  A5\n",
      " |\n",
      " |      >>> df.join(other.set_index('key'), on='key', validate='m:1')\n",
      " |        key   A    B\n",
      " |      0  K0  A0   B0\n",
      " |      1  K1  A1   B1\n",
      " |      2  K1  A2   B1\n",
      " |      3  K3  A3  NaN\n",
      " |      4  K0  A4   B0\n",
      " |      5  K1  A5   B1\n",
      " |\n",
      " |  kurt(self, axis: 'Axis | None' = 0, skipna: 'bool' = True, numeric_only: 'bool' = False, **kwargs)\n",
      " |      Return unbiased kurtosis over requested axis.\n",
      " |\n",
      " |      Kurtosis obtained using Fisher's definition of\n",
      " |      kurtosis (kurtosis of normal == 0.0). Normalized by N-1.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {index (0), columns (1)}\n",
      " |          Axis for the function to be applied on.\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |\n",
      " |          For DataFrames, specifying ``axis=None`` will apply the aggregation\n",
      " |          across both axes.\n",
      " |\n",
      " |          .. versionadded:: 2.0.0\n",
      " |\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values when computing the result.\n",
      " |      numeric_only : bool, default False\n",
      " |          Include only float, int, boolean columns. Not implemented for Series.\n",
      " |\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to be passed to the function.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or scalar\n",
      " |\n",
      " |                  Examples\n",
      " |                  --------\n",
      " |                  >>> s = pd.Series([1, 2, 2, 3], index=['cat', 'dog', 'dog', 'mouse'])\n",
      " |                  >>> s\n",
      " |                  cat    1\n",
      " |                  dog    2\n",
      " |                  dog    2\n",
      " |                  mouse  3\n",
      " |                  dtype: int64\n",
      " |                  >>> s.kurt()\n",
      " |                  1.5\n",
      " |\n",
      " |                  With a DataFrame\n",
      " |\n",
      " |                  >>> df = pd.DataFrame({'a': [1, 2, 2, 3], 'b': [3, 4, 4, 4]},\n",
      " |                  ...                   index=['cat', 'dog', 'dog', 'mouse'])\n",
      " |                  >>> df\n",
      " |                         a   b\n",
      " |                    cat  1   3\n",
      " |                    dog  2   4\n",
      " |                    dog  2   4\n",
      " |                  mouse  3   4\n",
      " |                  >>> df.kurt()\n",
      " |                  a   1.5\n",
      " |                  b   4.0\n",
      " |                  dtype: float64\n",
      " |\n",
      " |                  With axis=None\n",
      " |\n",
      " |                  >>> df.kurt(axis=None).round(6)\n",
      " |                  -0.988693\n",
      " |\n",
      " |                  Using axis=1\n",
      " |\n",
      " |                  >>> df = pd.DataFrame({'a': [1, 2], 'b': [3, 4], 'c': [3, 4], 'd': [1, 2]},\n",
      " |                  ...                   index=['cat', 'dog'])\n",
      " |                  >>> df.kurt(axis=1)\n",
      " |                  cat   -6.0\n",
      " |                  dog   -6.0\n",
      " |                  dtype: float64\n",
      " |\n",
      " |  kurtosis = kurt(self, axis: 'Axis | None' = 0, skipna: 'bool' = True, numeric_only: 'bool' = False, **kwargs)\n",
      " |\n",
      " |  le(self, other, axis: 'Axis' = 'columns', level=None) -> 'DataFrame'\n",
      " |      Get Less than or equal to of dataframe and other, element-wise (binary operator `le`).\n",
      " |\n",
      " |      Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n",
      " |      operators.\n",
      " |\n",
      " |      Equivalent to `==`, `!=`, `<=`, `<`, `>=`, `>` with support to choose axis\n",
      " |      (rows or columns) and level for comparison.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns').\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the passed\n",
      " |          MultiIndex level.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame of bool\n",
      " |          Result of the comparison.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.eq : Compare DataFrames for equality elementwise.\n",
      " |      DataFrame.ne : Compare DataFrames for inequality elementwise.\n",
      " |      DataFrame.le : Compare DataFrames for less than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.lt : Compare DataFrames for strictly less than\n",
      " |          inequality elementwise.\n",
      " |      DataFrame.ge : Compare DataFrames for greater than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.gt : Compare DataFrames for strictly greater than\n",
      " |          inequality elementwise.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      `NaN` values are considered different (i.e. `NaN` != `NaN`).\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'cost': [250, 150, 100],\n",
      " |      ...                    'revenue': [100, 250, 300]},\n",
      " |      ...                   index=['A', 'B', 'C'])\n",
      " |      >>> df\n",
      " |         cost  revenue\n",
      " |      A   250      100\n",
      " |      B   150      250\n",
      " |      C   100      300\n",
      " |\n",
      " |      Comparison with a scalar, using either the operator or method:\n",
      " |\n",
      " |      >>> df == 100\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |\n",
      " |      >>> df.eq(100)\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |\n",
      " |      When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n",
      " |      with the index of `other` and broadcast:\n",
      " |\n",
      " |      >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B   True    False\n",
      " |      C  False     True\n",
      " |\n",
      " |      Use the method to control the broadcast axis:\n",
      " |\n",
      " |      >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')\n",
      " |         cost  revenue\n",
      " |      A  True    False\n",
      " |      B  True     True\n",
      " |      C  True     True\n",
      " |      D  True     True\n",
      " |\n",
      " |      When comparing to an arbitrary sequence, the number of columns must\n",
      " |      match the number elements in `other`:\n",
      " |\n",
      " |      >>> df == [250, 100]\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B  False    False\n",
      " |      C  False    False\n",
      " |\n",
      " |      Use the method to control the axis:\n",
      " |\n",
      " |      >>> df.eq([250, 250, 100], axis='index')\n",
      " |          cost  revenue\n",
      " |      A   True    False\n",
      " |      B  False     True\n",
      " |      C   True    False\n",
      " |\n",
      " |      Compare to a DataFrame of different shape.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n",
      " |      ...                      index=['A', 'B', 'C', 'D'])\n",
      " |      >>> other\n",
      " |         revenue\n",
      " |      A      300\n",
      " |      B      250\n",
      " |      C      100\n",
      " |      D      150\n",
      " |\n",
      " |      >>> df.gt(other)\n",
      " |          cost  revenue\n",
      " |      A  False    False\n",
      " |      B  False    False\n",
      " |      C  False     True\n",
      " |      D  False    False\n",
      " |\n",
      " |      Compare to a MultiIndex by level.\n",
      " |\n",
      " |      >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n",
      " |      ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n",
      " |      ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n",
      " |      ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n",
      " |      >>> df_multindex\n",
      " |            cost  revenue\n",
      " |      Q1 A   250      100\n",
      " |         B   150      250\n",
      " |         C   100      300\n",
      " |      Q2 A   150      200\n",
      " |         B   300      175\n",
      " |         C   220      225\n",
      " |\n",
      " |      >>> df.le(df_multindex, level=1)\n",
      " |             cost  revenue\n",
      " |      Q1 A   True     True\n",
      " |         B   True     True\n",
      " |         C   True     True\n",
      " |      Q2 A  False     True\n",
      " |         B   True    False\n",
      " |         C   True    False\n",
      " |\n",
      " |  lt(self, other, axis: 'Axis' = 'columns', level=None) -> 'DataFrame'\n",
      " |      Get Less than of dataframe and other, element-wise (binary operator `lt`).\n",
      " |\n",
      " |      Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n",
      " |      operators.\n",
      " |\n",
      " |      Equivalent to `==`, `!=`, `<=`, `<`, `>=`, `>` with support to choose axis\n",
      " |      (rows or columns) and level for comparison.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns').\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the passed\n",
      " |          MultiIndex level.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame of bool\n",
      " |          Result of the comparison.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.eq : Compare DataFrames for equality elementwise.\n",
      " |      DataFrame.ne : Compare DataFrames for inequality elementwise.\n",
      " |      DataFrame.le : Compare DataFrames for less than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.lt : Compare DataFrames for strictly less than\n",
      " |          inequality elementwise.\n",
      " |      DataFrame.ge : Compare DataFrames for greater than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.gt : Compare DataFrames for strictly greater than\n",
      " |          inequality elementwise.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      `NaN` values are considered different (i.e. `NaN` != `NaN`).\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'cost': [250, 150, 100],\n",
      " |      ...                    'revenue': [100, 250, 300]},\n",
      " |      ...                   index=['A', 'B', 'C'])\n",
      " |      >>> df\n",
      " |         cost  revenue\n",
      " |      A   250      100\n",
      " |      B   150      250\n",
      " |      C   100      300\n",
      " |\n",
      " |      Comparison with a scalar, using either the operator or method:\n",
      " |\n",
      " |      >>> df == 100\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |\n",
      " |      >>> df.eq(100)\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |\n",
      " |      When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n",
      " |      with the index of `other` and broadcast:\n",
      " |\n",
      " |      >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B   True    False\n",
      " |      C  False     True\n",
      " |\n",
      " |      Use the method to control the broadcast axis:\n",
      " |\n",
      " |      >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')\n",
      " |         cost  revenue\n",
      " |      A  True    False\n",
      " |      B  True     True\n",
      " |      C  True     True\n",
      " |      D  True     True\n",
      " |\n",
      " |      When comparing to an arbitrary sequence, the number of columns must\n",
      " |      match the number elements in `other`:\n",
      " |\n",
      " |      >>> df == [250, 100]\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B  False    False\n",
      " |      C  False    False\n",
      " |\n",
      " |      Use the method to control the axis:\n",
      " |\n",
      " |      >>> df.eq([250, 250, 100], axis='index')\n",
      " |          cost  revenue\n",
      " |      A   True    False\n",
      " |      B  False     True\n",
      " |      C   True    False\n",
      " |\n",
      " |      Compare to a DataFrame of different shape.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n",
      " |      ...                      index=['A', 'B', 'C', 'D'])\n",
      " |      >>> other\n",
      " |         revenue\n",
      " |      A      300\n",
      " |      B      250\n",
      " |      C      100\n",
      " |      D      150\n",
      " |\n",
      " |      >>> df.gt(other)\n",
      " |          cost  revenue\n",
      " |      A  False    False\n",
      " |      B  False    False\n",
      " |      C  False     True\n",
      " |      D  False    False\n",
      " |\n",
      " |      Compare to a MultiIndex by level.\n",
      " |\n",
      " |      >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n",
      " |      ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n",
      " |      ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n",
      " |      ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n",
      " |      >>> df_multindex\n",
      " |            cost  revenue\n",
      " |      Q1 A   250      100\n",
      " |         B   150      250\n",
      " |         C   100      300\n",
      " |      Q2 A   150      200\n",
      " |         B   300      175\n",
      " |         C   220      225\n",
      " |\n",
      " |      >>> df.le(df_multindex, level=1)\n",
      " |             cost  revenue\n",
      " |      Q1 A   True     True\n",
      " |         B   True     True\n",
      " |         C   True     True\n",
      " |      Q2 A  False     True\n",
      " |         B   True    False\n",
      " |         C   True    False\n",
      " |\n",
      " |  map(self, func: 'PythonFuncType', na_action: 'str | None' = None, **kwargs) -> 'DataFrame'\n",
      " |      Apply a function to a Dataframe elementwise.\n",
      " |\n",
      " |      .. versionadded:: 2.1.0\n",
      " |\n",
      " |         DataFrame.applymap was deprecated and renamed to DataFrame.map.\n",
      " |\n",
      " |      This method applies a function that accepts and returns a scalar\n",
      " |      to every element of a DataFrame.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : callable\n",
      " |          Python function, returns a single value from a single value.\n",
      " |      na_action : {None, 'ignore'}, default None\n",
      " |          If 'ignore', propagate NaN values, without passing them to func.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to pass as keywords arguments to\n",
      " |          `func`.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Transformed DataFrame.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.apply : Apply a function along input axis of DataFrame.\n",
      " |      DataFrame.replace: Replace values given in `to_replace` with `value`.\n",
      " |      Series.map : Apply a function elementwise on a Series.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([[1, 2.12], [3.356, 4.567]])\n",
      " |      >>> df\n",
      " |             0      1\n",
      " |      0  1.000  2.120\n",
      " |      1  3.356  4.567\n",
      " |\n",
      " |      >>> df.map(lambda x: len(str(x)))\n",
      " |         0  1\n",
      " |      0  3  4\n",
      " |      1  5  5\n",
      " |\n",
      " |      Like Series.map, NA values can be ignored:\n",
      " |\n",
      " |      >>> df_copy = df.copy()\n",
      " |      >>> df_copy.iloc[0, 0] = pd.NA\n",
      " |      >>> df_copy.map(lambda x: len(str(x)), na_action='ignore')\n",
      " |           0  1\n",
      " |      0  NaN  4\n",
      " |      1  5.0  5\n",
      " |\n",
      " |      It is also possible to use `map` with functions that are not\n",
      " |      `lambda` functions:\n",
      " |\n",
      " |      >>> df.map(round, ndigits=1)\n",
      " |           0    1\n",
      " |      0  1.0  2.1\n",
      " |      1  3.4  4.6\n",
      " |\n",
      " |      Note that a vectorized version of `func` often exists, which will\n",
      " |      be much faster. You could square each number elementwise.\n",
      " |\n",
      " |      >>> df.map(lambda x: x**2)\n",
      " |                 0          1\n",
      " |      0   1.000000   4.494400\n",
      " |      1  11.262736  20.857489\n",
      " |\n",
      " |      But it's better to avoid map in that case.\n",
      " |\n",
      " |      >>> df ** 2\n",
      " |                 0          1\n",
      " |      0   1.000000   4.494400\n",
      " |      1  11.262736  20.857489\n",
      " |\n",
      " |  max(self, axis: 'Axis | None' = 0, skipna: 'bool' = True, numeric_only: 'bool' = False, **kwargs)\n",
      " |      Return the maximum of the values over the requested axis.\n",
      " |\n",
      " |      If you want the *index* of the maximum, use ``idxmax``. This is the equivalent of the ``numpy.ndarray`` method ``argmax``.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {index (0), columns (1)}\n",
      " |          Axis for the function to be applied on.\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |\n",
      " |          For DataFrames, specifying ``axis=None`` will apply the aggregation\n",
      " |          across both axes.\n",
      " |\n",
      " |          .. versionadded:: 2.0.0\n",
      " |\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values when computing the result.\n",
      " |      numeric_only : bool, default False\n",
      " |          Include only float, int, boolean columns. Not implemented for Series.\n",
      " |\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to be passed to the function.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or scalar\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.sum : Return the sum.\n",
      " |      Series.min : Return the minimum.\n",
      " |      Series.max : Return the maximum.\n",
      " |      Series.idxmin : Return the index of the minimum.\n",
      " |      Series.idxmax : Return the index of the maximum.\n",
      " |      DataFrame.sum : Return the sum over the requested axis.\n",
      " |      DataFrame.min : Return the minimum over the requested axis.\n",
      " |      DataFrame.max : Return the maximum over the requested axis.\n",
      " |      DataFrame.idxmin : Return the index of the minimum over the requested axis.\n",
      " |      DataFrame.idxmax : Return the index of the maximum over the requested axis.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> idx = pd.MultiIndex.from_arrays([\n",
      " |      ...     ['warm', 'warm', 'cold', 'cold'],\n",
      " |      ...     ['dog', 'falcon', 'fish', 'spider']],\n",
      " |      ...     names=['blooded', 'animal'])\n",
      " |      >>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n",
      " |      >>> s\n",
      " |      blooded  animal\n",
      " |      warm     dog       4\n",
      " |               falcon    2\n",
      " |      cold     fish      0\n",
      " |               spider    8\n",
      " |      Name: legs, dtype: int64\n",
      " |\n",
      " |      >>> s.max()\n",
      " |      8\n",
      " |\n",
      " |  mean(self, axis: 'Axis | None' = 0, skipna: 'bool' = True, numeric_only: 'bool' = False, **kwargs)\n",
      " |      Return the mean of the values over the requested axis.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {index (0), columns (1)}\n",
      " |          Axis for the function to be applied on.\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |\n",
      " |          For DataFrames, specifying ``axis=None`` will apply the aggregation\n",
      " |          across both axes.\n",
      " |\n",
      " |          .. versionadded:: 2.0.0\n",
      " |\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values when computing the result.\n",
      " |      numeric_only : bool, default False\n",
      " |          Include only float, int, boolean columns. Not implemented for Series.\n",
      " |\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to be passed to the function.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or scalar\n",
      " |\n",
      " |                  Examples\n",
      " |                  --------\n",
      " |                  >>> s = pd.Series([1, 2, 3])\n",
      " |                  >>> s.mean()\n",
      " |                  2.0\n",
      " |\n",
      " |                  With a DataFrame\n",
      " |\n",
      " |                  >>> df = pd.DataFrame({'a': [1, 2], 'b': [2, 3]}, index=['tiger', 'zebra'])\n",
      " |                  >>> df\n",
      " |                         a   b\n",
      " |                  tiger  1   2\n",
      " |                  zebra  2   3\n",
      " |                  >>> df.mean()\n",
      " |                  a   1.5\n",
      " |                  b   2.5\n",
      " |                  dtype: float64\n",
      " |\n",
      " |                  Using axis=1\n",
      " |\n",
      " |                  >>> df.mean(axis=1)\n",
      " |                  tiger   1.5\n",
      " |                  zebra   2.5\n",
      " |                  dtype: float64\n",
      " |\n",
      " |                  In this case, `numeric_only` should be set to `True` to avoid\n",
      " |                  getting an error.\n",
      " |\n",
      " |                  >>> df = pd.DataFrame({'a': [1, 2], 'b': ['T', 'Z']},\n",
      " |                  ...                   index=['tiger', 'zebra'])\n",
      " |                  >>> df.mean(numeric_only=True)\n",
      " |                  a   1.5\n",
      " |                  dtype: float64\n",
      " |\n",
      " |  median(self, axis: 'Axis | None' = 0, skipna: 'bool' = True, numeric_only: 'bool' = False, **kwargs)\n",
      " |      Return the median of the values over the requested axis.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {index (0), columns (1)}\n",
      " |          Axis for the function to be applied on.\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |\n",
      " |          For DataFrames, specifying ``axis=None`` will apply the aggregation\n",
      " |          across both axes.\n",
      " |\n",
      " |          .. versionadded:: 2.0.0\n",
      " |\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values when computing the result.\n",
      " |      numeric_only : bool, default False\n",
      " |          Include only float, int, boolean columns. Not implemented for Series.\n",
      " |\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to be passed to the function.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or scalar\n",
      " |\n",
      " |                  Examples\n",
      " |                  --------\n",
      " |                  >>> s = pd.Series([1, 2, 3])\n",
      " |                  >>> s.median()\n",
      " |                  2.0\n",
      " |\n",
      " |                  With a DataFrame\n",
      " |\n",
      " |                  >>> df = pd.DataFrame({'a': [1, 2], 'b': [2, 3]}, index=['tiger', 'zebra'])\n",
      " |                  >>> df\n",
      " |                         a   b\n",
      " |                  tiger  1   2\n",
      " |                  zebra  2   3\n",
      " |                  >>> df.median()\n",
      " |                  a   1.5\n",
      " |                  b   2.5\n",
      " |                  dtype: float64\n",
      " |\n",
      " |                  Using axis=1\n",
      " |\n",
      " |                  >>> df.median(axis=1)\n",
      " |                  tiger   1.5\n",
      " |                  zebra   2.5\n",
      " |                  dtype: float64\n",
      " |\n",
      " |                  In this case, `numeric_only` should be set to `True`\n",
      " |                  to avoid getting an error.\n",
      " |\n",
      " |                  >>> df = pd.DataFrame({'a': [1, 2], 'b': ['T', 'Z']},\n",
      " |                  ...                   index=['tiger', 'zebra'])\n",
      " |                  >>> df.median(numeric_only=True)\n",
      " |                  a   1.5\n",
      " |                  dtype: float64\n",
      " |\n",
      " |  melt(self, id_vars=None, value_vars=None, var_name=None, value_name: 'Hashable' = 'value', col_level: 'Level | None' = None, ignore_index: 'bool' = True) -> 'DataFrame'\n",
      " |      Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.\n",
      " |\n",
      " |      This function is useful to massage a DataFrame into a format where one\n",
      " |      or more columns are identifier variables (`id_vars`), while all other\n",
      " |      columns, considered measured variables (`value_vars`), are \"unpivoted\" to\n",
      " |      the row axis, leaving just two non-identifier columns, 'variable' and\n",
      " |      'value'.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      id_vars : scalar, tuple, list, or ndarray, optional\n",
      " |          Column(s) to use as identifier variables.\n",
      " |      value_vars : scalar, tuple, list, or ndarray, optional\n",
      " |          Column(s) to unpivot. If not specified, uses all columns that\n",
      " |          are not set as `id_vars`.\n",
      " |      var_name : scalar, default None\n",
      " |          Name to use for the 'variable' column. If None it uses\n",
      " |          ``frame.columns.name`` or 'variable'.\n",
      " |      value_name : scalar, default 'value'\n",
      " |          Name to use for the 'value' column, can't be an existing column label.\n",
      " |      col_level : scalar, optional\n",
      " |          If columns are a MultiIndex then use this level to melt.\n",
      " |      ignore_index : bool, default True\n",
      " |          If True, original index is ignored. If False, the original index is retained.\n",
      " |          Index labels will be repeated as necessary.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Unpivoted DataFrame.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      melt : Identical method.\n",
      " |      pivot_table : Create a spreadsheet-style pivot table as a DataFrame.\n",
      " |      DataFrame.pivot : Return reshaped DataFrame organized\n",
      " |          by given index / column values.\n",
      " |      DataFrame.explode : Explode a DataFrame from list-like\n",
      " |              columns to long format.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Reference :ref:`the user guide <reshaping.melt>` for more examples.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},\n",
      " |      ...                    'B': {0: 1, 1: 3, 2: 5},\n",
      " |      ...                    'C': {0: 2, 1: 4, 2: 6}})\n",
      " |      >>> df\n",
      " |         A  B  C\n",
      " |      0  a  1  2\n",
      " |      1  b  3  4\n",
      " |      2  c  5  6\n",
      " |\n",
      " |      >>> df.melt(id_vars=['A'], value_vars=['B'])\n",
      " |         A variable  value\n",
      " |      0  a        B      1\n",
      " |      1  b        B      3\n",
      " |      2  c        B      5\n",
      " |\n",
      " |      >>> df.melt(id_vars=['A'], value_vars=['B', 'C'])\n",
      " |         A variable  value\n",
      " |      0  a        B      1\n",
      " |      1  b        B      3\n",
      " |      2  c        B      5\n",
      " |      3  a        C      2\n",
      " |      4  b        C      4\n",
      " |      5  c        C      6\n",
      " |\n",
      " |      The names of 'variable' and 'value' columns can be customized:\n",
      " |\n",
      " |      >>> df.melt(id_vars=['A'], value_vars=['B'],\n",
      " |      ...         var_name='myVarname', value_name='myValname')\n",
      " |         A myVarname  myValname\n",
      " |      0  a         B          1\n",
      " |      1  b         B          3\n",
      " |      2  c         B          5\n",
      " |\n",
      " |      Original index values can be kept around:\n",
      " |\n",
      " |      >>> df.melt(id_vars=['A'], value_vars=['B', 'C'], ignore_index=False)\n",
      " |         A variable  value\n",
      " |      0  a        B      1\n",
      " |      1  b        B      3\n",
      " |      2  c        B      5\n",
      " |      0  a        C      2\n",
      " |      1  b        C      4\n",
      " |      2  c        C      6\n",
      " |\n",
      " |      If you have multi-index columns:\n",
      " |\n",
      " |      >>> df.columns = [list('ABC'), list('DEF')]\n",
      " |      >>> df\n",
      " |         A  B  C\n",
      " |         D  E  F\n",
      " |      0  a  1  2\n",
      " |      1  b  3  4\n",
      " |      2  c  5  6\n",
      " |\n",
      " |      >>> df.melt(col_level=0, id_vars=['A'], value_vars=['B'])\n",
      " |         A variable  value\n",
      " |      0  a        B      1\n",
      " |      1  b        B      3\n",
      " |      2  c        B      5\n",
      " |\n",
      " |      >>> df.melt(id_vars=[('A', 'D')], value_vars=[('B', 'E')])\n",
      " |        (A, D) variable_0 variable_1  value\n",
      " |      0      a          B          E      1\n",
      " |      1      b          B          E      3\n",
      " |      2      c          B          E      5\n",
      " |\n",
      " |  memory_usage(self, index: 'bool' = True, deep: 'bool' = False) -> 'Series'\n",
      " |      Return the memory usage of each column in bytes.\n",
      " |\n",
      " |      The memory usage can optionally include the contribution of\n",
      " |      the index and elements of `object` dtype.\n",
      " |\n",
      " |      This value is displayed in `DataFrame.info` by default. This can be\n",
      " |      suppressed by setting ``pandas.options.display.memory_usage`` to False.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      index : bool, default True\n",
      " |          Specifies whether to include the memory usage of the DataFrame's\n",
      " |          index in returned Series. If ``index=True``, the memory usage of\n",
      " |          the index is the first item in the output.\n",
      " |      deep : bool, default False\n",
      " |          If True, introspect the data deeply by interrogating\n",
      " |          `object` dtypes for system-level memory consumption, and include\n",
      " |          it in the returned values.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series\n",
      " |          A Series whose index is the original column names and whose values\n",
      " |          is the memory usage of each column in bytes.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.ndarray.nbytes : Total bytes consumed by the elements of an\n",
      " |          ndarray.\n",
      " |      Series.memory_usage : Bytes consumed by a Series.\n",
      " |      Categorical : Memory-efficient array for string values with\n",
      " |          many repeated values.\n",
      " |      DataFrame.info : Concise summary of a DataFrame.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      See the :ref:`Frequently Asked Questions <df-memory-usage>` for more\n",
      " |      details.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> dtypes = ['int64', 'float64', 'complex128', 'object', 'bool']\n",
      " |      >>> data = dict([(t, np.ones(shape=5000, dtype=int).astype(t))\n",
      " |      ...              for t in dtypes])\n",
      " |      >>> df = pd.DataFrame(data)\n",
      " |      >>> df.head()\n",
      " |         int64  float64            complex128  object  bool\n",
      " |      0      1      1.0              1.0+0.0j       1  True\n",
      " |      1      1      1.0              1.0+0.0j       1  True\n",
      " |      2      1      1.0              1.0+0.0j       1  True\n",
      " |      3      1      1.0              1.0+0.0j       1  True\n",
      " |      4      1      1.0              1.0+0.0j       1  True\n",
      " |\n",
      " |      >>> df.memory_usage()\n",
      " |      Index           128\n",
      " |      int64         40000\n",
      " |      float64       40000\n",
      " |      complex128    80000\n",
      " |      object        40000\n",
      " |      bool           5000\n",
      " |      dtype: int64\n",
      " |\n",
      " |      >>> df.memory_usage(index=False)\n",
      " |      int64         40000\n",
      " |      float64       40000\n",
      " |      complex128    80000\n",
      " |      object        40000\n",
      " |      bool           5000\n",
      " |      dtype: int64\n",
      " |\n",
      " |      The memory footprint of `object` dtype columns is ignored by default:\n",
      " |\n",
      " |      >>> df.memory_usage(deep=True)\n",
      " |      Index            128\n",
      " |      int64          40000\n",
      " |      float64        40000\n",
      " |      complex128     80000\n",
      " |      object        180000\n",
      " |      bool            5000\n",
      " |      dtype: int64\n",
      " |\n",
      " |      Use a Categorical for efficient storage of an object-dtype column with\n",
      " |      many repeated values.\n",
      " |\n",
      " |      >>> df['object'].astype('category').memory_usage(deep=True)\n",
      " |      5244\n",
      " |\n",
      " |  min(self, axis: 'Axis | None' = 0, skipna: 'bool' = True, numeric_only: 'bool' = False, **kwargs)\n",
      " |      Return the minimum of the values over the requested axis.\n",
      " |\n",
      " |      If you want the *index* of the minimum, use ``idxmin``. This is the equivalent of the ``numpy.ndarray`` method ``argmin``.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {index (0), columns (1)}\n",
      " |          Axis for the function to be applied on.\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |\n",
      " |          For DataFrames, specifying ``axis=None`` will apply the aggregation\n",
      " |          across both axes.\n",
      " |\n",
      " |          .. versionadded:: 2.0.0\n",
      " |\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values when computing the result.\n",
      " |      numeric_only : bool, default False\n",
      " |          Include only float, int, boolean columns. Not implemented for Series.\n",
      " |\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to be passed to the function.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or scalar\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.sum : Return the sum.\n",
      " |      Series.min : Return the minimum.\n",
      " |      Series.max : Return the maximum.\n",
      " |      Series.idxmin : Return the index of the minimum.\n",
      " |      Series.idxmax : Return the index of the maximum.\n",
      " |      DataFrame.sum : Return the sum over the requested axis.\n",
      " |      DataFrame.min : Return the minimum over the requested axis.\n",
      " |      DataFrame.max : Return the maximum over the requested axis.\n",
      " |      DataFrame.idxmin : Return the index of the minimum over the requested axis.\n",
      " |      DataFrame.idxmax : Return the index of the maximum over the requested axis.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> idx = pd.MultiIndex.from_arrays([\n",
      " |      ...     ['warm', 'warm', 'cold', 'cold'],\n",
      " |      ...     ['dog', 'falcon', 'fish', 'spider']],\n",
      " |      ...     names=['blooded', 'animal'])\n",
      " |      >>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n",
      " |      >>> s\n",
      " |      blooded  animal\n",
      " |      warm     dog       4\n",
      " |               falcon    2\n",
      " |      cold     fish      0\n",
      " |               spider    8\n",
      " |      Name: legs, dtype: int64\n",
      " |\n",
      " |      >>> s.min()\n",
      " |      0\n",
      " |\n",
      " |  mod(self, other, axis: 'Axis' = 'columns', level=None, fill_value=None) -> 'DataFrame'\n",
      " |      Get Modulo of dataframe and other, element-wise (binary operator `mod`).\n",
      " |\n",
      " |      Equivalent to ``dataframe % other``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `rmod`.\n",
      " |\n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `floordiv`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, dict or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns.\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |\n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |\n",
      " |      >>> df + 1\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      >>> df.add(1)\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      Divide by constant with reverse version.\n",
      " |\n",
      " |      >>> df.div(10)\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |\n",
      " |      >>> df.rdiv(10)\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |\n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |\n",
      " |      >>> df - [1, 2]\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub([1, 2], axis='columns')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |\n",
      " |      Multiply a dictionary by axis.\n",
      " |\n",
      " |      >>> df.mul({'angles': 0, 'degrees': 2})\n",
      " |                  angles  degrees\n",
      " |      circle           0      720\n",
      " |      triangle         0      360\n",
      " |      rectangle        0      720\n",
      " |\n",
      " |      >>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n",
      " |                  angles  degrees\n",
      " |      circle           0        0\n",
      " |      triangle         6      360\n",
      " |      rectangle       12     1080\n",
      " |\n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |\n",
      " |      >>> df * other\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |\n",
      " |      >>> df.mul(other, fill_value=0)\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |\n",
      " |      Divide by a MultiIndex by level.\n",
      " |\n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |\n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |\n",
      " |  mode(self, axis: 'Axis' = 0, numeric_only: 'bool' = False, dropna: 'bool' = True) -> 'DataFrame'\n",
      " |      Get the mode(s) of each element along the selected axis.\n",
      " |\n",
      " |      The mode of a set of values is the value that appears most often.\n",
      " |      It can be multiple values.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The axis to iterate over while searching for the mode:\n",
      " |\n",
      " |          * 0 or 'index' : get mode of each column\n",
      " |          * 1 or 'columns' : get mode of each row.\n",
      " |\n",
      " |      numeric_only : bool, default False\n",
      " |          If True, only apply to numeric columns.\n",
      " |      dropna : bool, default True\n",
      " |          Don't consider counts of NaN/NaT.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          The modes of each column or row.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.mode : Return the highest frequency value in a Series.\n",
      " |      Series.value_counts : Return the counts of values in a Series.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([('bird', 2, 2),\n",
      " |      ...                    ('mammal', 4, np.nan),\n",
      " |      ...                    ('arthropod', 8, 0),\n",
      " |      ...                    ('bird', 2, np.nan)],\n",
      " |      ...                   index=('falcon', 'horse', 'spider', 'ostrich'),\n",
      " |      ...                   columns=('species', 'legs', 'wings'))\n",
      " |      >>> df\n",
      " |                 species  legs  wings\n",
      " |      falcon        bird     2    2.0\n",
      " |      horse       mammal     4    NaN\n",
      " |      spider   arthropod     8    0.0\n",
      " |      ostrich       bird     2    NaN\n",
      " |\n",
      " |      By default, missing values are not considered, and the mode of wings\n",
      " |      are both 0 and 2. Because the resulting DataFrame has two rows,\n",
      " |      the second row of ``species`` and ``legs`` contains ``NaN``.\n",
      " |\n",
      " |      >>> df.mode()\n",
      " |        species  legs  wings\n",
      " |      0    bird   2.0    0.0\n",
      " |      1     NaN   NaN    2.0\n",
      " |\n",
      " |      Setting ``dropna=False`` ``NaN`` values are considered and they can be\n",
      " |      the mode (like for wings).\n",
      " |\n",
      " |      >>> df.mode(dropna=False)\n",
      " |        species  legs  wings\n",
      " |      0    bird     2    NaN\n",
      " |\n",
      " |      Setting ``numeric_only=True``, only the mode of numeric columns is\n",
      " |      computed, and columns of other types are ignored.\n",
      " |\n",
      " |      >>> df.mode(numeric_only=True)\n",
      " |         legs  wings\n",
      " |      0   2.0    0.0\n",
      " |      1   NaN    2.0\n",
      " |\n",
      " |      To compute the mode over columns and not rows, use the axis parameter:\n",
      " |\n",
      " |      >>> df.mode(axis='columns', numeric_only=True)\n",
      " |                 0    1\n",
      " |      falcon   2.0  NaN\n",
      " |      horse    4.0  NaN\n",
      " |      spider   0.0  8.0\n",
      " |      ostrich  2.0  NaN\n",
      " |\n",
      " |  mul(self, other, axis: 'Axis' = 'columns', level=None, fill_value=None) -> 'DataFrame'\n",
      " |      Get Multiplication of dataframe and other, element-wise (binary operator `mul`).\n",
      " |\n",
      " |      Equivalent to ``dataframe * other``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `rmul`.\n",
      " |\n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `floordiv`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, dict or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns.\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |\n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |\n",
      " |      >>> df + 1\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      >>> df.add(1)\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      Divide by constant with reverse version.\n",
      " |\n",
      " |      >>> df.div(10)\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |\n",
      " |      >>> df.rdiv(10)\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |\n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |\n",
      " |      >>> df - [1, 2]\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub([1, 2], axis='columns')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |\n",
      " |      Multiply a dictionary by axis.\n",
      " |\n",
      " |      >>> df.mul({'angles': 0, 'degrees': 2})\n",
      " |                  angles  degrees\n",
      " |      circle           0      720\n",
      " |      triangle         0      360\n",
      " |      rectangle        0      720\n",
      " |\n",
      " |      >>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n",
      " |                  angles  degrees\n",
      " |      circle           0        0\n",
      " |      triangle         6      360\n",
      " |      rectangle       12     1080\n",
      " |\n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |\n",
      " |      >>> df * other\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |\n",
      " |      >>> df.mul(other, fill_value=0)\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |\n",
      " |      Divide by a MultiIndex by level.\n",
      " |\n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |\n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |\n",
      " |  multiply = mul(self, other, axis: 'Axis' = 'columns', level=None, fill_value=None) -> 'DataFrame'\n",
      " |\n",
      " |  ne(self, other, axis: 'Axis' = 'columns', level=None) -> 'DataFrame'\n",
      " |      Get Not equal to of dataframe and other, element-wise (binary operator `ne`).\n",
      " |\n",
      " |      Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n",
      " |      operators.\n",
      " |\n",
      " |      Equivalent to `==`, `!=`, `<=`, `<`, `>=`, `>` with support to choose axis\n",
      " |      (rows or columns) and level for comparison.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns').\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the passed\n",
      " |          MultiIndex level.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame of bool\n",
      " |          Result of the comparison.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.eq : Compare DataFrames for equality elementwise.\n",
      " |      DataFrame.ne : Compare DataFrames for inequality elementwise.\n",
      " |      DataFrame.le : Compare DataFrames for less than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.lt : Compare DataFrames for strictly less than\n",
      " |          inequality elementwise.\n",
      " |      DataFrame.ge : Compare DataFrames for greater than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.gt : Compare DataFrames for strictly greater than\n",
      " |          inequality elementwise.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      `NaN` values are considered different (i.e. `NaN` != `NaN`).\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'cost': [250, 150, 100],\n",
      " |      ...                    'revenue': [100, 250, 300]},\n",
      " |      ...                   index=['A', 'B', 'C'])\n",
      " |      >>> df\n",
      " |         cost  revenue\n",
      " |      A   250      100\n",
      " |      B   150      250\n",
      " |      C   100      300\n",
      " |\n",
      " |      Comparison with a scalar, using either the operator or method:\n",
      " |\n",
      " |      >>> df == 100\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |\n",
      " |      >>> df.eq(100)\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |\n",
      " |      When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n",
      " |      with the index of `other` and broadcast:\n",
      " |\n",
      " |      >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B   True    False\n",
      " |      C  False     True\n",
      " |\n",
      " |      Use the method to control the broadcast axis:\n",
      " |\n",
      " |      >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')\n",
      " |         cost  revenue\n",
      " |      A  True    False\n",
      " |      B  True     True\n",
      " |      C  True     True\n",
      " |      D  True     True\n",
      " |\n",
      " |      When comparing to an arbitrary sequence, the number of columns must\n",
      " |      match the number elements in `other`:\n",
      " |\n",
      " |      >>> df == [250, 100]\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B  False    False\n",
      " |      C  False    False\n",
      " |\n",
      " |      Use the method to control the axis:\n",
      " |\n",
      " |      >>> df.eq([250, 250, 100], axis='index')\n",
      " |          cost  revenue\n",
      " |      A   True    False\n",
      " |      B  False     True\n",
      " |      C   True    False\n",
      " |\n",
      " |      Compare to a DataFrame of different shape.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n",
      " |      ...                      index=['A', 'B', 'C', 'D'])\n",
      " |      >>> other\n",
      " |         revenue\n",
      " |      A      300\n",
      " |      B      250\n",
      " |      C      100\n",
      " |      D      150\n",
      " |\n",
      " |      >>> df.gt(other)\n",
      " |          cost  revenue\n",
      " |      A  False    False\n",
      " |      B  False    False\n",
      " |      C  False     True\n",
      " |      D  False    False\n",
      " |\n",
      " |      Compare to a MultiIndex by level.\n",
      " |\n",
      " |      >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n",
      " |      ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n",
      " |      ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n",
      " |      ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n",
      " |      >>> df_multindex\n",
      " |            cost  revenue\n",
      " |      Q1 A   250      100\n",
      " |         B   150      250\n",
      " |         C   100      300\n",
      " |      Q2 A   150      200\n",
      " |         B   300      175\n",
      " |         C   220      225\n",
      " |\n",
      " |      >>> df.le(df_multindex, level=1)\n",
      " |             cost  revenue\n",
      " |      Q1 A   True     True\n",
      " |         B   True     True\n",
      " |         C   True     True\n",
      " |      Q2 A  False     True\n",
      " |         B   True    False\n",
      " |         C   True    False\n",
      " |\n",
      " |  nlargest(self, n: 'int', columns: 'IndexLabel', keep: 'NsmallestNlargestKeep' = 'first') -> 'DataFrame'\n",
      " |      Return the first `n` rows ordered by `columns` in descending order.\n",
      " |\n",
      " |      Return the first `n` rows with the largest values in `columns`, in\n",
      " |      descending order. The columns that are not specified are returned as\n",
      " |      well, but not used for ordering.\n",
      " |\n",
      " |      This method is equivalent to\n",
      " |      ``df.sort_values(columns, ascending=False).head(n)``, but more\n",
      " |      performant.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int\n",
      " |          Number of rows to return.\n",
      " |      columns : label or list of labels\n",
      " |          Column label(s) to order by.\n",
      " |      keep : {'first', 'last', 'all'}, default 'first'\n",
      " |          Where there are duplicate values:\n",
      " |\n",
      " |          - ``first`` : prioritize the first occurrence(s)\n",
      " |          - ``last`` : prioritize the last occurrence(s)\n",
      " |          - ``all`` : keep all the ties of the smallest item even if it means\n",
      " |            selecting more than ``n`` items.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          The first `n` rows ordered by the given columns in descending\n",
      " |          order.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.nsmallest : Return the first `n` rows ordered by `columns` in\n",
      " |          ascending order.\n",
      " |      DataFrame.sort_values : Sort DataFrame by the values.\n",
      " |      DataFrame.head : Return the first `n` rows without re-ordering.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This function cannot be used with all column types. For example, when\n",
      " |      specifying columns with `object` or `category` dtypes, ``TypeError`` is\n",
      " |      raised.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'population': [59000000, 65000000, 434000,\n",
      " |      ...                                   434000, 434000, 337000, 11300,\n",
      " |      ...                                   11300, 11300],\n",
      " |      ...                    'GDP': [1937894, 2583560 , 12011, 4520, 12128,\n",
      " |      ...                            17036, 182, 38, 311],\n",
      " |      ...                    'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\",\n",
      " |      ...                                \"IS\", \"NR\", \"TV\", \"AI\"]},\n",
      " |      ...                   index=[\"Italy\", \"France\", \"Malta\",\n",
      " |      ...                          \"Maldives\", \"Brunei\", \"Iceland\",\n",
      " |      ...                          \"Nauru\", \"Tuvalu\", \"Anguilla\"])\n",
      " |      >>> df\n",
      " |                population      GDP alpha-2\n",
      " |      Italy       59000000  1937894      IT\n",
      " |      France      65000000  2583560      FR\n",
      " |      Malta         434000    12011      MT\n",
      " |      Maldives      434000     4520      MV\n",
      " |      Brunei        434000    12128      BN\n",
      " |      Iceland       337000    17036      IS\n",
      " |      Nauru          11300      182      NR\n",
      " |      Tuvalu         11300       38      TV\n",
      " |      Anguilla       11300      311      AI\n",
      " |\n",
      " |      In the following example, we will use ``nlargest`` to select the three\n",
      " |      rows having the largest values in column \"population\".\n",
      " |\n",
      " |      >>> df.nlargest(3, 'population')\n",
      " |              population      GDP alpha-2\n",
      " |      France    65000000  2583560      FR\n",
      " |      Italy     59000000  1937894      IT\n",
      " |      Malta       434000    12011      MT\n",
      " |\n",
      " |      When using ``keep='last'``, ties are resolved in reverse order:\n",
      " |\n",
      " |      >>> df.nlargest(3, 'population', keep='last')\n",
      " |              population      GDP alpha-2\n",
      " |      France    65000000  2583560      FR\n",
      " |      Italy     59000000  1937894      IT\n",
      " |      Brunei      434000    12128      BN\n",
      " |\n",
      " |      When using ``keep='all'``, the number of element kept can go beyond ``n``\n",
      " |      if there are duplicate values for the smallest element, all the\n",
      " |      ties are kept:\n",
      " |\n",
      " |      >>> df.nlargest(3, 'population', keep='all')\n",
      " |                population      GDP alpha-2\n",
      " |      France      65000000  2583560      FR\n",
      " |      Italy       59000000  1937894      IT\n",
      " |      Malta         434000    12011      MT\n",
      " |      Maldives      434000     4520      MV\n",
      " |      Brunei        434000    12128      BN\n",
      " |\n",
      " |      However, ``nlargest`` does not keep ``n`` distinct largest elements:\n",
      " |\n",
      " |      >>> df.nlargest(5, 'population', keep='all')\n",
      " |                population      GDP alpha-2\n",
      " |      France      65000000  2583560      FR\n",
      " |      Italy       59000000  1937894      IT\n",
      " |      Malta         434000    12011      MT\n",
      " |      Maldives      434000     4520      MV\n",
      " |      Brunei        434000    12128      BN\n",
      " |\n",
      " |      To order by the largest values in column \"population\" and then \"GDP\",\n",
      " |      we can specify multiple columns like in the next example.\n",
      " |\n",
      " |      >>> df.nlargest(3, ['population', 'GDP'])\n",
      " |              population      GDP alpha-2\n",
      " |      France    65000000  2583560      FR\n",
      " |      Italy     59000000  1937894      IT\n",
      " |      Brunei      434000    12128      BN\n",
      " |\n",
      " |  notna(self) -> 'DataFrame'\n",
      " |      Detect existing (non-missing) values.\n",
      " |\n",
      " |      Return a boolean same-sized object indicating if the values are not NA.\n",
      " |      Non-missing values get mapped to True. Characters such as empty\n",
      " |      strings ``''`` or :attr:`numpy.inf` are not considered NA values\n",
      " |      (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n",
      " |      NA values, such as None or :attr:`numpy.NaN`, get mapped to False\n",
      " |      values.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Mask of bool values for each element in DataFrame that\n",
      " |          indicates whether an element is not an NA value.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.notnull : Alias of notna.\n",
      " |      DataFrame.isna : Boolean inverse of notna.\n",
      " |      DataFrame.dropna : Omit axes labels with missing values.\n",
      " |      notna : Top-level notna.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Show which entries in a DataFrame are not NA.\n",
      " |\n",
      " |      >>> df = pd.DataFrame(dict(age=[5, 6, np.nan],\n",
      " |      ...                        born=[pd.NaT, pd.Timestamp('1939-05-27'),\n",
      " |      ...                              pd.Timestamp('1940-04-25')],\n",
      " |      ...                        name=['Alfred', 'Batman', ''],\n",
      " |      ...                        toy=[None, 'Batmobile', 'Joker']))\n",
      " |      >>> df\n",
      " |         age       born    name        toy\n",
      " |      0  5.0        NaT  Alfred       None\n",
      " |      1  6.0 1939-05-27  Batman  Batmobile\n",
      " |      2  NaN 1940-04-25              Joker\n",
      " |\n",
      " |      >>> df.notna()\n",
      " |           age   born  name    toy\n",
      " |      0   True  False  True  False\n",
      " |      1   True   True  True   True\n",
      " |      2  False   True  True   True\n",
      " |\n",
      " |      Show which entries in a Series are not NA.\n",
      " |\n",
      " |      >>> ser = pd.Series([5, 6, np.nan])\n",
      " |      >>> ser\n",
      " |      0    5.0\n",
      " |      1    6.0\n",
      " |      2    NaN\n",
      " |      dtype: float64\n",
      " |\n",
      " |      >>> ser.notna()\n",
      " |      0     True\n",
      " |      1     True\n",
      " |      2    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |  notnull(self) -> 'DataFrame'\n",
      " |      DataFrame.notnull is an alias for DataFrame.notna.\n",
      " |\n",
      " |      Detect existing (non-missing) values.\n",
      " |\n",
      " |      Return a boolean same-sized object indicating if the values are not NA.\n",
      " |      Non-missing values get mapped to True. Characters such as empty\n",
      " |      strings ``''`` or :attr:`numpy.inf` are not considered NA values\n",
      " |      (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n",
      " |      NA values, such as None or :attr:`numpy.NaN`, get mapped to False\n",
      " |      values.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Mask of bool values for each element in DataFrame that\n",
      " |          indicates whether an element is not an NA value.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.notnull : Alias of notna.\n",
      " |      DataFrame.isna : Boolean inverse of notna.\n",
      " |      DataFrame.dropna : Omit axes labels with missing values.\n",
      " |      notna : Top-level notna.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Show which entries in a DataFrame are not NA.\n",
      " |\n",
      " |      >>> df = pd.DataFrame(dict(age=[5, 6, np.nan],\n",
      " |      ...                        born=[pd.NaT, pd.Timestamp('1939-05-27'),\n",
      " |      ...                              pd.Timestamp('1940-04-25')],\n",
      " |      ...                        name=['Alfred', 'Batman', ''],\n",
      " |      ...                        toy=[None, 'Batmobile', 'Joker']))\n",
      " |      >>> df\n",
      " |         age       born    name        toy\n",
      " |      0  5.0        NaT  Alfred       None\n",
      " |      1  6.0 1939-05-27  Batman  Batmobile\n",
      " |      2  NaN 1940-04-25              Joker\n",
      " |\n",
      " |      >>> df.notna()\n",
      " |           age   born  name    toy\n",
      " |      0   True  False  True  False\n",
      " |      1   True   True  True   True\n",
      " |      2  False   True  True   True\n",
      " |\n",
      " |      Show which entries in a Series are not NA.\n",
      " |\n",
      " |      >>> ser = pd.Series([5, 6, np.nan])\n",
      " |      >>> ser\n",
      " |      0    5.0\n",
      " |      1    6.0\n",
      " |      2    NaN\n",
      " |      dtype: float64\n",
      " |\n",
      " |      >>> ser.notna()\n",
      " |      0     True\n",
      " |      1     True\n",
      " |      2    False\n",
      " |      dtype: bool\n",
      " |\n",
      " |  nsmallest(self, n: 'int', columns: 'IndexLabel', keep: 'NsmallestNlargestKeep' = 'first') -> 'DataFrame'\n",
      " |      Return the first `n` rows ordered by `columns` in ascending order.\n",
      " |\n",
      " |      Return the first `n` rows with the smallest values in `columns`, in\n",
      " |      ascending order. The columns that are not specified are returned as\n",
      " |      well, but not used for ordering.\n",
      " |\n",
      " |      This method is equivalent to\n",
      " |      ``df.sort_values(columns, ascending=True).head(n)``, but more\n",
      " |      performant.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int\n",
      " |          Number of items to retrieve.\n",
      " |      columns : list or str\n",
      " |          Column name or names to order by.\n",
      " |      keep : {'first', 'last', 'all'}, default 'first'\n",
      " |          Where there are duplicate values:\n",
      " |\n",
      " |          - ``first`` : take the first occurrence.\n",
      " |          - ``last`` : take the last occurrence.\n",
      " |          - ``all`` : keep all the ties of the largest item even if it means\n",
      " |            selecting more than ``n`` items.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.nlargest : Return the first `n` rows ordered by `columns` in\n",
      " |          descending order.\n",
      " |      DataFrame.sort_values : Sort DataFrame by the values.\n",
      " |      DataFrame.head : Return the first `n` rows without re-ordering.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'population': [59000000, 65000000, 434000,\n",
      " |      ...                                   434000, 434000, 337000, 337000,\n",
      " |      ...                                   11300, 11300],\n",
      " |      ...                    'GDP': [1937894, 2583560 , 12011, 4520, 12128,\n",
      " |      ...                            17036, 182, 38, 311],\n",
      " |      ...                    'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\",\n",
      " |      ...                                \"IS\", \"NR\", \"TV\", \"AI\"]},\n",
      " |      ...                   index=[\"Italy\", \"France\", \"Malta\",\n",
      " |      ...                          \"Maldives\", \"Brunei\", \"Iceland\",\n",
      " |      ...                          \"Nauru\", \"Tuvalu\", \"Anguilla\"])\n",
      " |      >>> df\n",
      " |                population      GDP alpha-2\n",
      " |      Italy       59000000  1937894      IT\n",
      " |      France      65000000  2583560      FR\n",
      " |      Malta         434000    12011      MT\n",
      " |      Maldives      434000     4520      MV\n",
      " |      Brunei        434000    12128      BN\n",
      " |      Iceland       337000    17036      IS\n",
      " |      Nauru         337000      182      NR\n",
      " |      Tuvalu         11300       38      TV\n",
      " |      Anguilla       11300      311      AI\n",
      " |\n",
      " |      In the following example, we will use ``nsmallest`` to select the\n",
      " |      three rows having the smallest values in column \"population\".\n",
      " |\n",
      " |      >>> df.nsmallest(3, 'population')\n",
      " |                population    GDP alpha-2\n",
      " |      Tuvalu         11300     38      TV\n",
      " |      Anguilla       11300    311      AI\n",
      " |      Iceland       337000  17036      IS\n",
      " |\n",
      " |      When using ``keep='last'``, ties are resolved in reverse order:\n",
      " |\n",
      " |      >>> df.nsmallest(3, 'population', keep='last')\n",
      " |                population  GDP alpha-2\n",
      " |      Anguilla       11300  311      AI\n",
      " |      Tuvalu         11300   38      TV\n",
      " |      Nauru         337000  182      NR\n",
      " |\n",
      " |      When using ``keep='all'``, the number of element kept can go beyond ``n``\n",
      " |      if there are duplicate values for the largest element, all the\n",
      " |      ties are kept.\n",
      " |\n",
      " |      >>> df.nsmallest(3, 'population', keep='all')\n",
      " |                population    GDP alpha-2\n",
      " |      Tuvalu         11300     38      TV\n",
      " |      Anguilla       11300    311      AI\n",
      " |      Iceland       337000  17036      IS\n",
      " |      Nauru         337000    182      NR\n",
      " |\n",
      " |      However, ``nsmallest`` does not keep ``n`` distinct\n",
      " |      smallest elements:\n",
      " |\n",
      " |      >>> df.nsmallest(4, 'population', keep='all')\n",
      " |                population    GDP alpha-2\n",
      " |      Tuvalu         11300     38      TV\n",
      " |      Anguilla       11300    311      AI\n",
      " |      Iceland       337000  17036      IS\n",
      " |      Nauru         337000    182      NR\n",
      " |\n",
      " |      To order by the smallest values in column \"population\" and then \"GDP\", we can\n",
      " |      specify multiple columns like in the next example.\n",
      " |\n",
      " |      >>> df.nsmallest(3, ['population', 'GDP'])\n",
      " |                population  GDP alpha-2\n",
      " |      Tuvalu         11300   38      TV\n",
      " |      Anguilla       11300  311      AI\n",
      " |      Nauru         337000  182      NR\n",
      " |\n",
      " |  nunique(self, axis: 'Axis' = 0, dropna: 'bool' = True) -> 'Series'\n",
      " |      Count number of distinct elements in specified axis.\n",
      " |\n",
      " |      Return Series with number of distinct elements. Can ignore NaN\n",
      " |      values.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for\n",
      " |          column-wise.\n",
      " |      dropna : bool, default True\n",
      " |          Don't include NaN in the counts.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.nunique: Method nunique for Series.\n",
      " |      DataFrame.count: Count non-NA cells for each column or row.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'A': [4, 5, 6], 'B': [4, 1, 1]})\n",
      " |      >>> df.nunique()\n",
      " |      A    3\n",
      " |      B    2\n",
      " |      dtype: int64\n",
      " |\n",
      " |      >>> df.nunique(axis=1)\n",
      " |      0    1\n",
      " |      1    2\n",
      " |      2    2\n",
      " |      dtype: int64\n",
      " |\n",
      " |  pivot(self, *, columns, index=<no_default>, values=<no_default>) -> 'DataFrame'\n",
      " |      Return reshaped DataFrame organized by given index / column values.\n",
      " |\n",
      " |      Reshape data (produce a \"pivot\" table) based on column values. Uses\n",
      " |      unique values from specified `index` / `columns` to form axes of the\n",
      " |      resulting DataFrame. This function does not support data\n",
      " |      aggregation, multiple values will result in a MultiIndex in the\n",
      " |      columns. See the :ref:`User Guide <reshaping>` for more on reshaping.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      columns : str or object or a list of str\n",
      " |          Column to use to make new frame's columns.\n",
      " |      index : str or object or a list of str, optional\n",
      " |          Column to use to make new frame's index. If not given, uses existing index.\n",
      " |      values : str, object or a list of the previous, optional\n",
      " |          Column(s) to use for populating new frame's values. If not\n",
      " |          specified, all remaining columns will be used and the result will\n",
      " |          have hierarchically indexed columns.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Returns reshaped DataFrame.\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      ValueError:\n",
      " |          When there are any `index`, `columns` combinations with multiple\n",
      " |          values. `DataFrame.pivot_table` when you need to aggregate.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.pivot_table : Generalization of pivot that can handle\n",
      " |          duplicate values for one index/column pair.\n",
      " |      DataFrame.unstack : Pivot based on the index values instead of a\n",
      " |          column.\n",
      " |      wide_to_long : Wide panel to long format. Less flexible but more\n",
      " |          user-friendly than melt.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      For finer-tuned control, see hierarchical indexing documentation along\n",
      " |      with the related stack/unstack methods.\n",
      " |\n",
      " |      Reference :ref:`the user guide <reshaping.pivot>` for more examples.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'foo': ['one', 'one', 'one', 'two', 'two',\n",
      " |      ...                            'two'],\n",
      " |      ...                    'bar': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
      " |      ...                    'baz': [1, 2, 3, 4, 5, 6],\n",
      " |      ...                    'zoo': ['x', 'y', 'z', 'q', 'w', 't']})\n",
      " |      >>> df\n",
      " |          foo   bar  baz  zoo\n",
      " |      0   one   A    1    x\n",
      " |      1   one   B    2    y\n",
      " |      2   one   C    3    z\n",
      " |      3   two   A    4    q\n",
      " |      4   two   B    5    w\n",
      " |      5   two   C    6    t\n",
      " |\n",
      " |      >>> df.pivot(index='foo', columns='bar', values='baz')\n",
      " |      bar  A   B   C\n",
      " |      foo\n",
      " |      one  1   2   3\n",
      " |      two  4   5   6\n",
      " |\n",
      " |      >>> df.pivot(index='foo', columns='bar')['baz']\n",
      " |      bar  A   B   C\n",
      " |      foo\n",
      " |      one  1   2   3\n",
      " |      two  4   5   6\n",
      " |\n",
      " |      >>> df.pivot(index='foo', columns='bar', values=['baz', 'zoo'])\n",
      " |            baz       zoo\n",
      " |      bar   A  B  C   A  B  C\n",
      " |      foo\n",
      " |      one   1  2  3   x  y  z\n",
      " |      two   4  5  6   q  w  t\n",
      " |\n",
      " |      You could also assign a list of column names or a list of index names.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({\n",
      " |      ...        \"lev1\": [1, 1, 1, 2, 2, 2],\n",
      " |      ...        \"lev2\": [1, 1, 2, 1, 1, 2],\n",
      " |      ...        \"lev3\": [1, 2, 1, 2, 1, 2],\n",
      " |      ...        \"lev4\": [1, 2, 3, 4, 5, 6],\n",
      " |      ...        \"values\": [0, 1, 2, 3, 4, 5]})\n",
      " |      >>> df\n",
      " |          lev1 lev2 lev3 lev4 values\n",
      " |      0   1    1    1    1    0\n",
      " |      1   1    1    2    2    1\n",
      " |      2   1    2    1    3    2\n",
      " |      3   2    1    2    4    3\n",
      " |      4   2    1    1    5    4\n",
      " |      5   2    2    2    6    5\n",
      " |\n",
      " |      >>> df.pivot(index=\"lev1\", columns=[\"lev2\", \"lev3\"], values=\"values\")\n",
      " |      lev2    1         2\n",
      " |      lev3    1    2    1    2\n",
      " |      lev1\n",
      " |      1     0.0  1.0  2.0  NaN\n",
      " |      2     4.0  3.0  NaN  5.0\n",
      " |\n",
      " |      >>> df.pivot(index=[\"lev1\", \"lev2\"], columns=[\"lev3\"], values=\"values\")\n",
      " |            lev3    1    2\n",
      " |      lev1  lev2\n",
      " |         1     1  0.0  1.0\n",
      " |               2  2.0  NaN\n",
      " |         2     1  4.0  3.0\n",
      " |               2  NaN  5.0\n",
      " |\n",
      " |      A ValueError is raised if there are any duplicates.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({\"foo\": ['one', 'one', 'two', 'two'],\n",
      " |      ...                    \"bar\": ['A', 'A', 'B', 'C'],\n",
      " |      ...                    \"baz\": [1, 2, 3, 4]})\n",
      " |      >>> df\n",
      " |         foo bar  baz\n",
      " |      0  one   A    1\n",
      " |      1  one   A    2\n",
      " |      2  two   B    3\n",
      " |      3  two   C    4\n",
      " |\n",
      " |      Notice that the first two rows are the same for our `index`\n",
      " |      and `columns` arguments.\n",
      " |\n",
      " |      >>> df.pivot(index='foo', columns='bar', values='baz')\n",
      " |      Traceback (most recent call last):\n",
      " |         ...\n",
      " |      ValueError: Index contains duplicate entries, cannot reshape\n",
      " |\n",
      " |  pivot_table(self, values=None, index=None, columns=None, aggfunc: 'AggFuncType' = 'mean', fill_value=None, margins: 'bool' = False, dropna: 'bool' = True, margins_name: 'Level' = 'All', observed: 'bool | lib.NoDefault' = <no_default>, sort: 'bool' = True) -> 'DataFrame'\n",
      " |      Create a spreadsheet-style pivot table as a DataFrame.\n",
      " |\n",
      " |      The levels in the pivot table will be stored in MultiIndex objects\n",
      " |      (hierarchical indexes) on the index and columns of the result DataFrame.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      values : list-like or scalar, optional\n",
      " |          Column or columns to aggregate.\n",
      " |      index : column, Grouper, array, or list of the previous\n",
      " |          Keys to group by on the pivot table index. If a list is passed,\n",
      " |          it can contain any of the other types (except list). If an array is\n",
      " |          passed, it must be the same length as the data and will be used in\n",
      " |          the same manner as column values.\n",
      " |      columns : column, Grouper, array, or list of the previous\n",
      " |          Keys to group by on the pivot table column. If a list is passed,\n",
      " |          it can contain any of the other types (except list). If an array is\n",
      " |          passed, it must be the same length as the data and will be used in\n",
      " |          the same manner as column values.\n",
      " |      aggfunc : function, list of functions, dict, default \"mean\"\n",
      " |          If a list of functions is passed, the resulting pivot table will have\n",
      " |          hierarchical columns whose top level are the function names\n",
      " |          (inferred from the function objects themselves).\n",
      " |          If a dict is passed, the key is column to aggregate and the value is\n",
      " |          function or list of functions. If ``margin=True``, aggfunc will be\n",
      " |          used to calculate the partial aggregates.\n",
      " |      fill_value : scalar, default None\n",
      " |          Value to replace missing values with (in the resulting pivot table,\n",
      " |          after aggregation).\n",
      " |      margins : bool, default False\n",
      " |          If ``margins=True``, special ``All`` columns and rows\n",
      " |          will be added with partial group aggregates across the categories\n",
      " |          on the rows and columns.\n",
      " |      dropna : bool, default True\n",
      " |          Do not include columns whose entries are all NaN. If True,\n",
      " |          rows with a NaN value in any column will be omitted before\n",
      " |          computing margins.\n",
      " |      margins_name : str, default 'All'\n",
      " |          Name of the row / column that will contain the totals\n",
      " |          when margins is True.\n",
      " |      observed : bool, default False\n",
      " |          This only applies if any of the groupers are Categoricals.\n",
      " |          If True: only show observed values for categorical groupers.\n",
      " |          If False: show all values for categorical groupers.\n",
      " |\n",
      " |          .. deprecated:: 2.2.0\n",
      " |\n",
      " |              The default value of ``False`` is deprecated and will change to\n",
      " |              ``True`` in a future version of pandas.\n",
      " |\n",
      " |      sort : bool, default True\n",
      " |          Specifies if the result should be sorted.\n",
      " |\n",
      " |          .. versionadded:: 1.3.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          An Excel style pivot table.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.pivot : Pivot without aggregation that can handle\n",
      " |          non-numeric data.\n",
      " |      DataFrame.melt: Unpivot a DataFrame from wide to long format,\n",
      " |          optionally leaving identifiers set.\n",
      " |      wide_to_long : Wide panel to long format. Less flexible but more\n",
      " |          user-friendly than melt.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Reference :ref:`the user guide <reshaping.pivot>` for more examples.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({\"A\": [\"foo\", \"foo\", \"foo\", \"foo\", \"foo\",\n",
      " |      ...                          \"bar\", \"bar\", \"bar\", \"bar\"],\n",
      " |      ...                    \"B\": [\"one\", \"one\", \"one\", \"two\", \"two\",\n",
      " |      ...                          \"one\", \"one\", \"two\", \"two\"],\n",
      " |      ...                    \"C\": [\"small\", \"large\", \"large\", \"small\",\n",
      " |      ...                          \"small\", \"large\", \"small\", \"small\",\n",
      " |      ...                          \"large\"],\n",
      " |      ...                    \"D\": [1, 2, 2, 3, 3, 4, 5, 6, 7],\n",
      " |      ...                    \"E\": [2, 4, 5, 5, 6, 6, 8, 9, 9]})\n",
      " |      >>> df\n",
      " |           A    B      C  D  E\n",
      " |      0  foo  one  small  1  2\n",
      " |      1  foo  one  large  2  4\n",
      " |      2  foo  one  large  2  5\n",
      " |      3  foo  two  small  3  5\n",
      " |      4  foo  two  small  3  6\n",
      " |      5  bar  one  large  4  6\n",
      " |      6  bar  one  small  5  8\n",
      " |      7  bar  two  small  6  9\n",
      " |      8  bar  two  large  7  9\n",
      " |\n",
      " |      This first example aggregates values by taking the sum.\n",
      " |\n",
      " |      >>> table = pd.pivot_table(df, values='D', index=['A', 'B'],\n",
      " |      ...                        columns=['C'], aggfunc=\"sum\")\n",
      " |      >>> table\n",
      " |      C        large  small\n",
      " |      A   B\n",
      " |      bar one    4.0    5.0\n",
      " |          two    7.0    6.0\n",
      " |      foo one    4.0    1.0\n",
      " |          two    NaN    6.0\n",
      " |\n",
      " |      We can also fill missing values using the `fill_value` parameter.\n",
      " |\n",
      " |      >>> table = pd.pivot_table(df, values='D', index=['A', 'B'],\n",
      " |      ...                        columns=['C'], aggfunc=\"sum\", fill_value=0)\n",
      " |      >>> table\n",
      " |      C        large  small\n",
      " |      A   B\n",
      " |      bar one      4      5\n",
      " |          two      7      6\n",
      " |      foo one      4      1\n",
      " |          two      0      6\n",
      " |\n",
      " |      The next example aggregates by taking the mean across multiple columns.\n",
      " |\n",
      " |      >>> table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],\n",
      " |      ...                        aggfunc={'D': \"mean\", 'E': \"mean\"})\n",
      " |      >>> table\n",
      " |                      D         E\n",
      " |      A   C\n",
      " |      bar large  5.500000  7.500000\n",
      " |          small  5.500000  8.500000\n",
      " |      foo large  2.000000  4.500000\n",
      " |          small  2.333333  4.333333\n",
      " |\n",
      " |      We can also calculate multiple types of aggregations for any given\n",
      " |      value column.\n",
      " |\n",
      " |      >>> table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],\n",
      " |      ...                        aggfunc={'D': \"mean\",\n",
      " |      ...                                 'E': [\"min\", \"max\", \"mean\"]})\n",
      " |      >>> table\n",
      " |                        D   E\n",
      " |                     mean max      mean  min\n",
      " |      A   C\n",
      " |      bar large  5.500000   9  7.500000    6\n",
      " |          small  5.500000   9  8.500000    8\n",
      " |      foo large  2.000000   5  4.500000    4\n",
      " |          small  2.333333   6  4.333333    2\n",
      " |\n",
      " |  pop(self, item: 'Hashable') -> 'Series'\n",
      " |      Return item and drop from frame. Raise KeyError if not found.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      item : label\n",
      " |          Label of column to be popped.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([('falcon', 'bird', 389.0),\n",
      " |      ...                    ('parrot', 'bird', 24.0),\n",
      " |      ...                    ('lion', 'mammal', 80.5),\n",
      " |      ...                    ('monkey', 'mammal', np.nan)],\n",
      " |      ...                   columns=('name', 'class', 'max_speed'))\n",
      " |      >>> df\n",
      " |           name   class  max_speed\n",
      " |      0  falcon    bird      389.0\n",
      " |      1  parrot    bird       24.0\n",
      " |      2    lion  mammal       80.5\n",
      " |      3  monkey  mammal        NaN\n",
      " |\n",
      " |      >>> df.pop('class')\n",
      " |      0      bird\n",
      " |      1      bird\n",
      " |      2    mammal\n",
      " |      3    mammal\n",
      " |      Name: class, dtype: object\n",
      " |\n",
      " |      >>> df\n",
      " |           name  max_speed\n",
      " |      0  falcon      389.0\n",
      " |      1  parrot       24.0\n",
      " |      2    lion       80.5\n",
      " |      3  monkey        NaN\n",
      " |\n",
      " |  pow(self, other, axis: 'Axis' = 'columns', level=None, fill_value=None) -> 'DataFrame'\n",
      " |      Get Exponential power of dataframe and other, element-wise (binary operator `pow`).\n",
      " |\n",
      " |      Equivalent to ``dataframe ** other``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `rpow`.\n",
      " |\n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `floordiv`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, dict or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns.\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |\n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |\n",
      " |      >>> df + 1\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      >>> df.add(1)\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      Divide by constant with reverse version.\n",
      " |\n",
      " |      >>> df.div(10)\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |\n",
      " |      >>> df.rdiv(10)\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |\n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |\n",
      " |      >>> df - [1, 2]\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub([1, 2], axis='columns')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |\n",
      " |      Multiply a dictionary by axis.\n",
      " |\n",
      " |      >>> df.mul({'angles': 0, 'degrees': 2})\n",
      " |                  angles  degrees\n",
      " |      circle           0      720\n",
      " |      triangle         0      360\n",
      " |      rectangle        0      720\n",
      " |\n",
      " |      >>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n",
      " |                  angles  degrees\n",
      " |      circle           0        0\n",
      " |      triangle         6      360\n",
      " |      rectangle       12     1080\n",
      " |\n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |\n",
      " |      >>> df * other\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |\n",
      " |      >>> df.mul(other, fill_value=0)\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |\n",
      " |      Divide by a MultiIndex by level.\n",
      " |\n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |\n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |\n",
      " |  prod(self, axis: 'Axis | None' = 0, skipna: 'bool' = True, numeric_only: 'bool' = False, min_count: 'int' = 0, **kwargs)\n",
      " |      Return the product of the values over the requested axis.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {index (0), columns (1)}\n",
      " |          Axis for the function to be applied on.\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |\n",
      " |          .. warning::\n",
      " |\n",
      " |              The behavior of DataFrame.prod with ``axis=None`` is deprecated,\n",
      " |              in a future version this will reduce over both axes and return a scalar\n",
      " |              To retain the old behavior, pass axis=0 (or do not pass axis).\n",
      " |\n",
      " |          .. versionadded:: 2.0.0\n",
      " |\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values when computing the result.\n",
      " |      numeric_only : bool, default False\n",
      " |          Include only float, int, boolean columns. Not implemented for Series.\n",
      " |\n",
      " |      min_count : int, default 0\n",
      " |          The required number of valid values to perform the operation. If fewer than\n",
      " |          ``min_count`` non-NA values are present the result will be NA.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to be passed to the function.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or scalar\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.sum : Return the sum.\n",
      " |      Series.min : Return the minimum.\n",
      " |      Series.max : Return the maximum.\n",
      " |      Series.idxmin : Return the index of the minimum.\n",
      " |      Series.idxmax : Return the index of the maximum.\n",
      " |      DataFrame.sum : Return the sum over the requested axis.\n",
      " |      DataFrame.min : Return the minimum over the requested axis.\n",
      " |      DataFrame.max : Return the maximum over the requested axis.\n",
      " |      DataFrame.idxmin : Return the index of the minimum over the requested axis.\n",
      " |      DataFrame.idxmax : Return the index of the maximum over the requested axis.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      By default, the product of an empty or all-NA Series is ``1``\n",
      " |\n",
      " |      >>> pd.Series([], dtype=\"float64\").prod()\n",
      " |      1.0\n",
      " |\n",
      " |      This can be controlled with the ``min_count`` parameter\n",
      " |\n",
      " |      >>> pd.Series([], dtype=\"float64\").prod(min_count=1)\n",
      " |      nan\n",
      " |\n",
      " |      Thanks to the ``skipna`` parameter, ``min_count`` handles all-NA and\n",
      " |      empty series identically.\n",
      " |\n",
      " |      >>> pd.Series([np.nan]).prod()\n",
      " |      1.0\n",
      " |\n",
      " |      >>> pd.Series([np.nan]).prod(min_count=1)\n",
      " |      nan\n",
      " |\n",
      " |  product = prod(self, axis: 'Axis | None' = 0, skipna: 'bool' = True, numeric_only: 'bool' = False, min_count: 'int' = 0, **kwargs)\n",
      " |\n",
      " |  quantile(self, q: 'float | AnyArrayLike | Sequence[float]' = 0.5, axis: 'Axis' = 0, numeric_only: 'bool' = False, interpolation: 'QuantileInterpolation' = 'linear', method: \"Literal['single', 'table']\" = 'single') -> 'Series | DataFrame'\n",
      " |      Return values at the given quantile over requested axis.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      q : float or array-like, default 0.5 (50% quantile)\n",
      " |          Value between 0 <= q <= 1, the quantile(s) to compute.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          Equals 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\n",
      " |      numeric_only : bool, default False\n",
      " |          Include only `float`, `int` or `boolean` data.\n",
      " |\n",
      " |          .. versionchanged:: 2.0.0\n",
      " |              The default value of ``numeric_only`` is now ``False``.\n",
      " |\n",
      " |      interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n",
      " |          This optional parameter specifies the interpolation method to use,\n",
      " |          when the desired quantile lies between two data points `i` and `j`:\n",
      " |\n",
      " |          * linear: `i + (j - i) * fraction`, where `fraction` is the\n",
      " |            fractional part of the index surrounded by `i` and `j`.\n",
      " |          * lower: `i`.\n",
      " |          * higher: `j`.\n",
      " |          * nearest: `i` or `j` whichever is nearest.\n",
      " |          * midpoint: (`i` + `j`) / 2.\n",
      " |      method : {'single', 'table'}, default 'single'\n",
      " |          Whether to compute quantiles per-column ('single') or over all columns\n",
      " |          ('table'). When 'table', the only allowed interpolation methods are\n",
      " |          'nearest', 'lower', and 'higher'.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |\n",
      " |          If ``q`` is an array, a DataFrame will be returned where the\n",
      " |            index is ``q``, the columns are the columns of self, and the\n",
      " |            values are the quantiles.\n",
      " |          If ``q`` is a float, a Series will be returned where the\n",
      " |            index is the columns of self and the values are the quantiles.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      core.window.rolling.Rolling.quantile: Rolling quantile.\n",
      " |      numpy.percentile: Numpy function to compute the percentile.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame(np.array([[1, 1], [2, 10], [3, 100], [4, 100]]),\n",
      " |      ...                   columns=['a', 'b'])\n",
      " |      >>> df.quantile(.1)\n",
      " |      a    1.3\n",
      " |      b    3.7\n",
      " |      Name: 0.1, dtype: float64\n",
      " |      >>> df.quantile([.1, .5])\n",
      " |             a     b\n",
      " |      0.1  1.3   3.7\n",
      " |      0.5  2.5  55.0\n",
      " |\n",
      " |      Specifying `method='table'` will compute the quantile over all columns.\n",
      " |\n",
      " |      >>> df.quantile(.1, method=\"table\", interpolation=\"nearest\")\n",
      " |      a    1\n",
      " |      b    1\n",
      " |      Name: 0.1, dtype: int64\n",
      " |      >>> df.quantile([.1, .5], method=\"table\", interpolation=\"nearest\")\n",
      " |           a    b\n",
      " |      0.1  1    1\n",
      " |      0.5  3  100\n",
      " |\n",
      " |      Specifying `numeric_only=False` will also compute the quantile of\n",
      " |      datetime and timedelta data.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'A': [1, 2],\n",
      " |      ...                    'B': [pd.Timestamp('2010'),\n",
      " |      ...                          pd.Timestamp('2011')],\n",
      " |      ...                    'C': [pd.Timedelta('1 days'),\n",
      " |      ...                          pd.Timedelta('2 days')]})\n",
      " |      >>> df.quantile(0.5, numeric_only=False)\n",
      " |      A                    1.5\n",
      " |      B    2010-07-02 12:00:00\n",
      " |      C        1 days 12:00:00\n",
      " |      Name: 0.5, dtype: object\n",
      " |\n",
      " |  query(self, expr: 'str', *, inplace: 'bool' = False, **kwargs) -> 'DataFrame | None'\n",
      " |      Query the columns of a DataFrame with a boolean expression.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      expr : str\n",
      " |          The query string to evaluate.\n",
      " |\n",
      " |          You can refer to variables\n",
      " |          in the environment by prefixing them with an '@' character like\n",
      " |          ``@a + b``.\n",
      " |\n",
      " |          You can refer to column names that are not valid Python variable names\n",
      " |          by surrounding them in backticks. Thus, column names containing spaces\n",
      " |          or punctuations (besides underscores) or starting with digits must be\n",
      " |          surrounded by backticks. (For example, a column named \"Area (cm^2)\" would\n",
      " |          be referenced as ```Area (cm^2)```). Column names which are Python keywords\n",
      " |          (like \"list\", \"for\", \"import\", etc) cannot be used.\n",
      " |\n",
      " |          For example, if one of your columns is called ``a a`` and you want\n",
      " |          to sum it with ``b``, your query should be ```a a` + b``.\n",
      " |\n",
      " |      inplace : bool\n",
      " |          Whether to modify the DataFrame rather than creating a new one.\n",
      " |      **kwargs\n",
      " |          See the documentation for :func:`eval` for complete details\n",
      " |          on the keyword arguments accepted by :meth:`DataFrame.query`.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame or None\n",
      " |          DataFrame resulting from the provided query expression or\n",
      " |          None if ``inplace=True``.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      eval : Evaluate a string describing operations on\n",
      " |          DataFrame columns.\n",
      " |      DataFrame.eval : Evaluate a string describing operations on\n",
      " |          DataFrame columns.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      The result of the evaluation of this expression is first passed to\n",
      " |      :attr:`DataFrame.loc` and if that fails because of a\n",
      " |      multidimensional key (e.g., a DataFrame) then the result will be passed\n",
      " |      to :meth:`DataFrame.__getitem__`.\n",
      " |\n",
      " |      This method uses the top-level :func:`eval` function to\n",
      " |      evaluate the passed query.\n",
      " |\n",
      " |      The :meth:`~pandas.DataFrame.query` method uses a slightly\n",
      " |      modified Python syntax by default. For example, the ``&`` and ``|``\n",
      " |      (bitwise) operators have the precedence of their boolean cousins,\n",
      " |      :keyword:`and` and :keyword:`or`. This *is* syntactically valid Python,\n",
      " |      however the semantics are different.\n",
      " |\n",
      " |      You can change the semantics of the expression by passing the keyword\n",
      " |      argument ``parser='python'``. This enforces the same semantics as\n",
      " |      evaluation in Python space. Likewise, you can pass ``engine='python'``\n",
      " |      to evaluate an expression using Python itself as a backend. This is not\n",
      " |      recommended as it is inefficient compared to using ``numexpr`` as the\n",
      " |      engine.\n",
      " |\n",
      " |      The :attr:`DataFrame.index` and\n",
      " |      :attr:`DataFrame.columns` attributes of the\n",
      " |      :class:`~pandas.DataFrame` instance are placed in the query namespace\n",
      " |      by default, which allows you to treat both the index and columns of the\n",
      " |      frame as a column in the frame.\n",
      " |      The identifier ``index`` is used for the frame index; you can also\n",
      " |      use the name of the index to identify it in a query. Please note that\n",
      " |      Python keywords may not be used as identifiers.\n",
      " |\n",
      " |      For further details and examples see the ``query`` documentation in\n",
      " |      :ref:`indexing <indexing.query>`.\n",
      " |\n",
      " |      *Backtick quoted variables*\n",
      " |\n",
      " |      Backtick quoted variables are parsed as literal Python code and\n",
      " |      are converted internally to a Python valid identifier.\n",
      " |      This can lead to the following problems.\n",
      " |\n",
      " |      During parsing a number of disallowed characters inside the backtick\n",
      " |      quoted string are replaced by strings that are allowed as a Python identifier.\n",
      " |      These characters include all operators in Python, the space character, the\n",
      " |      question mark, the exclamation mark, the dollar sign, and the euro sign.\n",
      " |      For other characters that fall outside the ASCII range (U+0001..U+007F)\n",
      " |      and those that are not further specified in PEP 3131,\n",
      " |      the query parser will raise an error.\n",
      " |      This excludes whitespace different than the space character,\n",
      " |      but also the hashtag (as it is used for comments) and the backtick\n",
      " |      itself (backtick can also not be escaped).\n",
      " |\n",
      " |      In a special case, quotes that make a pair around a backtick can\n",
      " |      confuse the parser.\n",
      " |      For example, ```it's` > `that's``` will raise an error,\n",
      " |      as it forms a quoted string (``'s > `that'``) with a backtick inside.\n",
      " |\n",
      " |      See also the Python documentation about lexical analysis\n",
      " |      (https://docs.python.org/3/reference/lexical_analysis.html)\n",
      " |      in combination with the source code in :mod:`pandas.core.computation.parsing`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'A': range(1, 6),\n",
      " |      ...                    'B': range(10, 0, -2),\n",
      " |      ...                    'C C': range(10, 5, -1)})\n",
      " |      >>> df\n",
      " |         A   B  C C\n",
      " |      0  1  10   10\n",
      " |      1  2   8    9\n",
      " |      2  3   6    8\n",
      " |      3  4   4    7\n",
      " |      4  5   2    6\n",
      " |      >>> df.query('A > B')\n",
      " |         A  B  C C\n",
      " |      4  5  2    6\n",
      " |\n",
      " |      The previous expression is equivalent to\n",
      " |\n",
      " |      >>> df[df.A > df.B]\n",
      " |         A  B  C C\n",
      " |      4  5  2    6\n",
      " |\n",
      " |      For columns with spaces in their name, you can use backtick quoting.\n",
      " |\n",
      " |      >>> df.query('B == `C C`')\n",
      " |         A   B  C C\n",
      " |      0  1  10   10\n",
      " |\n",
      " |      The previous expression is equivalent to\n",
      " |\n",
      " |      >>> df[df.B == df['C C']]\n",
      " |         A   B  C C\n",
      " |      0  1  10   10\n",
      " |\n",
      " |  radd(self, other, axis: 'Axis' = 'columns', level=None, fill_value=None) -> 'DataFrame'\n",
      " |      Get Addition of dataframe and other, element-wise (binary operator `radd`).\n",
      " |\n",
      " |      Equivalent to ``other + dataframe``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `add`.\n",
      " |\n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `floordiv`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, dict or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns.\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |\n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |\n",
      " |      >>> df + 1\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      >>> df.add(1)\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      Divide by constant with reverse version.\n",
      " |\n",
      " |      >>> df.div(10)\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |\n",
      " |      >>> df.rdiv(10)\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |\n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |\n",
      " |      >>> df - [1, 2]\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub([1, 2], axis='columns')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |\n",
      " |      Multiply a dictionary by axis.\n",
      " |\n",
      " |      >>> df.mul({'angles': 0, 'degrees': 2})\n",
      " |                  angles  degrees\n",
      " |      circle           0      720\n",
      " |      triangle         0      360\n",
      " |      rectangle        0      720\n",
      " |\n",
      " |      >>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n",
      " |                  angles  degrees\n",
      " |      circle           0        0\n",
      " |      triangle         6      360\n",
      " |      rectangle       12     1080\n",
      " |\n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |\n",
      " |      >>> df * other\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |\n",
      " |      >>> df.mul(other, fill_value=0)\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |\n",
      " |      Divide by a MultiIndex by level.\n",
      " |\n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |\n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |\n",
      " |  rdiv = rtruediv(self, other, axis: 'Axis' = 'columns', level=None, fill_value=None) -> 'DataFrame'\n",
      " |\n",
      " |  reindex(self, labels=None, *, index=None, columns=None, axis: 'Axis | None' = None, method: 'ReindexMethod | None' = None, copy: 'bool | None' = None, level: 'Level | None' = None, fill_value: 'Scalar | None' = nan, limit: 'int | None' = None, tolerance=None) -> 'DataFrame'\n",
      " |      Conform DataFrame to new index with optional filling logic.\n",
      " |\n",
      " |      Places NA/NaN in locations having no value in the previous index. A new object\n",
      " |      is produced unless the new index is equivalent to the current one and\n",
      " |      ``copy=False``.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |\n",
      " |      labels : array-like, optional\n",
      " |          New labels / index to conform the axis specified by 'axis' to.\n",
      " |      index : array-like, optional\n",
      " |          New labels for the index. Preferably an Index object to avoid\n",
      " |          duplicating data.\n",
      " |      columns : array-like, optional\n",
      " |          New labels for the columns. Preferably an Index object to avoid\n",
      " |          duplicating data.\n",
      " |      axis : int or str, optional\n",
      " |          Axis to target. Can be either the axis name ('index', 'columns')\n",
      " |          or number (0, 1).\n",
      " |      method : {None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'}\n",
      " |          Method to use for filling holes in reindexed DataFrame.\n",
      " |          Please note: this is only applicable to DataFrames/Series with a\n",
      " |          monotonically increasing/decreasing index.\n",
      " |\n",
      " |          * None (default): don't fill gaps\n",
      " |          * pad / ffill: Propagate last valid observation forward to next\n",
      " |            valid.\n",
      " |          * backfill / bfill: Use next valid observation to fill gap.\n",
      " |          * nearest: Use nearest valid observations to fill gap.\n",
      " |\n",
      " |      copy : bool, default True\n",
      " |          Return a new object, even if the passed indexes are the same.\n",
      " |\n",
      " |          .. note::\n",
      " |              The `copy` keyword will change behavior in pandas 3.0.\n",
      " |              `Copy-on-Write\n",
      " |              <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n",
      " |              will be enabled by default, which means that all methods with a\n",
      " |              `copy` keyword will use a lazy copy mechanism to defer the copy and\n",
      " |              ignore the `copy` keyword. The `copy` keyword will be removed in a\n",
      " |              future version of pandas.\n",
      " |\n",
      " |              You can already get the future behavior and improvements through\n",
      " |              enabling copy on write ``pd.options.mode.copy_on_write = True``\n",
      " |      level : int or name\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : scalar, default np.nan\n",
      " |          Value to use for missing values. Defaults to NaN, but can be any\n",
      " |          \"compatible\" value.\n",
      " |      limit : int, default None\n",
      " |          Maximum number of consecutive elements to forward or backward fill.\n",
      " |      tolerance : optional\n",
      " |          Maximum distance between original and new labels for inexact\n",
      " |          matches. The values of the index at the matching locations most\n",
      " |          satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n",
      " |\n",
      " |          Tolerance may be a scalar value, which applies the same tolerance\n",
      " |          to all values, or list-like, which applies variable tolerance per\n",
      " |          element. List-like includes list, tuple, array, Series, and must be\n",
      " |          the same size as the index and its dtype must exactly match the\n",
      " |          index's type.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame with changed index.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.set_index : Set row labels.\n",
      " |      DataFrame.reset_index : Remove row labels or move them to new columns.\n",
      " |      DataFrame.reindex_like : Change to same indices as other DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      ``DataFrame.reindex`` supports two calling conventions\n",
      " |\n",
      " |      * ``(index=index_labels, columns=column_labels, ...)``\n",
      " |      * ``(labels, axis={'index', 'columns'}, ...)``\n",
      " |\n",
      " |      We *highly* recommend using keyword arguments to clarify your\n",
      " |      intent.\n",
      " |\n",
      " |      Create a dataframe with some fictional data.\n",
      " |\n",
      " |      >>> index = ['Firefox', 'Chrome', 'Safari', 'IE10', 'Konqueror']\n",
      " |      >>> df = pd.DataFrame({'http_status': [200, 200, 404, 404, 301],\n",
      " |      ...                   'response_time': [0.04, 0.02, 0.07, 0.08, 1.0]},\n",
      " |      ...                   index=index)\n",
      " |      >>> df\n",
      " |                 http_status  response_time\n",
      " |      Firefox            200           0.04\n",
      " |      Chrome             200           0.02\n",
      " |      Safari             404           0.07\n",
      " |      IE10               404           0.08\n",
      " |      Konqueror          301           1.00\n",
      " |\n",
      " |      Create a new index and reindex the dataframe. By default\n",
      " |      values in the new index that do not have corresponding\n",
      " |      records in the dataframe are assigned ``NaN``.\n",
      " |\n",
      " |      >>> new_index = ['Safari', 'Iceweasel', 'Comodo Dragon', 'IE10',\n",
      " |      ...              'Chrome']\n",
      " |      >>> df.reindex(new_index)\n",
      " |                     http_status  response_time\n",
      " |      Safari               404.0           0.07\n",
      " |      Iceweasel              NaN            NaN\n",
      " |      Comodo Dragon          NaN            NaN\n",
      " |      IE10                 404.0           0.08\n",
      " |      Chrome               200.0           0.02\n",
      " |\n",
      " |      We can fill in the missing values by passing a value to\n",
      " |      the keyword ``fill_value``. Because the index is not monotonically\n",
      " |      increasing or decreasing, we cannot use arguments to the keyword\n",
      " |      ``method`` to fill the ``NaN`` values.\n",
      " |\n",
      " |      >>> df.reindex(new_index, fill_value=0)\n",
      " |                     http_status  response_time\n",
      " |      Safari                 404           0.07\n",
      " |      Iceweasel                0           0.00\n",
      " |      Comodo Dragon            0           0.00\n",
      " |      IE10                   404           0.08\n",
      " |      Chrome                 200           0.02\n",
      " |\n",
      " |      >>> df.reindex(new_index, fill_value='missing')\n",
      " |                    http_status response_time\n",
      " |      Safari                404          0.07\n",
      " |      Iceweasel         missing       missing\n",
      " |      Comodo Dragon     missing       missing\n",
      " |      IE10                  404          0.08\n",
      " |      Chrome                200          0.02\n",
      " |\n",
      " |      We can also reindex the columns.\n",
      " |\n",
      " |      >>> df.reindex(columns=['http_status', 'user_agent'])\n",
      " |                 http_status  user_agent\n",
      " |      Firefox            200         NaN\n",
      " |      Chrome             200         NaN\n",
      " |      Safari             404         NaN\n",
      " |      IE10               404         NaN\n",
      " |      Konqueror          301         NaN\n",
      " |\n",
      " |      Or we can use \"axis-style\" keyword arguments\n",
      " |\n",
      " |      >>> df.reindex(['http_status', 'user_agent'], axis=\"columns\")\n",
      " |                 http_status  user_agent\n",
      " |      Firefox            200         NaN\n",
      " |      Chrome             200         NaN\n",
      " |      Safari             404         NaN\n",
      " |      IE10               404         NaN\n",
      " |      Konqueror          301         NaN\n",
      " |\n",
      " |      To further illustrate the filling functionality in\n",
      " |      ``reindex``, we will create a dataframe with a\n",
      " |      monotonically increasing index (for example, a sequence\n",
      " |      of dates).\n",
      " |\n",
      " |      >>> date_index = pd.date_range('1/1/2010', periods=6, freq='D')\n",
      " |      >>> df2 = pd.DataFrame({\"prices\": [100, 101, np.nan, 100, 89, 88]},\n",
      " |      ...                    index=date_index)\n",
      " |      >>> df2\n",
      " |                  prices\n",
      " |      2010-01-01   100.0\n",
      " |      2010-01-02   101.0\n",
      " |      2010-01-03     NaN\n",
      " |      2010-01-04   100.0\n",
      " |      2010-01-05    89.0\n",
      " |      2010-01-06    88.0\n",
      " |\n",
      " |      Suppose we decide to expand the dataframe to cover a wider\n",
      " |      date range.\n",
      " |\n",
      " |      >>> date_index2 = pd.date_range('12/29/2009', periods=10, freq='D')\n",
      " |      >>> df2.reindex(date_index2)\n",
      " |                  prices\n",
      " |      2009-12-29     NaN\n",
      " |      2009-12-30     NaN\n",
      " |      2009-12-31     NaN\n",
      " |      2010-01-01   100.0\n",
      " |      2010-01-02   101.0\n",
      " |      2010-01-03     NaN\n",
      " |      2010-01-04   100.0\n",
      " |      2010-01-05    89.0\n",
      " |      2010-01-06    88.0\n",
      " |      2010-01-07     NaN\n",
      " |\n",
      " |      The index entries that did not have a value in the original data frame\n",
      " |      (for example, '2009-12-29') are by default filled with ``NaN``.\n",
      " |      If desired, we can fill in the missing values using one of several\n",
      " |      options.\n",
      " |\n",
      " |      For example, to back-propagate the last valid value to fill the ``NaN``\n",
      " |      values, pass ``bfill`` as an argument to the ``method`` keyword.\n",
      " |\n",
      " |      >>> df2.reindex(date_index2, method='bfill')\n",
      " |                  prices\n",
      " |      2009-12-29   100.0\n",
      " |      2009-12-30   100.0\n",
      " |      2009-12-31   100.0\n",
      " |      2010-01-01   100.0\n",
      " |      2010-01-02   101.0\n",
      " |      2010-01-03     NaN\n",
      " |      2010-01-04   100.0\n",
      " |      2010-01-05    89.0\n",
      " |      2010-01-06    88.0\n",
      " |      2010-01-07     NaN\n",
      " |\n",
      " |      Please note that the ``NaN`` value present in the original dataframe\n",
      " |      (at index value 2010-01-03) will not be filled by any of the\n",
      " |      value propagation schemes. This is because filling while reindexing\n",
      " |      does not look at dataframe values, but only compares the original and\n",
      " |      desired indexes. If you do want to fill in the ``NaN`` values present\n",
      " |      in the original dataframe, use the ``fillna()`` method.\n",
      " |\n",
      " |      See the :ref:`user guide <basics.reindexing>` for more.\n",
      " |\n",
      " |  rename(self, mapper: 'Renamer | None' = None, *, index: 'Renamer | None' = None, columns: 'Renamer | None' = None, axis: 'Axis | None' = None, copy: 'bool | None' = None, inplace: 'bool' = False, level: 'Level | None' = None, errors: 'IgnoreRaise' = 'ignore') -> 'DataFrame | None'\n",
      " |      Rename columns or index labels.\n",
      " |\n",
      " |      Function / dict values must be unique (1-to-1). Labels not contained in\n",
      " |      a dict / Series will be left as-is. Extra labels listed don't throw an\n",
      " |      error.\n",
      " |\n",
      " |      See the :ref:`user guide <basics.rename>` for more.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      mapper : dict-like or function\n",
      " |          Dict-like or function transformations to apply to\n",
      " |          that axis' values. Use either ``mapper`` and ``axis`` to\n",
      " |          specify the axis to target with ``mapper``, or ``index`` and\n",
      " |          ``columns``.\n",
      " |      index : dict-like or function\n",
      " |          Alternative to specifying axis (``mapper, axis=0``\n",
      " |          is equivalent to ``index=mapper``).\n",
      " |      columns : dict-like or function\n",
      " |          Alternative to specifying axis (``mapper, axis=1``\n",
      " |          is equivalent to ``columns=mapper``).\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          Axis to target with ``mapper``. Can be either the axis name\n",
      " |          ('index', 'columns') or number (0, 1). The default is 'index'.\n",
      " |      copy : bool, default True\n",
      " |          Also copy underlying data.\n",
      " |\n",
      " |          .. note::\n",
      " |              The `copy` keyword will change behavior in pandas 3.0.\n",
      " |              `Copy-on-Write\n",
      " |              <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n",
      " |              will be enabled by default, which means that all methods with a\n",
      " |              `copy` keyword will use a lazy copy mechanism to defer the copy and\n",
      " |              ignore the `copy` keyword. The `copy` keyword will be removed in a\n",
      " |              future version of pandas.\n",
      " |\n",
      " |              You can already get the future behavior and improvements through\n",
      " |              enabling copy on write ``pd.options.mode.copy_on_write = True``\n",
      " |      inplace : bool, default False\n",
      " |          Whether to modify the DataFrame rather than creating a new one.\n",
      " |          If True then value of copy is ignored.\n",
      " |      level : int or level name, default None\n",
      " |          In case of a MultiIndex, only rename labels in the specified\n",
      " |          level.\n",
      " |      errors : {'ignore', 'raise'}, default 'ignore'\n",
      " |          If 'raise', raise a `KeyError` when a dict-like `mapper`, `index`,\n",
      " |          or `columns` contains labels that are not present in the Index\n",
      " |          being transformed.\n",
      " |          If 'ignore', existing keys will be renamed and extra keys will be\n",
      " |          ignored.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame or None\n",
      " |          DataFrame with the renamed axis labels or None if ``inplace=True``.\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If any of the labels is not found in the selected axis and\n",
      " |          \"errors='raise'\".\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.rename_axis : Set the name of the axis.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      ``DataFrame.rename`` supports two calling conventions\n",
      " |\n",
      " |      * ``(index=index_mapper, columns=columns_mapper, ...)``\n",
      " |      * ``(mapper, axis={'index', 'columns'}, ...)``\n",
      " |\n",
      " |      We *highly* recommend using keyword arguments to clarify your\n",
      " |      intent.\n",
      " |\n",
      " |      Rename columns using a mapping:\n",
      " |\n",
      " |      >>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n",
      " |      >>> df.rename(columns={\"A\": \"a\", \"B\": \"c\"})\n",
      " |         a  c\n",
      " |      0  1  4\n",
      " |      1  2  5\n",
      " |      2  3  6\n",
      " |\n",
      " |      Rename index using a mapping:\n",
      " |\n",
      " |      >>> df.rename(index={0: \"x\", 1: \"y\", 2: \"z\"})\n",
      " |         A  B\n",
      " |      x  1  4\n",
      " |      y  2  5\n",
      " |      z  3  6\n",
      " |\n",
      " |      Cast index labels to a different type:\n",
      " |\n",
      " |      >>> df.index\n",
      " |      RangeIndex(start=0, stop=3, step=1)\n",
      " |      >>> df.rename(index=str).index\n",
      " |      Index(['0', '1', '2'], dtype='object')\n",
      " |\n",
      " |      >>> df.rename(columns={\"A\": \"a\", \"B\": \"b\", \"C\": \"c\"}, errors=\"raise\")\n",
      " |      Traceback (most recent call last):\n",
      " |      KeyError: ['C'] not found in axis\n",
      " |\n",
      " |      Using axis-style parameters:\n",
      " |\n",
      " |      >>> df.rename(str.lower, axis='columns')\n",
      " |         a  b\n",
      " |      0  1  4\n",
      " |      1  2  5\n",
      " |      2  3  6\n",
      " |\n",
      " |      >>> df.rename({1: 2, 2: 4}, axis='index')\n",
      " |         A  B\n",
      " |      0  1  4\n",
      " |      2  2  5\n",
      " |      4  3  6\n",
      " |\n",
      " |  reorder_levels(self, order: 'Sequence[int | str]', axis: 'Axis' = 0) -> 'DataFrame'\n",
      " |      Rearrange index levels using input order. May not drop or duplicate levels.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      order : list of int or list of str\n",
      " |          List representing new level order. Reference level by number\n",
      " |          (position) or by key (label).\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          Where to reorder levels.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> data = {\n",
      " |      ...     \"class\": [\"Mammals\", \"Mammals\", \"Reptiles\"],\n",
      " |      ...     \"diet\": [\"Omnivore\", \"Carnivore\", \"Carnivore\"],\n",
      " |      ...     \"species\": [\"Humans\", \"Dogs\", \"Snakes\"],\n",
      " |      ... }\n",
      " |      >>> df = pd.DataFrame(data, columns=[\"class\", \"diet\", \"species\"])\n",
      " |      >>> df = df.set_index([\"class\", \"diet\"])\n",
      " |      >>> df\n",
      " |                                        species\n",
      " |      class      diet\n",
      " |      Mammals    Omnivore                Humans\n",
      " |                 Carnivore                 Dogs\n",
      " |      Reptiles   Carnivore               Snakes\n",
      " |\n",
      " |      Let's reorder the levels of the index:\n",
      " |\n",
      " |      >>> df.reorder_levels([\"diet\", \"class\"])\n",
      " |                                        species\n",
      " |      diet      class\n",
      " |      Omnivore  Mammals                  Humans\n",
      " |      Carnivore Mammals                    Dogs\n",
      " |                Reptiles                 Snakes\n",
      " |\n",
      " |  reset_index(self, level: 'IndexLabel | None' = None, *, drop: 'bool' = False, inplace: 'bool' = False, col_level: 'Hashable' = 0, col_fill: 'Hashable' = '', allow_duplicates: 'bool | lib.NoDefault' = <no_default>, names: 'Hashable | Sequence[Hashable] | None' = None) -> 'DataFrame | None'\n",
      " |      Reset the index, or a level of it.\n",
      " |\n",
      " |      Reset the index of the DataFrame, and use the default one instead.\n",
      " |      If the DataFrame has a MultiIndex, this method can remove one or more\n",
      " |      levels.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      level : int, str, tuple, or list, default None\n",
      " |          Only remove the given levels from the index. Removes all levels by\n",
      " |          default.\n",
      " |      drop : bool, default False\n",
      " |          Do not try to insert index into dataframe columns. This resets\n",
      " |          the index to the default integer index.\n",
      " |      inplace : bool, default False\n",
      " |          Whether to modify the DataFrame rather than creating a new one.\n",
      " |      col_level : int or str, default 0\n",
      " |          If the columns have multiple levels, determines which level the\n",
      " |          labels are inserted into. By default it is inserted into the first\n",
      " |          level.\n",
      " |      col_fill : object, default ''\n",
      " |          If the columns have multiple levels, determines how the other\n",
      " |          levels are named. If None then the index name is repeated.\n",
      " |      allow_duplicates : bool, optional, default lib.no_default\n",
      " |          Allow duplicate column labels to be created.\n",
      " |\n",
      " |          .. versionadded:: 1.5.0\n",
      " |\n",
      " |      names : int, str or 1-dimensional list, default None\n",
      " |          Using the given string, rename the DataFrame column which contains the\n",
      " |          index data. If the DataFrame has a MultiIndex, this has to be a list or\n",
      " |          tuple with length equal to the number of levels.\n",
      " |\n",
      " |          .. versionadded:: 1.5.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame or None\n",
      " |          DataFrame with the new index or None if ``inplace=True``.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.set_index : Opposite of reset_index.\n",
      " |      DataFrame.reindex : Change to new indices or expand indices.\n",
      " |      DataFrame.reindex_like : Change to same indices as other DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([('bird', 389.0),\n",
      " |      ...                    ('bird', 24.0),\n",
      " |      ...                    ('mammal', 80.5),\n",
      " |      ...                    ('mammal', np.nan)],\n",
      " |      ...                   index=['falcon', 'parrot', 'lion', 'monkey'],\n",
      " |      ...                   columns=('class', 'max_speed'))\n",
      " |      >>> df\n",
      " |               class  max_speed\n",
      " |      falcon    bird      389.0\n",
      " |      parrot    bird       24.0\n",
      " |      lion    mammal       80.5\n",
      " |      monkey  mammal        NaN\n",
      " |\n",
      " |      When we reset the index, the old index is added as a column, and a\n",
      " |      new sequential index is used:\n",
      " |\n",
      " |      >>> df.reset_index()\n",
      " |          index   class  max_speed\n",
      " |      0  falcon    bird      389.0\n",
      " |      1  parrot    bird       24.0\n",
      " |      2    lion  mammal       80.5\n",
      " |      3  monkey  mammal        NaN\n",
      " |\n",
      " |      We can use the `drop` parameter to avoid the old index being added as\n",
      " |      a column:\n",
      " |\n",
      " |      >>> df.reset_index(drop=True)\n",
      " |          class  max_speed\n",
      " |      0    bird      389.0\n",
      " |      1    bird       24.0\n",
      " |      2  mammal       80.5\n",
      " |      3  mammal        NaN\n",
      " |\n",
      " |      You can also use `reset_index` with `MultiIndex`.\n",
      " |\n",
      " |      >>> index = pd.MultiIndex.from_tuples([('bird', 'falcon'),\n",
      " |      ...                                    ('bird', 'parrot'),\n",
      " |      ...                                    ('mammal', 'lion'),\n",
      " |      ...                                    ('mammal', 'monkey')],\n",
      " |      ...                                   names=['class', 'name'])\n",
      " |      >>> columns = pd.MultiIndex.from_tuples([('speed', 'max'),\n",
      " |      ...                                      ('species', 'type')])\n",
      " |      >>> df = pd.DataFrame([(389.0, 'fly'),\n",
      " |      ...                    (24.0, 'fly'),\n",
      " |      ...                    (80.5, 'run'),\n",
      " |      ...                    (np.nan, 'jump')],\n",
      " |      ...                   index=index,\n",
      " |      ...                   columns=columns)\n",
      " |      >>> df\n",
      " |                     speed species\n",
      " |                       max    type\n",
      " |      class  name\n",
      " |      bird   falcon  389.0     fly\n",
      " |             parrot   24.0     fly\n",
      " |      mammal lion     80.5     run\n",
      " |             monkey    NaN    jump\n",
      " |\n",
      " |      Using the `names` parameter, choose a name for the index column:\n",
      " |\n",
      " |      >>> df.reset_index(names=['classes', 'names'])\n",
      " |        classes   names  speed species\n",
      " |                           max    type\n",
      " |      0    bird  falcon  389.0     fly\n",
      " |      1    bird  parrot   24.0     fly\n",
      " |      2  mammal    lion   80.5     run\n",
      " |      3  mammal  monkey    NaN    jump\n",
      " |\n",
      " |      If the index has multiple levels, we can reset a subset of them:\n",
      " |\n",
      " |      >>> df.reset_index(level='class')\n",
      " |               class  speed species\n",
      " |                        max    type\n",
      " |      name\n",
      " |      falcon    bird  389.0     fly\n",
      " |      parrot    bird   24.0     fly\n",
      " |      lion    mammal   80.5     run\n",
      " |      monkey  mammal    NaN    jump\n",
      " |\n",
      " |      If we are not dropping the index, by default, it is placed in the top\n",
      " |      level. We can place it in another level:\n",
      " |\n",
      " |      >>> df.reset_index(level='class', col_level=1)\n",
      " |                      speed species\n",
      " |               class    max    type\n",
      " |      name\n",
      " |      falcon    bird  389.0     fly\n",
      " |      parrot    bird   24.0     fly\n",
      " |      lion    mammal   80.5     run\n",
      " |      monkey  mammal    NaN    jump\n",
      " |\n",
      " |      When the index is inserted under another level, we can specify under\n",
      " |      which one with the parameter `col_fill`:\n",
      " |\n",
      " |      >>> df.reset_index(level='class', col_level=1, col_fill='species')\n",
      " |                    species  speed species\n",
      " |                      class    max    type\n",
      " |      name\n",
      " |      falcon           bird  389.0     fly\n",
      " |      parrot           bird   24.0     fly\n",
      " |      lion           mammal   80.5     run\n",
      " |      monkey         mammal    NaN    jump\n",
      " |\n",
      " |      If we specify a nonexistent level for `col_fill`, it is created:\n",
      " |\n",
      " |      >>> df.reset_index(level='class', col_level=1, col_fill='genus')\n",
      " |                      genus  speed species\n",
      " |                      class    max    type\n",
      " |      name\n",
      " |      falcon           bird  389.0     fly\n",
      " |      parrot           bird   24.0     fly\n",
      " |      lion           mammal   80.5     run\n",
      " |      monkey         mammal    NaN    jump\n",
      " |\n",
      " |  rfloordiv(self, other, axis: 'Axis' = 'columns', level=None, fill_value=None) -> 'DataFrame'\n",
      " |      Get Integer division of dataframe and other, element-wise (binary operator `rfloordiv`).\n",
      " |\n",
      " |      Equivalent to ``other // dataframe``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `floordiv`.\n",
      " |\n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `floordiv`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, dict or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns.\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |\n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |\n",
      " |      >>> df + 1\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      >>> df.add(1)\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      Divide by constant with reverse version.\n",
      " |\n",
      " |      >>> df.div(10)\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |\n",
      " |      >>> df.rdiv(10)\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |\n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |\n",
      " |      >>> df - [1, 2]\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub([1, 2], axis='columns')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |\n",
      " |      Multiply a dictionary by axis.\n",
      " |\n",
      " |      >>> df.mul({'angles': 0, 'degrees': 2})\n",
      " |                  angles  degrees\n",
      " |      circle           0      720\n",
      " |      triangle         0      360\n",
      " |      rectangle        0      720\n",
      " |\n",
      " |      >>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n",
      " |                  angles  degrees\n",
      " |      circle           0        0\n",
      " |      triangle         6      360\n",
      " |      rectangle       12     1080\n",
      " |\n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |\n",
      " |      >>> df * other\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |\n",
      " |      >>> df.mul(other, fill_value=0)\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |\n",
      " |      Divide by a MultiIndex by level.\n",
      " |\n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |\n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |\n",
      " |  rmod(self, other, axis: 'Axis' = 'columns', level=None, fill_value=None) -> 'DataFrame'\n",
      " |      Get Modulo of dataframe and other, element-wise (binary operator `rmod`).\n",
      " |\n",
      " |      Equivalent to ``other % dataframe``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `mod`.\n",
      " |\n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `floordiv`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, dict or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns.\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |\n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |\n",
      " |      >>> df + 1\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      >>> df.add(1)\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      Divide by constant with reverse version.\n",
      " |\n",
      " |      >>> df.div(10)\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |\n",
      " |      >>> df.rdiv(10)\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |\n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |\n",
      " |      >>> df - [1, 2]\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub([1, 2], axis='columns')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |\n",
      " |      Multiply a dictionary by axis.\n",
      " |\n",
      " |      >>> df.mul({'angles': 0, 'degrees': 2})\n",
      " |                  angles  degrees\n",
      " |      circle           0      720\n",
      " |      triangle         0      360\n",
      " |      rectangle        0      720\n",
      " |\n",
      " |      >>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n",
      " |                  angles  degrees\n",
      " |      circle           0        0\n",
      " |      triangle         6      360\n",
      " |      rectangle       12     1080\n",
      " |\n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |\n",
      " |      >>> df * other\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |\n",
      " |      >>> df.mul(other, fill_value=0)\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |\n",
      " |      Divide by a MultiIndex by level.\n",
      " |\n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |\n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |\n",
      " |  rmul(self, other, axis: 'Axis' = 'columns', level=None, fill_value=None) -> 'DataFrame'\n",
      " |      Get Multiplication of dataframe and other, element-wise (binary operator `rmul`).\n",
      " |\n",
      " |      Equivalent to ``other * dataframe``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `mul`.\n",
      " |\n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `floordiv`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, dict or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns.\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |\n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |\n",
      " |      >>> df + 1\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      >>> df.add(1)\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      Divide by constant with reverse version.\n",
      " |\n",
      " |      >>> df.div(10)\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |\n",
      " |      >>> df.rdiv(10)\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |\n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |\n",
      " |      >>> df - [1, 2]\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub([1, 2], axis='columns')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |\n",
      " |      Multiply a dictionary by axis.\n",
      " |\n",
      " |      >>> df.mul({'angles': 0, 'degrees': 2})\n",
      " |                  angles  degrees\n",
      " |      circle           0      720\n",
      " |      triangle         0      360\n",
      " |      rectangle        0      720\n",
      " |\n",
      " |      >>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n",
      " |                  angles  degrees\n",
      " |      circle           0        0\n",
      " |      triangle         6      360\n",
      " |      rectangle       12     1080\n",
      " |\n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |\n",
      " |      >>> df * other\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |\n",
      " |      >>> df.mul(other, fill_value=0)\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |\n",
      " |      Divide by a MultiIndex by level.\n",
      " |\n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |\n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |\n",
      " |  round(self, decimals: 'int | dict[IndexLabel, int] | Series' = 0, *args, **kwargs) -> 'DataFrame'\n",
      " |      Round a DataFrame to a variable number of decimal places.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      decimals : int, dict, Series\n",
      " |          Number of decimal places to round each column to. If an int is\n",
      " |          given, round each column to the same number of places.\n",
      " |          Otherwise dict and Series round to variable numbers of places.\n",
      " |          Column names should be in the keys if `decimals` is a\n",
      " |          dict-like, or in the index if `decimals` is a Series. Any\n",
      " |          columns not included in `decimals` will be left as is. Elements\n",
      " |          of `decimals` which are not columns of the input will be\n",
      " |          ignored.\n",
      " |      *args\n",
      " |          Additional keywords have no effect but might be accepted for\n",
      " |          compatibility with numpy.\n",
      " |      **kwargs\n",
      " |          Additional keywords have no effect but might be accepted for\n",
      " |          compatibility with numpy.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          A DataFrame with the affected columns rounded to the specified\n",
      " |          number of decimal places.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.around : Round a numpy array to the given number of decimals.\n",
      " |      Series.round : Round a Series to the given number of decimals.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([(.21, .32), (.01, .67), (.66, .03), (.21, .18)],\n",
      " |      ...                   columns=['dogs', 'cats'])\n",
      " |      >>> df\n",
      " |          dogs  cats\n",
      " |      0  0.21  0.32\n",
      " |      1  0.01  0.67\n",
      " |      2  0.66  0.03\n",
      " |      3  0.21  0.18\n",
      " |\n",
      " |      By providing an integer each column is rounded to the same number\n",
      " |      of decimal places\n",
      " |\n",
      " |      >>> df.round(1)\n",
      " |          dogs  cats\n",
      " |      0   0.2   0.3\n",
      " |      1   0.0   0.7\n",
      " |      2   0.7   0.0\n",
      " |      3   0.2   0.2\n",
      " |\n",
      " |      With a dict, the number of places for specific columns can be\n",
      " |      specified with the column names as key and the number of decimal\n",
      " |      places as value\n",
      " |\n",
      " |      >>> df.round({'dogs': 1, 'cats': 0})\n",
      " |          dogs  cats\n",
      " |      0   0.2   0.0\n",
      " |      1   0.0   1.0\n",
      " |      2   0.7   0.0\n",
      " |      3   0.2   0.0\n",
      " |\n",
      " |      Using a Series, the number of places for specific columns can be\n",
      " |      specified with the column names as index and the number of\n",
      " |      decimal places as value\n",
      " |\n",
      " |      >>> decimals = pd.Series([0, 1], index=['cats', 'dogs'])\n",
      " |      >>> df.round(decimals)\n",
      " |          dogs  cats\n",
      " |      0   0.2   0.0\n",
      " |      1   0.0   1.0\n",
      " |      2   0.7   0.0\n",
      " |      3   0.2   0.0\n",
      " |\n",
      " |  rpow(self, other, axis: 'Axis' = 'columns', level=None, fill_value=None) -> 'DataFrame'\n",
      " |      Get Exponential power of dataframe and other, element-wise (binary operator `rpow`).\n",
      " |\n",
      " |      Equivalent to ``other ** dataframe``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `pow`.\n",
      " |\n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `floordiv`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, dict or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns.\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |\n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |\n",
      " |      >>> df + 1\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      >>> df.add(1)\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      Divide by constant with reverse version.\n",
      " |\n",
      " |      >>> df.div(10)\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |\n",
      " |      >>> df.rdiv(10)\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |\n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |\n",
      " |      >>> df - [1, 2]\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub([1, 2], axis='columns')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |\n",
      " |      Multiply a dictionary by axis.\n",
      " |\n",
      " |      >>> df.mul({'angles': 0, 'degrees': 2})\n",
      " |                  angles  degrees\n",
      " |      circle           0      720\n",
      " |      triangle         0      360\n",
      " |      rectangle        0      720\n",
      " |\n",
      " |      >>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n",
      " |                  angles  degrees\n",
      " |      circle           0        0\n",
      " |      triangle         6      360\n",
      " |      rectangle       12     1080\n",
      " |\n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |\n",
      " |      >>> df * other\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |\n",
      " |      >>> df.mul(other, fill_value=0)\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |\n",
      " |      Divide by a MultiIndex by level.\n",
      " |\n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |\n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |\n",
      " |  rsub(self, other, axis: 'Axis' = 'columns', level=None, fill_value=None) -> 'DataFrame'\n",
      " |      Get Subtraction of dataframe and other, element-wise (binary operator `rsub`).\n",
      " |\n",
      " |      Equivalent to ``other - dataframe``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `sub`.\n",
      " |\n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `floordiv`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, dict or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns.\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |\n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |\n",
      " |      >>> df + 1\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      >>> df.add(1)\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      Divide by constant with reverse version.\n",
      " |\n",
      " |      >>> df.div(10)\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |\n",
      " |      >>> df.rdiv(10)\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |\n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |\n",
      " |      >>> df - [1, 2]\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub([1, 2], axis='columns')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |\n",
      " |      Multiply a dictionary by axis.\n",
      " |\n",
      " |      >>> df.mul({'angles': 0, 'degrees': 2})\n",
      " |                  angles  degrees\n",
      " |      circle           0      720\n",
      " |      triangle         0      360\n",
      " |      rectangle        0      720\n",
      " |\n",
      " |      >>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n",
      " |                  angles  degrees\n",
      " |      circle           0        0\n",
      " |      triangle         6      360\n",
      " |      rectangle       12     1080\n",
      " |\n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |\n",
      " |      >>> df * other\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |\n",
      " |      >>> df.mul(other, fill_value=0)\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |\n",
      " |      Divide by a MultiIndex by level.\n",
      " |\n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |\n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |\n",
      " |  rtruediv(self, other, axis: 'Axis' = 'columns', level=None, fill_value=None) -> 'DataFrame'\n",
      " |      Get Floating division of dataframe and other, element-wise (binary operator `rtruediv`).\n",
      " |\n",
      " |      Equivalent to ``other / dataframe``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `truediv`.\n",
      " |\n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `floordiv`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, dict or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns.\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |\n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |\n",
      " |      >>> df + 1\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      >>> df.add(1)\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      Divide by constant with reverse version.\n",
      " |\n",
      " |      >>> df.div(10)\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |\n",
      " |      >>> df.rdiv(10)\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |\n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |\n",
      " |      >>> df - [1, 2]\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub([1, 2], axis='columns')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |\n",
      " |      Multiply a dictionary by axis.\n",
      " |\n",
      " |      >>> df.mul({'angles': 0, 'degrees': 2})\n",
      " |                  angles  degrees\n",
      " |      circle           0      720\n",
      " |      triangle         0      360\n",
      " |      rectangle        0      720\n",
      " |\n",
      " |      >>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n",
      " |                  angles  degrees\n",
      " |      circle           0        0\n",
      " |      triangle         6      360\n",
      " |      rectangle       12     1080\n",
      " |\n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |\n",
      " |      >>> df * other\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |\n",
      " |      >>> df.mul(other, fill_value=0)\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |\n",
      " |      Divide by a MultiIndex by level.\n",
      " |\n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |\n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |\n",
      " |  select_dtypes(self, include=None, exclude=None) -> 'Self'\n",
      " |      Return a subset of the DataFrame's columns based on the column dtypes.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      include, exclude : scalar or list-like\n",
      " |          A selection of dtypes or strings to be included/excluded. At least\n",
      " |          one of these parameters must be supplied.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          The subset of the frame including the dtypes in ``include`` and\n",
      " |          excluding the dtypes in ``exclude``.\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      ValueError\n",
      " |          * If both of ``include`` and ``exclude`` are empty\n",
      " |          * If ``include`` and ``exclude`` have overlapping elements\n",
      " |          * If any kind of string dtype is passed in.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.dtypes: Return Series with the data type of each column.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      * To select all *numeric* types, use ``np.number`` or ``'number'``\n",
      " |      * To select strings you must use the ``object`` dtype, but note that\n",
      " |        this will return *all* object dtype columns\n",
      " |      * See the `numpy dtype hierarchy\n",
      " |        <https://numpy.org/doc/stable/reference/arrays.scalars.html>`__\n",
      " |      * To select datetimes, use ``np.datetime64``, ``'datetime'`` or\n",
      " |        ``'datetime64'``\n",
      " |      * To select timedeltas, use ``np.timedelta64``, ``'timedelta'`` or\n",
      " |        ``'timedelta64'``\n",
      " |      * To select Pandas categorical dtypes, use ``'category'``\n",
      " |      * To select Pandas datetimetz dtypes, use ``'datetimetz'``\n",
      " |        or ``'datetime64[ns, tz]'``\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'a': [1, 2] * 3,\n",
      " |      ...                    'b': [True, False] * 3,\n",
      " |      ...                    'c': [1.0, 2.0] * 3})\n",
      " |      >>> df\n",
      " |              a      b  c\n",
      " |      0       1   True  1.0\n",
      " |      1       2  False  2.0\n",
      " |      2       1   True  1.0\n",
      " |      3       2  False  2.0\n",
      " |      4       1   True  1.0\n",
      " |      5       2  False  2.0\n",
      " |\n",
      " |      >>> df.select_dtypes(include='bool')\n",
      " |         b\n",
      " |      0  True\n",
      " |      1  False\n",
      " |      2  True\n",
      " |      3  False\n",
      " |      4  True\n",
      " |      5  False\n",
      " |\n",
      " |      >>> df.select_dtypes(include=['float64'])\n",
      " |         c\n",
      " |      0  1.0\n",
      " |      1  2.0\n",
      " |      2  1.0\n",
      " |      3  2.0\n",
      " |      4  1.0\n",
      " |      5  2.0\n",
      " |\n",
      " |      >>> df.select_dtypes(exclude=['int64'])\n",
      " |             b    c\n",
      " |      0   True  1.0\n",
      " |      1  False  2.0\n",
      " |      2   True  1.0\n",
      " |      3  False  2.0\n",
      " |      4   True  1.0\n",
      " |      5  False  2.0\n",
      " |\n",
      " |  sem(self, axis: 'Axis | None' = 0, skipna: 'bool' = True, ddof: 'int' = 1, numeric_only: 'bool' = False, **kwargs)\n",
      " |      Return unbiased standard error of the mean over requested axis.\n",
      " |\n",
      " |      Normalized by N-1 by default. This can be changed using the ddof argument\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {index (0), columns (1)}\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |\n",
      " |          .. warning::\n",
      " |\n",
      " |              The behavior of DataFrame.sem with ``axis=None`` is deprecated,\n",
      " |              in a future version this will reduce over both axes and return a scalar\n",
      " |              To retain the old behavior, pass axis=0 (or do not pass axis).\n",
      " |\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If an entire row/column is NA, the result\n",
      " |          will be NA.\n",
      " |      ddof : int, default 1\n",
      " |          Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\n",
      " |          where N represents the number of elements.\n",
      " |      numeric_only : bool, default False\n",
      " |          Include only float, int, boolean columns. Not implemented for Series.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame (if level specified)\n",
      " |\n",
      " |                  Examples\n",
      " |                  --------\n",
      " |                  >>> s = pd.Series([1, 2, 3])\n",
      " |                  >>> s.sem().round(6)\n",
      " |                  0.57735\n",
      " |\n",
      " |                  With a DataFrame\n",
      " |\n",
      " |                  >>> df = pd.DataFrame({'a': [1, 2], 'b': [2, 3]}, index=['tiger', 'zebra'])\n",
      " |                  >>> df\n",
      " |                         a   b\n",
      " |                  tiger  1   2\n",
      " |                  zebra  2   3\n",
      " |                  >>> df.sem()\n",
      " |                  a   0.5\n",
      " |                  b   0.5\n",
      " |                  dtype: float64\n",
      " |\n",
      " |                  Using axis=1\n",
      " |\n",
      " |                  >>> df.sem(axis=1)\n",
      " |                  tiger   0.5\n",
      " |                  zebra   0.5\n",
      " |                  dtype: float64\n",
      " |\n",
      " |                  In this case, `numeric_only` should be set to `True`\n",
      " |                  to avoid getting an error.\n",
      " |\n",
      " |                  >>> df = pd.DataFrame({'a': [1, 2], 'b': ['T', 'Z']},\n",
      " |                  ...                   index=['tiger', 'zebra'])\n",
      " |                  >>> df.sem(numeric_only=True)\n",
      " |                  a   0.5\n",
      " |                  dtype: float64\n",
      " |\n",
      " |  set_axis(self, labels, *, axis: 'Axis' = 0, copy: 'bool | None' = None) -> 'DataFrame'\n",
      " |      Assign desired index to given axis.\n",
      " |\n",
      " |      Indexes for column or row labels can be changed by assigning\n",
      " |      a list-like or Index.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      labels : list-like, Index\n",
      " |          The values for the new index.\n",
      " |\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The axis to update. The value 0 identifies the rows. For `Series`\n",
      " |          this parameter is unused and defaults to 0.\n",
      " |\n",
      " |      copy : bool, default True\n",
      " |          Whether to make a copy of the underlying data.\n",
      " |\n",
      " |          .. note::\n",
      " |              The `copy` keyword will change behavior in pandas 3.0.\n",
      " |              `Copy-on-Write\n",
      " |              <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n",
      " |              will be enabled by default, which means that all methods with a\n",
      " |              `copy` keyword will use a lazy copy mechanism to defer the copy and\n",
      " |              ignore the `copy` keyword. The `copy` keyword will be removed in a\n",
      " |              future version of pandas.\n",
      " |\n",
      " |              You can already get the future behavior and improvements through\n",
      " |              enabling copy on write ``pd.options.mode.copy_on_write = True``\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          An object of type DataFrame.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.rename_axis : Alter the name of the index or columns.\n",
      " |\n",
      " |              Examples\n",
      " |              --------\n",
      " |              >>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n",
      " |\n",
      " |              Change the row labels.\n",
      " |\n",
      " |              >>> df.set_axis(['a', 'b', 'c'], axis='index')\n",
      " |                 A  B\n",
      " |              a  1  4\n",
      " |              b  2  5\n",
      " |              c  3  6\n",
      " |\n",
      " |              Change the column labels.\n",
      " |\n",
      " |              >>> df.set_axis(['I', 'II'], axis='columns')\n",
      " |                 I  II\n",
      " |              0  1   4\n",
      " |              1  2   5\n",
      " |              2  3   6\n",
      " |\n",
      " |  set_index(self, keys, *, drop: 'bool' = True, append: 'bool' = False, inplace: 'bool' = False, verify_integrity: 'bool' = False) -> 'DataFrame | None'\n",
      " |      Set the DataFrame index using existing columns.\n",
      " |\n",
      " |      Set the DataFrame index (row labels) using one or more existing\n",
      " |      columns or arrays (of the correct length). The index can replace the\n",
      " |      existing index or expand on it.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : label or array-like or list of labels/arrays\n",
      " |          This parameter can be either a single column key, a single array of\n",
      " |          the same length as the calling DataFrame, or a list containing an\n",
      " |          arbitrary combination of column keys and arrays. Here, \"array\"\n",
      " |          encompasses :class:`Series`, :class:`Index`, ``np.ndarray``, and\n",
      " |          instances of :class:`~collections.abc.Iterator`.\n",
      " |      drop : bool, default True\n",
      " |          Delete columns to be used as the new index.\n",
      " |      append : bool, default False\n",
      " |          Whether to append columns to existing index.\n",
      " |      inplace : bool, default False\n",
      " |          Whether to modify the DataFrame rather than creating a new one.\n",
      " |      verify_integrity : bool, default False\n",
      " |          Check the new index for duplicates. Otherwise defer the check until\n",
      " |          necessary. Setting to False will improve the performance of this\n",
      " |          method.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame or None\n",
      " |          Changed row labels or None if ``inplace=True``.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.reset_index : Opposite of set_index.\n",
      " |      DataFrame.reindex : Change to new indices or expand indices.\n",
      " |      DataFrame.reindex_like : Change to same indices as other DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'month': [1, 4, 7, 10],\n",
      " |      ...                    'year': [2012, 2014, 2013, 2014],\n",
      " |      ...                    'sale': [55, 40, 84, 31]})\n",
      " |      >>> df\n",
      " |         month  year  sale\n",
      " |      0      1  2012    55\n",
      " |      1      4  2014    40\n",
      " |      2      7  2013    84\n",
      " |      3     10  2014    31\n",
      " |\n",
      " |      Set the index to become the 'month' column:\n",
      " |\n",
      " |      >>> df.set_index('month')\n",
      " |             year  sale\n",
      " |      month\n",
      " |      1      2012    55\n",
      " |      4      2014    40\n",
      " |      7      2013    84\n",
      " |      10     2014    31\n",
      " |\n",
      " |      Create a MultiIndex using columns 'year' and 'month':\n",
      " |\n",
      " |      >>> df.set_index(['year', 'month'])\n",
      " |                  sale\n",
      " |      year  month\n",
      " |      2012  1     55\n",
      " |      2014  4     40\n",
      " |      2013  7     84\n",
      " |      2014  10    31\n",
      " |\n",
      " |      Create a MultiIndex using an Index and a column:\n",
      " |\n",
      " |      >>> df.set_index([pd.Index([1, 2, 3, 4]), 'year'])\n",
      " |               month  sale\n",
      " |         year\n",
      " |      1  2012  1      55\n",
      " |      2  2014  4      40\n",
      " |      3  2013  7      84\n",
      " |      4  2014  10     31\n",
      " |\n",
      " |      Create a MultiIndex using two Series:\n",
      " |\n",
      " |      >>> s = pd.Series([1, 2, 3, 4])\n",
      " |      >>> df.set_index([s, s**2])\n",
      " |            month  year  sale\n",
      " |      1 1       1  2012    55\n",
      " |      2 4       4  2014    40\n",
      " |      3 9       7  2013    84\n",
      " |      4 16     10  2014    31\n",
      " |\n",
      " |  shift(self, periods: 'int | Sequence[int]' = 1, freq: 'Frequency | None' = None, axis: 'Axis' = 0, fill_value: 'Hashable' = <no_default>, suffix: 'str | None' = None) -> 'DataFrame'\n",
      " |      Shift index by desired number of periods with an optional time `freq`.\n",
      " |\n",
      " |      When `freq` is not passed, shift the index without realigning the data.\n",
      " |      If `freq` is passed (in this case, the index must be date or datetime,\n",
      " |      or it will raise a `NotImplementedError`), the index will be\n",
      " |      increased using the periods and the `freq`. `freq` can be inferred\n",
      " |      when specified as \"infer\" as long as either freq or inferred_freq\n",
      " |      attribute is set in the index.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      periods : int or Sequence\n",
      " |          Number of periods to shift. Can be positive or negative.\n",
      " |          If an iterable of ints, the data will be shifted once by each int.\n",
      " |          This is equivalent to shifting by one value at a time and\n",
      " |          concatenating all resulting frames. The resulting columns will have\n",
      " |          the shift suffixed to their column names. For multiple periods,\n",
      " |          axis must not be 1.\n",
      " |      freq : DateOffset, tseries.offsets, timedelta, or str, optional\n",
      " |          Offset to use from the tseries module or time rule (e.g. 'EOM').\n",
      " |          If `freq` is specified then the index values are shifted but the\n",
      " |          data is not realigned. That is, use `freq` if you would like to\n",
      " |          extend the index when shifting and preserve the original data.\n",
      " |          If `freq` is specified as \"infer\" then it will be inferred from\n",
      " |          the freq or inferred_freq attributes of the index. If neither of\n",
      " |          those attributes exist, a ValueError is thrown.\n",
      " |      axis : {0 or 'index', 1 or 'columns', None}, default None\n",
      " |          Shift direction. For `Series` this parameter is unused and defaults to 0.\n",
      " |      fill_value : object, optional\n",
      " |          The scalar value to use for newly introduced missing values.\n",
      " |          the default depends on the dtype of `self`.\n",
      " |          For numeric data, ``np.nan`` is used.\n",
      " |          For datetime, timedelta, or period data, etc. :attr:`NaT` is used.\n",
      " |          For extension dtypes, ``self.dtype.na_value`` is used.\n",
      " |      suffix : str, optional\n",
      " |          If str and periods is an iterable, this is added after the column\n",
      " |          name and before the shift value for each shifted column name.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Copy of input object, shifted.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Index.shift : Shift values of Index.\n",
      " |      DatetimeIndex.shift : Shift values of DatetimeIndex.\n",
      " |      PeriodIndex.shift : Shift values of PeriodIndex.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({\"Col1\": [10, 20, 15, 30, 45],\n",
      " |      ...                    \"Col2\": [13, 23, 18, 33, 48],\n",
      " |      ...                    \"Col3\": [17, 27, 22, 37, 52]},\n",
      " |      ...                   index=pd.date_range(\"2020-01-01\", \"2020-01-05\"))\n",
      " |      >>> df\n",
      " |                  Col1  Col2  Col3\n",
      " |      2020-01-01    10    13    17\n",
      " |      2020-01-02    20    23    27\n",
      " |      2020-01-03    15    18    22\n",
      " |      2020-01-04    30    33    37\n",
      " |      2020-01-05    45    48    52\n",
      " |\n",
      " |      >>> df.shift(periods=3)\n",
      " |                  Col1  Col2  Col3\n",
      " |      2020-01-01   NaN   NaN   NaN\n",
      " |      2020-01-02   NaN   NaN   NaN\n",
      " |      2020-01-03   NaN   NaN   NaN\n",
      " |      2020-01-04  10.0  13.0  17.0\n",
      " |      2020-01-05  20.0  23.0  27.0\n",
      " |\n",
      " |      >>> df.shift(periods=1, axis=\"columns\")\n",
      " |                  Col1  Col2  Col3\n",
      " |      2020-01-01   NaN    10    13\n",
      " |      2020-01-02   NaN    20    23\n",
      " |      2020-01-03   NaN    15    18\n",
      " |      2020-01-04   NaN    30    33\n",
      " |      2020-01-05   NaN    45    48\n",
      " |\n",
      " |      >>> df.shift(periods=3, fill_value=0)\n",
      " |                  Col1  Col2  Col3\n",
      " |      2020-01-01     0     0     0\n",
      " |      2020-01-02     0     0     0\n",
      " |      2020-01-03     0     0     0\n",
      " |      2020-01-04    10    13    17\n",
      " |      2020-01-05    20    23    27\n",
      " |\n",
      " |      >>> df.shift(periods=3, freq=\"D\")\n",
      " |                  Col1  Col2  Col3\n",
      " |      2020-01-04    10    13    17\n",
      " |      2020-01-05    20    23    27\n",
      " |      2020-01-06    15    18    22\n",
      " |      2020-01-07    30    33    37\n",
      " |      2020-01-08    45    48    52\n",
      " |\n",
      " |      >>> df.shift(periods=3, freq=\"infer\")\n",
      " |                  Col1  Col2  Col3\n",
      " |      2020-01-04    10    13    17\n",
      " |      2020-01-05    20    23    27\n",
      " |      2020-01-06    15    18    22\n",
      " |      2020-01-07    30    33    37\n",
      " |      2020-01-08    45    48    52\n",
      " |\n",
      " |      >>> df['Col1'].shift(periods=[0, 1, 2])\n",
      " |                  Col1_0  Col1_1  Col1_2\n",
      " |      2020-01-01      10     NaN     NaN\n",
      " |      2020-01-02      20    10.0     NaN\n",
      " |      2020-01-03      15    20.0    10.0\n",
      " |      2020-01-04      30    15.0    20.0\n",
      " |      2020-01-05      45    30.0    15.0\n",
      " |\n",
      " |  sort_index(self, *, axis: 'Axis' = 0, level: 'IndexLabel | None' = None, ascending: 'bool | Sequence[bool]' = True, inplace: 'bool' = False, kind: 'SortKind' = 'quicksort', na_position: 'NaPosition' = 'last', sort_remaining: 'bool' = True, ignore_index: 'bool' = False, key: 'IndexKeyFunc | None' = None) -> 'DataFrame | None'\n",
      " |      Sort object by labels (along an axis).\n",
      " |\n",
      " |      Returns a new DataFrame sorted by label if `inplace` argument is\n",
      " |      ``False``, otherwise updates the original DataFrame and returns None.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The axis along which to sort.  The value 0 identifies the rows,\n",
      " |          and 1 identifies the columns.\n",
      " |      level : int or level name or list of ints or list of level names\n",
      " |          If not None, sort on values in specified index level(s).\n",
      " |      ascending : bool or list-like of bools, default True\n",
      " |          Sort ascending vs. descending. When the index is a MultiIndex the\n",
      " |          sort direction can be controlled for each level individually.\n",
      " |      inplace : bool, default False\n",
      " |          Whether to modify the DataFrame rather than creating a new one.\n",
      " |      kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n",
      " |          Choice of sorting algorithm. See also :func:`numpy.sort` for more\n",
      " |          information. `mergesort` and `stable` are the only stable algorithms. For\n",
      " |          DataFrames, this option is only applied when sorting on a single\n",
      " |          column or label.\n",
      " |      na_position : {'first', 'last'}, default 'last'\n",
      " |          Puts NaNs at the beginning if `first`; `last` puts NaNs at the end.\n",
      " |          Not implemented for MultiIndex.\n",
      " |      sort_remaining : bool, default True\n",
      " |          If True and sorting by level and index is multilevel, sort by other\n",
      " |          levels too (in order) after sorting by specified level.\n",
      " |      ignore_index : bool, default False\n",
      " |          If True, the resulting axis will be labeled 0, 1, …, n - 1.\n",
      " |      key : callable, optional\n",
      " |          If not None, apply the key function to the index values\n",
      " |          before sorting. This is similar to the `key` argument in the\n",
      " |          builtin :meth:`sorted` function, with the notable difference that\n",
      " |          this `key` function should be *vectorized*. It should expect an\n",
      " |          ``Index`` and return an ``Index`` of the same shape. For MultiIndex\n",
      " |          inputs, the key is applied *per level*.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame or None\n",
      " |          The original DataFrame sorted by the labels or None if ``inplace=True``.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.sort_index : Sort Series by the index.\n",
      " |      DataFrame.sort_values : Sort DataFrame by the value.\n",
      " |      Series.sort_values : Sort Series by the value.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150],\n",
      " |      ...                   columns=['A'])\n",
      " |      >>> df.sort_index()\n",
      " |           A\n",
      " |      1    4\n",
      " |      29   2\n",
      " |      100  1\n",
      " |      150  5\n",
      " |      234  3\n",
      " |\n",
      " |      By default, it sorts in ascending order, to sort in descending order,\n",
      " |      use ``ascending=False``\n",
      " |\n",
      " |      >>> df.sort_index(ascending=False)\n",
      " |           A\n",
      " |      234  3\n",
      " |      150  5\n",
      " |      100  1\n",
      " |      29   2\n",
      " |      1    4\n",
      " |\n",
      " |      A key function can be specified which is applied to the index before\n",
      " |      sorting. For a ``MultiIndex`` this is applied to each level separately.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({\"a\": [1, 2, 3, 4]}, index=['A', 'b', 'C', 'd'])\n",
      " |      >>> df.sort_index(key=lambda x: x.str.lower())\n",
      " |         a\n",
      " |      A  1\n",
      " |      b  2\n",
      " |      C  3\n",
      " |      d  4\n",
      " |\n",
      " |  sort_values(self, by: 'IndexLabel', *, axis: 'Axis' = 0, ascending: 'bool | list[bool] | tuple[bool, ...]' = True, inplace: 'bool' = False, kind: 'SortKind' = 'quicksort', na_position: 'str' = 'last', ignore_index: 'bool' = False, key: 'ValueKeyFunc | None' = None) -> 'DataFrame | None'\n",
      " |      Sort by the values along either axis.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      by : str or list of str\n",
      " |          Name or list of names to sort by.\n",
      " |\n",
      " |          - if `axis` is 0 or `'index'` then `by` may contain index\n",
      " |            levels and/or column labels.\n",
      " |          - if `axis` is 1 or `'columns'` then `by` may contain column\n",
      " |            levels and/or index labels.\n",
      " |      axis : \"{0 or 'index', 1 or 'columns'}\", default 0\n",
      " |           Axis to be sorted.\n",
      " |      ascending : bool or list of bool, default True\n",
      " |           Sort ascending vs. descending. Specify list for multiple sort\n",
      " |           orders.  If this is a list of bools, must match the length of\n",
      " |           the by.\n",
      " |      inplace : bool, default False\n",
      " |           If True, perform operation in-place.\n",
      " |      kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n",
      " |           Choice of sorting algorithm. See also :func:`numpy.sort` for more\n",
      " |           information. `mergesort` and `stable` are the only stable algorithms. For\n",
      " |           DataFrames, this option is only applied when sorting on a single\n",
      " |           column or label.\n",
      " |      na_position : {'first', 'last'}, default 'last'\n",
      " |           Puts NaNs at the beginning if `first`; `last` puts NaNs at the\n",
      " |           end.\n",
      " |      ignore_index : bool, default False\n",
      " |           If True, the resulting axis will be labeled 0, 1, …, n - 1.\n",
      " |      key : callable, optional\n",
      " |          Apply the key function to the values\n",
      " |          before sorting. This is similar to the `key` argument in the\n",
      " |          builtin :meth:`sorted` function, with the notable difference that\n",
      " |          this `key` function should be *vectorized*. It should expect a\n",
      " |          ``Series`` and return a Series with the same shape as the input.\n",
      " |          It will be applied to each column in `by` independently.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame or None\n",
      " |          DataFrame with sorted values or None if ``inplace=True``.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.sort_index : Sort a DataFrame by the index.\n",
      " |      Series.sort_values : Similar method for a Series.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({\n",
      " |      ...     'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n",
      " |      ...     'col2': [2, 1, 9, 8, 7, 4],\n",
      " |      ...     'col3': [0, 1, 9, 4, 2, 3],\n",
      " |      ...     'col4': ['a', 'B', 'c', 'D', 'e', 'F']\n",
      " |      ... })\n",
      " |      >>> df\n",
      " |        col1  col2  col3 col4\n",
      " |      0    A     2     0    a\n",
      " |      1    A     1     1    B\n",
      " |      2    B     9     9    c\n",
      " |      3  NaN     8     4    D\n",
      " |      4    D     7     2    e\n",
      " |      5    C     4     3    F\n",
      " |\n",
      " |      Sort by col1\n",
      " |\n",
      " |      >>> df.sort_values(by=['col1'])\n",
      " |        col1  col2  col3 col4\n",
      " |      0    A     2     0    a\n",
      " |      1    A     1     1    B\n",
      " |      2    B     9     9    c\n",
      " |      5    C     4     3    F\n",
      " |      4    D     7     2    e\n",
      " |      3  NaN     8     4    D\n",
      " |\n",
      " |      Sort by multiple columns\n",
      " |\n",
      " |      >>> df.sort_values(by=['col1', 'col2'])\n",
      " |        col1  col2  col3 col4\n",
      " |      1    A     1     1    B\n",
      " |      0    A     2     0    a\n",
      " |      2    B     9     9    c\n",
      " |      5    C     4     3    F\n",
      " |      4    D     7     2    e\n",
      " |      3  NaN     8     4    D\n",
      " |\n",
      " |      Sort Descending\n",
      " |\n",
      " |      >>> df.sort_values(by='col1', ascending=False)\n",
      " |        col1  col2  col3 col4\n",
      " |      4    D     7     2    e\n",
      " |      5    C     4     3    F\n",
      " |      2    B     9     9    c\n",
      " |      0    A     2     0    a\n",
      " |      1    A     1     1    B\n",
      " |      3  NaN     8     4    D\n",
      " |\n",
      " |      Putting NAs first\n",
      " |\n",
      " |      >>> df.sort_values(by='col1', ascending=False, na_position='first')\n",
      " |        col1  col2  col3 col4\n",
      " |      3  NaN     8     4    D\n",
      " |      4    D     7     2    e\n",
      " |      5    C     4     3    F\n",
      " |      2    B     9     9    c\n",
      " |      0    A     2     0    a\n",
      " |      1    A     1     1    B\n",
      " |\n",
      " |      Sorting with a key function\n",
      " |\n",
      " |      >>> df.sort_values(by='col4', key=lambda col: col.str.lower())\n",
      " |         col1  col2  col3 col4\n",
      " |      0    A     2     0    a\n",
      " |      1    A     1     1    B\n",
      " |      2    B     9     9    c\n",
      " |      3  NaN     8     4    D\n",
      " |      4    D     7     2    e\n",
      " |      5    C     4     3    F\n",
      " |\n",
      " |      Natural sort with the key argument,\n",
      " |      using the `natsort <https://github.com/SethMMorton/natsort>` package.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({\n",
      " |      ...    \"time\": ['0hr', '128hr', '72hr', '48hr', '96hr'],\n",
      " |      ...    \"value\": [10, 20, 30, 40, 50]\n",
      " |      ... })\n",
      " |      >>> df\n",
      " |          time  value\n",
      " |      0    0hr     10\n",
      " |      1  128hr     20\n",
      " |      2   72hr     30\n",
      " |      3   48hr     40\n",
      " |      4   96hr     50\n",
      " |      >>> from natsort import index_natsorted\n",
      " |      >>> df.sort_values(\n",
      " |      ...     by=\"time\",\n",
      " |      ...     key=lambda x: np.argsort(index_natsorted(df[\"time\"]))\n",
      " |      ... )\n",
      " |          time  value\n",
      " |      0    0hr     10\n",
      " |      3   48hr     40\n",
      " |      2   72hr     30\n",
      " |      4   96hr     50\n",
      " |      1  128hr     20\n",
      " |\n",
      " |  stack(self, level: 'IndexLabel' = -1, dropna: 'bool | lib.NoDefault' = <no_default>, sort: 'bool | lib.NoDefault' = <no_default>, future_stack: 'bool' = False)\n",
      " |      Stack the prescribed level(s) from columns to index.\n",
      " |\n",
      " |      Return a reshaped DataFrame or Series having a multi-level\n",
      " |      index with one or more new inner-most levels compared to the current\n",
      " |      DataFrame. The new inner-most levels are created by pivoting the\n",
      " |      columns of the current dataframe:\n",
      " |\n",
      " |        - if the columns have a single level, the output is a Series;\n",
      " |        - if the columns have multiple levels, the new index\n",
      " |          level(s) is (are) taken from the prescribed level(s) and\n",
      " |          the output is a DataFrame.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      level : int, str, list, default -1\n",
      " |          Level(s) to stack from the column axis onto the index\n",
      " |          axis, defined as one index or label, or a list of indices\n",
      " |          or labels.\n",
      " |      dropna : bool, default True\n",
      " |          Whether to drop rows in the resulting Frame/Series with\n",
      " |          missing values. Stacking a column level onto the index\n",
      " |          axis can create combinations of index and column values\n",
      " |          that are missing from the original dataframe. See Examples\n",
      " |          section.\n",
      " |      sort : bool, default True\n",
      " |          Whether to sort the levels of the resulting MultiIndex.\n",
      " |      future_stack : bool, default False\n",
      " |          Whether to use the new implementation that will replace the current\n",
      " |          implementation in pandas 3.0. When True, dropna and sort have no impact\n",
      " |          on the result and must remain unspecified. See :ref:`pandas 2.1.0 Release\n",
      " |          notes <whatsnew_210.enhancements.new_stack>` for more details.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame or Series\n",
      " |          Stacked dataframe or series.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.unstack : Unstack prescribed level(s) from index axis\n",
      " |           onto column axis.\n",
      " |      DataFrame.pivot : Reshape dataframe from long format to wide\n",
      " |           format.\n",
      " |      DataFrame.pivot_table : Create a spreadsheet-style pivot table\n",
      " |           as a DataFrame.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      The function is named by analogy with a collection of books\n",
      " |      being reorganized from being side by side on a horizontal\n",
      " |      position (the columns of the dataframe) to being stacked\n",
      " |      vertically on top of each other (in the index of the\n",
      " |      dataframe).\n",
      " |\n",
      " |      Reference :ref:`the user guide <reshaping.stacking>` for more examples.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      **Single level columns**\n",
      " |\n",
      " |      >>> df_single_level_cols = pd.DataFrame([[0, 1], [2, 3]],\n",
      " |      ...                                     index=['cat', 'dog'],\n",
      " |      ...                                     columns=['weight', 'height'])\n",
      " |\n",
      " |      Stacking a dataframe with a single level column axis returns a Series:\n",
      " |\n",
      " |      >>> df_single_level_cols\n",
      " |           weight height\n",
      " |      cat       0      1\n",
      " |      dog       2      3\n",
      " |      >>> df_single_level_cols.stack(future_stack=True)\n",
      " |      cat  weight    0\n",
      " |           height    1\n",
      " |      dog  weight    2\n",
      " |           height    3\n",
      " |      dtype: int64\n",
      " |\n",
      " |      **Multi level columns: simple case**\n",
      " |\n",
      " |      >>> multicol1 = pd.MultiIndex.from_tuples([('weight', 'kg'),\n",
      " |      ...                                        ('weight', 'pounds')])\n",
      " |      >>> df_multi_level_cols1 = pd.DataFrame([[1, 2], [2, 4]],\n",
      " |      ...                                     index=['cat', 'dog'],\n",
      " |      ...                                     columns=multicol1)\n",
      " |\n",
      " |      Stacking a dataframe with a multi-level column axis:\n",
      " |\n",
      " |      >>> df_multi_level_cols1\n",
      " |           weight\n",
      " |               kg    pounds\n",
      " |      cat       1        2\n",
      " |      dog       2        4\n",
      " |      >>> df_multi_level_cols1.stack(future_stack=True)\n",
      " |                  weight\n",
      " |      cat kg           1\n",
      " |          pounds       2\n",
      " |      dog kg           2\n",
      " |          pounds       4\n",
      " |\n",
      " |      **Missing values**\n",
      " |\n",
      " |      >>> multicol2 = pd.MultiIndex.from_tuples([('weight', 'kg'),\n",
      " |      ...                                        ('height', 'm')])\n",
      " |      >>> df_multi_level_cols2 = pd.DataFrame([[1.0, 2.0], [3.0, 4.0]],\n",
      " |      ...                                     index=['cat', 'dog'],\n",
      " |      ...                                     columns=multicol2)\n",
      " |\n",
      " |      It is common to have missing values when stacking a dataframe\n",
      " |      with multi-level columns, as the stacked dataframe typically\n",
      " |      has more values than the original dataframe. Missing values\n",
      " |      are filled with NaNs:\n",
      " |\n",
      " |      >>> df_multi_level_cols2\n",
      " |          weight height\n",
      " |              kg      m\n",
      " |      cat    1.0    2.0\n",
      " |      dog    3.0    4.0\n",
      " |      >>> df_multi_level_cols2.stack(future_stack=True)\n",
      " |              weight  height\n",
      " |      cat kg     1.0     NaN\n",
      " |          m      NaN     2.0\n",
      " |      dog kg     3.0     NaN\n",
      " |          m      NaN     4.0\n",
      " |\n",
      " |      **Prescribing the level(s) to be stacked**\n",
      " |\n",
      " |      The first parameter controls which level or levels are stacked:\n",
      " |\n",
      " |      >>> df_multi_level_cols2.stack(0, future_stack=True)\n",
      " |                   kg    m\n",
      " |      cat weight  1.0  NaN\n",
      " |          height  NaN  2.0\n",
      " |      dog weight  3.0  NaN\n",
      " |          height  NaN  4.0\n",
      " |      >>> df_multi_level_cols2.stack([0, 1], future_stack=True)\n",
      " |      cat  weight  kg    1.0\n",
      " |           height  m     2.0\n",
      " |      dog  weight  kg    3.0\n",
      " |           height  m     4.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |  std(self, axis: 'Axis | None' = 0, skipna: 'bool' = True, ddof: 'int' = 1, numeric_only: 'bool' = False, **kwargs)\n",
      " |      Return sample standard deviation over requested axis.\n",
      " |\n",
      " |      Normalized by N-1 by default. This can be changed using the ddof argument.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {index (0), columns (1)}\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |\n",
      " |          .. warning::\n",
      " |\n",
      " |              The behavior of DataFrame.std with ``axis=None`` is deprecated,\n",
      " |              in a future version this will reduce over both axes and return a scalar\n",
      " |              To retain the old behavior, pass axis=0 (or do not pass axis).\n",
      " |\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If an entire row/column is NA, the result\n",
      " |          will be NA.\n",
      " |      ddof : int, default 1\n",
      " |          Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\n",
      " |          where N represents the number of elements.\n",
      " |      numeric_only : bool, default False\n",
      " |          Include only float, int, boolean columns. Not implemented for Series.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame (if level specified)\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      To have the same behaviour as `numpy.std`, use `ddof=0` (instead of the\n",
      " |      default `ddof=1`)\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'person_id': [0, 1, 2, 3],\n",
      " |      ...                    'age': [21, 25, 62, 43],\n",
      " |      ...                    'height': [1.61, 1.87, 1.49, 2.01]}\n",
      " |      ...                   ).set_index('person_id')\n",
      " |      >>> df\n",
      " |                 age  height\n",
      " |      person_id\n",
      " |      0           21    1.61\n",
      " |      1           25    1.87\n",
      " |      2           62    1.49\n",
      " |      3           43    2.01\n",
      " |\n",
      " |      The standard deviation of the columns can be found as follows:\n",
      " |\n",
      " |      >>> df.std()\n",
      " |      age       18.786076\n",
      " |      height     0.237417\n",
      " |      dtype: float64\n",
      " |\n",
      " |      Alternatively, `ddof=0` can be set to normalize by N instead of N-1:\n",
      " |\n",
      " |      >>> df.std(ddof=0)\n",
      " |      age       16.269219\n",
      " |      height     0.205609\n",
      " |      dtype: float64\n",
      " |\n",
      " |  sub(self, other, axis: 'Axis' = 'columns', level=None, fill_value=None) -> 'DataFrame'\n",
      " |      Get Subtraction of dataframe and other, element-wise (binary operator `sub`).\n",
      " |\n",
      " |      Equivalent to ``dataframe - other``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `rsub`.\n",
      " |\n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `floordiv`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, dict or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns.\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |\n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |\n",
      " |      >>> df + 1\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      >>> df.add(1)\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      Divide by constant with reverse version.\n",
      " |\n",
      " |      >>> df.div(10)\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |\n",
      " |      >>> df.rdiv(10)\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |\n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |\n",
      " |      >>> df - [1, 2]\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub([1, 2], axis='columns')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |\n",
      " |      Multiply a dictionary by axis.\n",
      " |\n",
      " |      >>> df.mul({'angles': 0, 'degrees': 2})\n",
      " |                  angles  degrees\n",
      " |      circle           0      720\n",
      " |      triangle         0      360\n",
      " |      rectangle        0      720\n",
      " |\n",
      " |      >>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n",
      " |                  angles  degrees\n",
      " |      circle           0        0\n",
      " |      triangle         6      360\n",
      " |      rectangle       12     1080\n",
      " |\n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |\n",
      " |      >>> df * other\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |\n",
      " |      >>> df.mul(other, fill_value=0)\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |\n",
      " |      Divide by a MultiIndex by level.\n",
      " |\n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |\n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |\n",
      " |  subtract = sub(self, other, axis: 'Axis' = 'columns', level=None, fill_value=None) -> 'DataFrame'\n",
      " |\n",
      " |  sum(self, axis: 'Axis | None' = 0, skipna: 'bool' = True, numeric_only: 'bool' = False, min_count: 'int' = 0, **kwargs)\n",
      " |      Return the sum of the values over the requested axis.\n",
      " |\n",
      " |      This is equivalent to the method ``numpy.sum``.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {index (0), columns (1)}\n",
      " |          Axis for the function to be applied on.\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |\n",
      " |          .. warning::\n",
      " |\n",
      " |              The behavior of DataFrame.sum with ``axis=None`` is deprecated,\n",
      " |              in a future version this will reduce over both axes and return a scalar\n",
      " |              To retain the old behavior, pass axis=0 (or do not pass axis).\n",
      " |\n",
      " |          .. versionadded:: 2.0.0\n",
      " |\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values when computing the result.\n",
      " |      numeric_only : bool, default False\n",
      " |          Include only float, int, boolean columns. Not implemented for Series.\n",
      " |\n",
      " |      min_count : int, default 0\n",
      " |          The required number of valid values to perform the operation. If fewer than\n",
      " |          ``min_count`` non-NA values are present the result will be NA.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to be passed to the function.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or scalar\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.sum : Return the sum.\n",
      " |      Series.min : Return the minimum.\n",
      " |      Series.max : Return the maximum.\n",
      " |      Series.idxmin : Return the index of the minimum.\n",
      " |      Series.idxmax : Return the index of the maximum.\n",
      " |      DataFrame.sum : Return the sum over the requested axis.\n",
      " |      DataFrame.min : Return the minimum over the requested axis.\n",
      " |      DataFrame.max : Return the maximum over the requested axis.\n",
      " |      DataFrame.idxmin : Return the index of the minimum over the requested axis.\n",
      " |      DataFrame.idxmax : Return the index of the maximum over the requested axis.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> idx = pd.MultiIndex.from_arrays([\n",
      " |      ...     ['warm', 'warm', 'cold', 'cold'],\n",
      " |      ...     ['dog', 'falcon', 'fish', 'spider']],\n",
      " |      ...     names=['blooded', 'animal'])\n",
      " |      >>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n",
      " |      >>> s\n",
      " |      blooded  animal\n",
      " |      warm     dog       4\n",
      " |               falcon    2\n",
      " |      cold     fish      0\n",
      " |               spider    8\n",
      " |      Name: legs, dtype: int64\n",
      " |\n",
      " |      >>> s.sum()\n",
      " |      14\n",
      " |\n",
      " |      By default, the sum of an empty or all-NA Series is ``0``.\n",
      " |\n",
      " |      >>> pd.Series([], dtype=\"float64\").sum()  # min_count=0 is the default\n",
      " |      0.0\n",
      " |\n",
      " |      This can be controlled with the ``min_count`` parameter. For example, if\n",
      " |      you'd like the sum of an empty series to be NaN, pass ``min_count=1``.\n",
      " |\n",
      " |      >>> pd.Series([], dtype=\"float64\").sum(min_count=1)\n",
      " |      nan\n",
      " |\n",
      " |      Thanks to the ``skipna`` parameter, ``min_count`` handles all-NA and\n",
      " |      empty series identically.\n",
      " |\n",
      " |      >>> pd.Series([np.nan]).sum()\n",
      " |      0.0\n",
      " |\n",
      " |      >>> pd.Series([np.nan]).sum(min_count=1)\n",
      " |      nan\n",
      " |\n",
      " |  swaplevel(self, i: 'Axis' = -2, j: 'Axis' = -1, axis: 'Axis' = 0) -> 'DataFrame'\n",
      " |      Swap levels i and j in a :class:`MultiIndex`.\n",
      " |\n",
      " |      Default is to swap the two innermost levels of the index.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      i, j : int or str\n",
      " |          Levels of the indices to be swapped. Can pass level name as string.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |                  The axis to swap levels on. 0 or 'index' for row-wise, 1 or\n",
      " |                  'columns' for column-wise.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          DataFrame with levels swapped in MultiIndex.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame(\n",
      " |      ...     {\"Grade\": [\"A\", \"B\", \"A\", \"C\"]},\n",
      " |      ...     index=[\n",
      " |      ...         [\"Final exam\", \"Final exam\", \"Coursework\", \"Coursework\"],\n",
      " |      ...         [\"History\", \"Geography\", \"History\", \"Geography\"],\n",
      " |      ...         [\"January\", \"February\", \"March\", \"April\"],\n",
      " |      ...     ],\n",
      " |      ... )\n",
      " |      >>> df\n",
      " |                                          Grade\n",
      " |      Final exam  History     January      A\n",
      " |                  Geography   February     B\n",
      " |      Coursework  History     March        A\n",
      " |                  Geography   April        C\n",
      " |\n",
      " |      In the following example, we will swap the levels of the indices.\n",
      " |      Here, we will swap the levels column-wise, but levels can be swapped row-wise\n",
      " |      in a similar manner. Note that column-wise is the default behaviour.\n",
      " |      By not supplying any arguments for i and j, we swap the last and second to\n",
      " |      last indices.\n",
      " |\n",
      " |      >>> df.swaplevel()\n",
      " |                                          Grade\n",
      " |      Final exam  January     History         A\n",
      " |                  February    Geography       B\n",
      " |      Coursework  March       History         A\n",
      " |                  April       Geography       C\n",
      " |\n",
      " |      By supplying one argument, we can choose which index to swap the last\n",
      " |      index with. We can for example swap the first index with the last one as\n",
      " |      follows.\n",
      " |\n",
      " |      >>> df.swaplevel(0)\n",
      " |                                          Grade\n",
      " |      January     History     Final exam      A\n",
      " |      February    Geography   Final exam      B\n",
      " |      March       History     Coursework      A\n",
      " |      April       Geography   Coursework      C\n",
      " |\n",
      " |      We can also define explicitly which indices we want to swap by supplying values\n",
      " |      for both i and j. Here, we for example swap the first and second indices.\n",
      " |\n",
      " |      >>> df.swaplevel(0, 1)\n",
      " |                                          Grade\n",
      " |      History     Final exam  January         A\n",
      " |      Geography   Final exam  February        B\n",
      " |      History     Coursework  March           A\n",
      " |      Geography   Coursework  April           C\n",
      " |\n",
      " |  to_dict(self, orient: \"Literal['dict', 'list', 'series', 'split', 'tight', 'records', 'index']\" = 'dict', *, into: 'type[MutableMappingT] | MutableMappingT' = <class 'dict'>, index: 'bool' = True) -> 'MutableMappingT | list[MutableMappingT]'\n",
      " |      Convert the DataFrame to a dictionary.\n",
      " |\n",
      " |      The type of the key-value pairs can be customized with the parameters\n",
      " |      (see below).\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      orient : str {'dict', 'list', 'series', 'split', 'tight', 'records', 'index'}\n",
      " |          Determines the type of the values of the dictionary.\n",
      " |\n",
      " |          - 'dict' (default) : dict like {column -> {index -> value}}\n",
      " |          - 'list' : dict like {column -> [values]}\n",
      " |          - 'series' : dict like {column -> Series(values)}\n",
      " |          - 'split' : dict like\n",
      " |            {'index' -> [index], 'columns' -> [columns], 'data' -> [values]}\n",
      " |          - 'tight' : dict like\n",
      " |            {'index' -> [index], 'columns' -> [columns], 'data' -> [values],\n",
      " |            'index_names' -> [index.names], 'column_names' -> [column.names]}\n",
      " |          - 'records' : list like\n",
      " |            [{column -> value}, ... , {column -> value}]\n",
      " |          - 'index' : dict like {index -> {column -> value}}\n",
      " |\n",
      " |          .. versionadded:: 1.4.0\n",
      " |              'tight' as an allowed value for the ``orient`` argument\n",
      " |\n",
      " |      into : class, default dict\n",
      " |          The collections.abc.MutableMapping subclass used for all Mappings\n",
      " |          in the return value.  Can be the actual class or an empty\n",
      " |          instance of the mapping type you want.  If you want a\n",
      " |          collections.defaultdict, you must pass it initialized.\n",
      " |\n",
      " |      index : bool, default True\n",
      " |          Whether to include the index item (and index_names item if `orient`\n",
      " |          is 'tight') in the returned dictionary. Can only be ``False``\n",
      " |          when `orient` is 'split' or 'tight'.\n",
      " |\n",
      " |          .. versionadded:: 2.0.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict, list or collections.abc.MutableMapping\n",
      " |          Return a collections.abc.MutableMapping object representing the\n",
      " |          DataFrame. The resulting transformation depends on the `orient`\n",
      " |          parameter.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.from_dict: Create a DataFrame from a dictionary.\n",
      " |      DataFrame.to_json: Convert a DataFrame to JSON format.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'col1': [1, 2],\n",
      " |      ...                    'col2': [0.5, 0.75]},\n",
      " |      ...                   index=['row1', 'row2'])\n",
      " |      >>> df\n",
      " |            col1  col2\n",
      " |      row1     1  0.50\n",
      " |      row2     2  0.75\n",
      " |      >>> df.to_dict()\n",
      " |      {'col1': {'row1': 1, 'row2': 2}, 'col2': {'row1': 0.5, 'row2': 0.75}}\n",
      " |\n",
      " |      You can specify the return orientation.\n",
      " |\n",
      " |      >>> df.to_dict('series')\n",
      " |      {'col1': row1    1\n",
      " |               row2    2\n",
      " |      Name: col1, dtype: int64,\n",
      " |      'col2': row1    0.50\n",
      " |              row2    0.75\n",
      " |      Name: col2, dtype: float64}\n",
      " |\n",
      " |      >>> df.to_dict('split')\n",
      " |      {'index': ['row1', 'row2'], 'columns': ['col1', 'col2'],\n",
      " |       'data': [[1, 0.5], [2, 0.75]]}\n",
      " |\n",
      " |      >>> df.to_dict('records')\n",
      " |      [{'col1': 1, 'col2': 0.5}, {'col1': 2, 'col2': 0.75}]\n",
      " |\n",
      " |      >>> df.to_dict('index')\n",
      " |      {'row1': {'col1': 1, 'col2': 0.5}, 'row2': {'col1': 2, 'col2': 0.75}}\n",
      " |\n",
      " |      >>> df.to_dict('tight')\n",
      " |      {'index': ['row1', 'row2'], 'columns': ['col1', 'col2'],\n",
      " |       'data': [[1, 0.5], [2, 0.75]], 'index_names': [None], 'column_names': [None]}\n",
      " |\n",
      " |      You can also specify the mapping type.\n",
      " |\n",
      " |      >>> from collections import OrderedDict, defaultdict\n",
      " |      >>> df.to_dict(into=OrderedDict)\n",
      " |      OrderedDict([('col1', OrderedDict([('row1', 1), ('row2', 2)])),\n",
      " |                   ('col2', OrderedDict([('row1', 0.5), ('row2', 0.75)]))])\n",
      " |\n",
      " |      If you want a `defaultdict`, you need to initialize it:\n",
      " |\n",
      " |      >>> dd = defaultdict(list)\n",
      " |      >>> df.to_dict('records', into=dd)\n",
      " |      [defaultdict(<class 'list'>, {'col1': 1, 'col2': 0.5}),\n",
      " |       defaultdict(<class 'list'>, {'col1': 2, 'col2': 0.75})]\n",
      " |\n",
      " |  to_gbq(self, destination_table: 'str', *, project_id: 'str | None' = None, chunksize: 'int | None' = None, reauth: 'bool' = False, if_exists: 'ToGbqIfexist' = 'fail', auth_local_webserver: 'bool' = True, table_schema: 'list[dict[str, str]] | None' = None, location: 'str | None' = None, progress_bar: 'bool' = True, credentials=None) -> 'None'\n",
      " |      Write a DataFrame to a Google BigQuery table.\n",
      " |\n",
      " |      .. deprecated:: 2.2.0\n",
      " |\n",
      " |         Please use ``pandas_gbq.to_gbq`` instead.\n",
      " |\n",
      " |      This function requires the `pandas-gbq package\n",
      " |      <https://pandas-gbq.readthedocs.io>`__.\n",
      " |\n",
      " |      See the `How to authenticate with Google BigQuery\n",
      " |      <https://pandas-gbq.readthedocs.io/en/latest/howto/authentication.html>`__\n",
      " |      guide for authentication instructions.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      destination_table : str\n",
      " |          Name of table to be written, in the form ``dataset.tablename``.\n",
      " |      project_id : str, optional\n",
      " |          Google BigQuery Account project ID. Optional when available from\n",
      " |          the environment.\n",
      " |      chunksize : int, optional\n",
      " |          Number of rows to be inserted in each chunk from the dataframe.\n",
      " |          Set to ``None`` to load the whole dataframe at once.\n",
      " |      reauth : bool, default False\n",
      " |          Force Google BigQuery to re-authenticate the user. This is useful\n",
      " |          if multiple accounts are used.\n",
      " |      if_exists : str, default 'fail'\n",
      " |          Behavior when the destination table exists. Value can be one of:\n",
      " |\n",
      " |          ``'fail'``\n",
      " |              If table exists raise pandas_gbq.gbq.TableCreationError.\n",
      " |          ``'replace'``\n",
      " |              If table exists, drop it, recreate it, and insert data.\n",
      " |          ``'append'``\n",
      " |              If table exists, insert data. Create if does not exist.\n",
      " |      auth_local_webserver : bool, default True\n",
      " |          Use the `local webserver flow`_ instead of the `console flow`_\n",
      " |          when getting user credentials.\n",
      " |\n",
      " |          .. _local webserver flow:\n",
      " |              https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_local_server\n",
      " |          .. _console flow:\n",
      " |              https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_console\n",
      " |\n",
      " |          *New in version 0.2.0 of pandas-gbq*.\n",
      " |\n",
      " |          .. versionchanged:: 1.5.0\n",
      " |             Default value is changed to ``True``. Google has deprecated the\n",
      " |             ``auth_local_webserver = False`` `\"out of band\" (copy-paste)\n",
      " |             flow\n",
      " |             <https://developers.googleblog.com/2022/02/making-oauth-flows-safer.html?m=1#disallowed-oob>`_.\n",
      " |      table_schema : list of dicts, optional\n",
      " |          List of BigQuery table fields to which according DataFrame\n",
      " |          columns conform to, e.g. ``[{'name': 'col1', 'type':\n",
      " |          'STRING'},...]``. If schema is not provided, it will be\n",
      " |          generated according to dtypes of DataFrame columns. See\n",
      " |          BigQuery API documentation on available names of a field.\n",
      " |\n",
      " |          *New in version 0.3.1 of pandas-gbq*.\n",
      " |      location : str, optional\n",
      " |          Location where the load job should run. See the `BigQuery locations\n",
      " |          documentation\n",
      " |          <https://cloud.google.com/bigquery/docs/dataset-locations>`__ for a\n",
      " |          list of available locations. The location must match that of the\n",
      " |          target dataset.\n",
      " |\n",
      " |          *New in version 0.5.0 of pandas-gbq*.\n",
      " |      progress_bar : bool, default True\n",
      " |          Use the library `tqdm` to show the progress bar for the upload,\n",
      " |          chunk by chunk.\n",
      " |\n",
      " |          *New in version 0.5.0 of pandas-gbq*.\n",
      " |      credentials : google.auth.credentials.Credentials, optional\n",
      " |          Credentials for accessing Google APIs. Use this parameter to\n",
      " |          override default credentials, such as to use Compute Engine\n",
      " |          :class:`google.auth.compute_engine.Credentials` or Service\n",
      " |          Account :class:`google.oauth2.service_account.Credentials`\n",
      " |          directly.\n",
      " |\n",
      " |          *New in version 0.8.0 of pandas-gbq*.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      pandas_gbq.to_gbq : This function in the pandas-gbq library.\n",
      " |      read_gbq : Read a DataFrame from Google BigQuery.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Example taken from `Google BigQuery documentation\n",
      " |      <https://cloud.google.com/bigquery/docs/samples/bigquery-pandas-gbq-to-gbq-simple>`_\n",
      " |\n",
      " |      >>> project_id = \"my-project\"\n",
      " |      >>> table_id = 'my_dataset.my_table'\n",
      " |      >>> df = pd.DataFrame({\n",
      " |      ...                   \"my_string\": [\"a\", \"b\", \"c\"],\n",
      " |      ...                   \"my_int64\": [1, 2, 3],\n",
      " |      ...                   \"my_float64\": [4.0, 5.0, 6.0],\n",
      " |      ...                   \"my_bool1\": [True, False, True],\n",
      " |      ...                   \"my_bool2\": [False, True, False],\n",
      " |      ...                   \"my_dates\": pd.date_range(\"now\", periods=3),\n",
      " |      ...                   }\n",
      " |      ...                   )\n",
      " |\n",
      " |      >>> df.to_gbq(table_id, project_id=project_id)  # doctest: +SKIP\n",
      " |\n",
      " |  to_html(self, buf: 'FilePath | WriteBuffer[str] | None' = None, *, columns: 'Axes | None' = None, col_space: 'ColspaceArgType | None' = None, header: 'bool' = True, index: 'bool' = True, na_rep: 'str' = 'NaN', formatters: 'FormattersType | None' = None, float_format: 'FloatFormatType | None' = None, sparsify: 'bool | None' = None, index_names: 'bool' = True, justify: 'str | None' = None, max_rows: 'int | None' = None, max_cols: 'int | None' = None, show_dimensions: 'bool | str' = False, decimal: 'str' = '.', bold_rows: 'bool' = True, classes: 'str | list | tuple | None' = None, escape: 'bool' = True, notebook: 'bool' = False, border: 'int | bool | None' = None, table_id: 'str | None' = None, render_links: 'bool' = False, encoding: 'str | None' = None) -> 'str | None'\n",
      " |      Render a DataFrame as an HTML table.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      buf : str, Path or StringIO-like, optional, default None\n",
      " |          Buffer to write to. If None, the output is returned as a string.\n",
      " |      columns : array-like, optional, default None\n",
      " |          The subset of columns to write. Writes all columns by default.\n",
      " |      col_space : str or int, list or dict of int or str, optional\n",
      " |          The minimum width of each column in CSS length units.  An int is assumed to be px units..\n",
      " |      header : bool, optional\n",
      " |          Whether to print column labels, default True.\n",
      " |      index : bool, optional, default True\n",
      " |          Whether to print index (row) labels.\n",
      " |      na_rep : str, optional, default 'NaN'\n",
      " |          String representation of ``NaN`` to use.\n",
      " |      formatters : list, tuple or dict of one-param. functions, optional\n",
      " |          Formatter functions to apply to columns' elements by position or\n",
      " |          name.\n",
      " |          The result of each function must be a unicode string.\n",
      " |          List/tuple must be of length equal to the number of columns.\n",
      " |      float_format : one-parameter function, optional, default None\n",
      " |          Formatter function to apply to columns' elements if they are\n",
      " |          floats. This function must return a unicode string and will be\n",
      " |          applied only to the non-``NaN`` elements, with ``NaN`` being\n",
      " |          handled by ``na_rep``.\n",
      " |      sparsify : bool, optional, default True\n",
      " |          Set to False for a DataFrame with a hierarchical index to print\n",
      " |          every multiindex key at each row.\n",
      " |      index_names : bool, optional, default True\n",
      " |          Prints the names of the indexes.\n",
      " |      justify : str, default None\n",
      " |          How to justify the column labels. If None uses the option from\n",
      " |          the print configuration (controlled by set_option), 'right' out\n",
      " |          of the box. Valid values are\n",
      " |\n",
      " |          * left\n",
      " |          * right\n",
      " |          * center\n",
      " |          * justify\n",
      " |          * justify-all\n",
      " |          * start\n",
      " |          * end\n",
      " |          * inherit\n",
      " |          * match-parent\n",
      " |          * initial\n",
      " |          * unset.\n",
      " |      max_rows : int, optional\n",
      " |          Maximum number of rows to display in the console.\n",
      " |      max_cols : int, optional\n",
      " |          Maximum number of columns to display in the console.\n",
      " |      show_dimensions : bool, default False\n",
      " |          Display DataFrame dimensions (number of rows by number of columns).\n",
      " |      decimal : str, default '.'\n",
      " |          Character recognized as decimal separator, e.g. ',' in Europe.\n",
      " |\n",
      " |      bold_rows : bool, default True\n",
      " |          Make the row labels bold in the output.\n",
      " |      classes : str or list or tuple, default None\n",
      " |          CSS class(es) to apply to the resulting html table.\n",
      " |      escape : bool, default True\n",
      " |          Convert the characters <, >, and & to HTML-safe sequences.\n",
      " |      notebook : {True, False}, default False\n",
      " |          Whether the generated HTML is for IPython Notebook.\n",
      " |      border : int\n",
      " |          A ``border=border`` attribute is included in the opening\n",
      " |          `<table>` tag. Default ``pd.options.display.html.border``.\n",
      " |      table_id : str, optional\n",
      " |          A css id is included in the opening `<table>` tag if specified.\n",
      " |      render_links : bool, default False\n",
      " |          Convert URLs to HTML links.\n",
      " |      encoding : str, default \"utf-8\"\n",
      " |          Set character encoding.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      str or None\n",
      " |          If buf is None, returns the result as a string. Otherwise returns\n",
      " |          None.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      to_string : Convert DataFrame to a string.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame(data={'col1': [1, 2], 'col2': [4, 3]})\n",
      " |      >>> html_string = '''<table border=\"1\" class=\"dataframe\">\n",
      " |      ...   <thead>\n",
      " |      ...     <tr style=\"text-align: right;\">\n",
      " |      ...       <th></th>\n",
      " |      ...       <th>col1</th>\n",
      " |      ...       <th>col2</th>\n",
      " |      ...     </tr>\n",
      " |      ...   </thead>\n",
      " |      ...   <tbody>\n",
      " |      ...     <tr>\n",
      " |      ...       <th>0</th>\n",
      " |      ...       <td>1</td>\n",
      " |      ...       <td>4</td>\n",
      " |      ...     </tr>\n",
      " |      ...     <tr>\n",
      " |      ...       <th>1</th>\n",
      " |      ...       <td>2</td>\n",
      " |      ...       <td>3</td>\n",
      " |      ...     </tr>\n",
      " |      ...   </tbody>\n",
      " |      ... </table>'''\n",
      " |      >>> assert html_string == df.to_html()\n",
      " |\n",
      " |  to_markdown(self, buf: 'FilePath | WriteBuffer[str] | None' = None, *, mode: 'str' = 'wt', index: 'bool' = True, storage_options: 'StorageOptions | None' = None, **kwargs) -> 'str | None'\n",
      " |      Print DataFrame in Markdown-friendly format.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      buf : str, Path or StringIO-like, optional, default None\n",
      " |          Buffer to write to. If None, the output is returned as a string.\n",
      " |      mode : str, optional\n",
      " |          Mode in which file is opened, \"wt\" by default.\n",
      " |      index : bool, optional, default True\n",
      " |          Add index (row) labels.\n",
      " |\n",
      " |      storage_options : dict, optional\n",
      " |          Extra options that make sense for a particular storage connection, e.g.\n",
      " |          host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      " |          are forwarded to ``urllib.request.Request`` as header options. For other\n",
      " |          URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      " |          forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      " |          details, and for more examples on storage options refer `here\n",
      " |          <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      " |          highlight=storage_options#reading-writing-remote-files>`_.\n",
      " |\n",
      " |      **kwargs\n",
      " |          These parameters will be passed to `tabulate                 <https://pypi.org/project/tabulate>`_.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          DataFrame in Markdown-friendly format.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Requires the `tabulate <https://pypi.org/project/tabulate>`_ package.\n",
      " |\n",
      " |      Examples\n",
      " |              --------\n",
      " |              >>> df = pd.DataFrame(\n",
      " |              ...     data={\"animal_1\": [\"elk\", \"pig\"], \"animal_2\": [\"dog\", \"quetzal\"]}\n",
      " |              ... )\n",
      " |              >>> print(df.to_markdown())\n",
      " |              |    | animal_1   | animal_2   |\n",
      " |              |---:|:-----------|:-----------|\n",
      " |              |  0 | elk        | dog        |\n",
      " |              |  1 | pig        | quetzal    |\n",
      " |\n",
      " |              Output markdown with a tabulate option.\n",
      " |\n",
      " |              >>> print(df.to_markdown(tablefmt=\"grid\"))\n",
      " |              +----+------------+------------+\n",
      " |              |    | animal_1   | animal_2   |\n",
      " |              +====+============+============+\n",
      " |              |  0 | elk        | dog        |\n",
      " |              +----+------------+------------+\n",
      " |              |  1 | pig        | quetzal    |\n",
      " |              +----+------------+------------+\n",
      " |\n",
      " |  to_numpy(self, dtype: 'npt.DTypeLike | None' = None, copy: 'bool' = False, na_value: 'object' = <no_default>) -> 'np.ndarray'\n",
      " |      Convert the DataFrame to a NumPy array.\n",
      " |\n",
      " |      By default, the dtype of the returned array will be the common NumPy\n",
      " |      dtype of all types in the DataFrame. For example, if the dtypes are\n",
      " |      ``float16`` and ``float32``, the results dtype will be ``float32``.\n",
      " |      This may require copying data and coercing values, which may be\n",
      " |      expensive.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtype : str or numpy.dtype, optional\n",
      " |          The dtype to pass to :meth:`numpy.asarray`.\n",
      " |      copy : bool, default False\n",
      " |          Whether to ensure that the returned value is not a view on\n",
      " |          another array. Note that ``copy=False`` does not *ensure* that\n",
      " |          ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\n",
      " |          a copy is made, even if not strictly necessary.\n",
      " |      na_value : Any, optional\n",
      " |          The value to use for missing values. The default value depends\n",
      " |          on `dtype` and the dtypes of the DataFrame columns.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.to_numpy : Similar method for Series.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]}).to_numpy()\n",
      " |      array([[1, 3],\n",
      " |             [2, 4]])\n",
      " |\n",
      " |      With heterogeneous data, the lowest common type will have to\n",
      " |      be used.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.5]})\n",
      " |      >>> df.to_numpy()\n",
      " |      array([[1. , 3. ],\n",
      " |             [2. , 4.5]])\n",
      " |\n",
      " |      For a mix of numeric and non-numeric types, the output array will\n",
      " |      have object dtype.\n",
      " |\n",
      " |      >>> df['C'] = pd.date_range('2000', periods=2)\n",
      " |      >>> df.to_numpy()\n",
      " |      array([[1, 3.0, Timestamp('2000-01-01 00:00:00')],\n",
      " |             [2, 4.5, Timestamp('2000-01-02 00:00:00')]], dtype=object)\n",
      " |\n",
      " |  to_orc(self, path: 'FilePath | WriteBuffer[bytes] | None' = None, *, engine: \"Literal['pyarrow']\" = 'pyarrow', index: 'bool | None' = None, engine_kwargs: 'dict[str, Any] | None' = None) -> 'bytes | None'\n",
      " |      Write a DataFrame to the ORC format.\n",
      " |\n",
      " |      .. versionadded:: 1.5.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str, file-like object or None, default None\n",
      " |          If a string, it will be used as Root Directory path\n",
      " |          when writing a partitioned dataset. By file-like object,\n",
      " |          we refer to objects with a write() method, such as a file handle\n",
      " |          (e.g. via builtin open function). If path is None,\n",
      " |          a bytes object is returned.\n",
      " |      engine : {'pyarrow'}, default 'pyarrow'\n",
      " |          ORC library to use.\n",
      " |      index : bool, optional\n",
      " |          If ``True``, include the dataframe's index(es) in the file output.\n",
      " |          If ``False``, they will not be written to the file.\n",
      " |          If ``None``, similar to ``infer`` the dataframe's index(es)\n",
      " |          will be saved. However, instead of being saved as values,\n",
      " |          the RangeIndex will be stored as a range in the metadata so it\n",
      " |          doesn't require much space and is faster. Other indexes will\n",
      " |          be included as columns in the file output.\n",
      " |      engine_kwargs : dict[str, Any] or None, default None\n",
      " |          Additional keyword arguments passed to :func:`pyarrow.orc.write_table`.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      bytes if no path argument is provided else None\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      NotImplementedError\n",
      " |          Dtype of one or more columns is category, unsigned integers, interval,\n",
      " |          period or sparse.\n",
      " |      ValueError\n",
      " |          engine is not pyarrow.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      read_orc : Read a ORC file.\n",
      " |      DataFrame.to_parquet : Write a parquet file.\n",
      " |      DataFrame.to_csv : Write a csv file.\n",
      " |      DataFrame.to_sql : Write to a sql table.\n",
      " |      DataFrame.to_hdf : Write to hdf.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      * Before using this function you should read the :ref:`user guide about\n",
      " |        ORC <io.orc>` and :ref:`install optional dependencies <install.warn_orc>`.\n",
      " |      * This function requires `pyarrow <https://arrow.apache.org/docs/python/>`_\n",
      " |        library.\n",
      " |      * For supported dtypes please refer to `supported ORC features in Arrow\n",
      " |        <https://arrow.apache.org/docs/cpp/orc.html#data-types>`__.\n",
      " |      * Currently timezones in datetime columns are not preserved when a\n",
      " |        dataframe is converted into ORC files.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame(data={'col1': [1, 2], 'col2': [4, 3]})\n",
      " |      >>> df.to_orc('df.orc')  # doctest: +SKIP\n",
      " |      >>> pd.read_orc('df.orc')  # doctest: +SKIP\n",
      " |         col1  col2\n",
      " |      0     1     4\n",
      " |      1     2     3\n",
      " |\n",
      " |      If you want to get a buffer to the orc content you can write it to io.BytesIO\n",
      " |\n",
      " |      >>> import io\n",
      " |      >>> b = io.BytesIO(df.to_orc())  # doctest: +SKIP\n",
      " |      >>> b.seek(0)  # doctest: +SKIP\n",
      " |      0\n",
      " |      >>> content = b.read()  # doctest: +SKIP\n",
      " |\n",
      " |  to_period(self, freq: 'Frequency | None' = None, axis: 'Axis' = 0, copy: 'bool | None' = None) -> 'DataFrame'\n",
      " |      Convert DataFrame from DatetimeIndex to PeriodIndex.\n",
      " |\n",
      " |      Convert DataFrame from DatetimeIndex to PeriodIndex with desired\n",
      " |      frequency (inferred from index if not passed).\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      freq : str, default\n",
      " |          Frequency of the PeriodIndex.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The axis to convert (the index by default).\n",
      " |      copy : bool, default True\n",
      " |          If False then underlying input data is not copied.\n",
      " |\n",
      " |          .. note::\n",
      " |              The `copy` keyword will change behavior in pandas 3.0.\n",
      " |              `Copy-on-Write\n",
      " |              <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n",
      " |              will be enabled by default, which means that all methods with a\n",
      " |              `copy` keyword will use a lazy copy mechanism to defer the copy and\n",
      " |              ignore the `copy` keyword. The `copy` keyword will be removed in a\n",
      " |              future version of pandas.\n",
      " |\n",
      " |              You can already get the future behavior and improvements through\n",
      " |              enabling copy on write ``pd.options.mode.copy_on_write = True``\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          The DataFrame has a PeriodIndex.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> idx = pd.to_datetime(\n",
      " |      ...     [\n",
      " |      ...         \"2001-03-31 00:00:00\",\n",
      " |      ...         \"2002-05-31 00:00:00\",\n",
      " |      ...         \"2003-08-31 00:00:00\",\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |\n",
      " |      >>> idx\n",
      " |      DatetimeIndex(['2001-03-31', '2002-05-31', '2003-08-31'],\n",
      " |      dtype='datetime64[ns]', freq=None)\n",
      " |\n",
      " |      >>> idx.to_period(\"M\")\n",
      " |      PeriodIndex(['2001-03', '2002-05', '2003-08'], dtype='period[M]')\n",
      " |\n",
      " |      For the yearly frequency\n",
      " |\n",
      " |      >>> idx.to_period(\"Y\")\n",
      " |      PeriodIndex(['2001', '2002', '2003'], dtype='period[Y-DEC]')\n",
      " |\n",
      " |  to_records(self, index: 'bool' = True, column_dtypes=None, index_dtypes=None) -> 'np.rec.recarray'\n",
      " |      Convert DataFrame to a NumPy record array.\n",
      " |\n",
      " |      Index will be included as the first field of the record array if\n",
      " |      requested.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      index : bool, default True\n",
      " |          Include index in resulting record array, stored in 'index'\n",
      " |          field or using the index label, if set.\n",
      " |      column_dtypes : str, type, dict, default None\n",
      " |          If a string or type, the data type to store all columns. If\n",
      " |          a dictionary, a mapping of column names and indices (zero-indexed)\n",
      " |          to specific data types.\n",
      " |      index_dtypes : str, type, dict, default None\n",
      " |          If a string or type, the data type to store all index levels. If\n",
      " |          a dictionary, a mapping of index level names and indices\n",
      " |          (zero-indexed) to specific data types.\n",
      " |\n",
      " |          This mapping is applied only if `index=True`.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.rec.recarray\n",
      " |          NumPy ndarray with the DataFrame labels as fields and each row\n",
      " |          of the DataFrame as entries.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.from_records: Convert structured or record ndarray\n",
      " |          to DataFrame.\n",
      " |      numpy.rec.recarray: An ndarray that allows field access using\n",
      " |          attributes, analogous to typed columns in a\n",
      " |          spreadsheet.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'A': [1, 2], 'B': [0.5, 0.75]},\n",
      " |      ...                   index=['a', 'b'])\n",
      " |      >>> df\n",
      " |         A     B\n",
      " |      a  1  0.50\n",
      " |      b  2  0.75\n",
      " |      >>> df.to_records()\n",
      " |      rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n",
      " |                dtype=[('index', 'O'), ('A', '<i8'), ('B', '<f8')])\n",
      " |\n",
      " |      If the DataFrame index has no label then the recarray field name\n",
      " |      is set to 'index'. If the index has a label then this is used as the\n",
      " |      field name:\n",
      " |\n",
      " |      >>> df.index = df.index.rename(\"I\")\n",
      " |      >>> df.to_records()\n",
      " |      rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n",
      " |                dtype=[('I', 'O'), ('A', '<i8'), ('B', '<f8')])\n",
      " |\n",
      " |      The index can be excluded from the record array:\n",
      " |\n",
      " |      >>> df.to_records(index=False)\n",
      " |      rec.array([(1, 0.5 ), (2, 0.75)],\n",
      " |                dtype=[('A', '<i8'), ('B', '<f8')])\n",
      " |\n",
      " |      Data types can be specified for the columns:\n",
      " |\n",
      " |      >>> df.to_records(column_dtypes={\"A\": \"int32\"})\n",
      " |      rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n",
      " |                dtype=[('I', 'O'), ('A', '<i4'), ('B', '<f8')])\n",
      " |\n",
      " |      As well as for the index:\n",
      " |\n",
      " |      >>> df.to_records(index_dtypes=\"<S2\")\n",
      " |      rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)],\n",
      " |                dtype=[('I', 'S2'), ('A', '<i8'), ('B', '<f8')])\n",
      " |\n",
      " |      >>> index_dtypes = f\"<S{df.index.str.len().max()}\"\n",
      " |      >>> df.to_records(index_dtypes=index_dtypes)\n",
      " |      rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)],\n",
      " |                dtype=[('I', 'S1'), ('A', '<i8'), ('B', '<f8')])\n",
      " |\n",
      " |  to_stata(self, path: 'FilePath | WriteBuffer[bytes]', *, convert_dates: 'dict[Hashable, str] | None' = None, write_index: 'bool' = True, byteorder: 'ToStataByteorder | None' = None, time_stamp: 'datetime.datetime | None' = None, data_label: 'str | None' = None, variable_labels: 'dict[Hashable, str] | None' = None, version: 'int | None' = 114, convert_strl: 'Sequence[Hashable] | None' = None, compression: 'CompressionOptions' = 'infer', storage_options: 'StorageOptions | None' = None, value_labels: 'dict[Hashable, dict[float, str]] | None' = None) -> 'None'\n",
      " |      Export DataFrame object to Stata dta format.\n",
      " |\n",
      " |      Writes the DataFrame to a Stata dataset file.\n",
      " |      \"dta\" files contain a Stata dataset.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str, path object, or buffer\n",
      " |          String, path object (implementing ``os.PathLike[str]``), or file-like\n",
      " |          object implementing a binary ``write()`` function.\n",
      " |\n",
      " |      convert_dates : dict\n",
      " |          Dictionary mapping columns containing datetime types to stata\n",
      " |          internal format to use when writing the dates. Options are 'tc',\n",
      " |          'td', 'tm', 'tw', 'th', 'tq', 'ty'. Column can be either an integer\n",
      " |          or a name. Datetime columns that do not have a conversion type\n",
      " |          specified will be converted to 'tc'. Raises NotImplementedError if\n",
      " |          a datetime column has timezone information.\n",
      " |      write_index : bool\n",
      " |          Write the index to Stata dataset.\n",
      " |      byteorder : str\n",
      " |          Can be \">\", \"<\", \"little\", or \"big\". default is `sys.byteorder`.\n",
      " |      time_stamp : datetime\n",
      " |          A datetime to use as file creation date.  Default is the current\n",
      " |          time.\n",
      " |      data_label : str, optional\n",
      " |          A label for the data set.  Must be 80 characters or smaller.\n",
      " |      variable_labels : dict\n",
      " |          Dictionary containing columns as keys and variable labels as\n",
      " |          values. Each label must be 80 characters or smaller.\n",
      " |      version : {114, 117, 118, 119, None}, default 114\n",
      " |          Version to use in the output dta file. Set to None to let pandas\n",
      " |          decide between 118 or 119 formats depending on the number of\n",
      " |          columns in the frame. Version 114 can be read by Stata 10 and\n",
      " |          later. Version 117 can be read by Stata 13 or later. Version 118\n",
      " |          is supported in Stata 14 and later. Version 119 is supported in\n",
      " |          Stata 15 and later. Version 114 limits string variables to 244\n",
      " |          characters or fewer while versions 117 and later allow strings\n",
      " |          with lengths up to 2,000,000 characters. Versions 118 and 119\n",
      " |          support Unicode characters, and version 119 supports more than\n",
      " |          32,767 variables.\n",
      " |\n",
      " |          Version 119 should usually only be used when the number of\n",
      " |          variables exceeds the capacity of dta format 118. Exporting\n",
      " |          smaller datasets in format 119 may have unintended consequences,\n",
      " |          and, as of November 2020, Stata SE cannot read version 119 files.\n",
      " |\n",
      " |      convert_strl : list, optional\n",
      " |          List of column names to convert to string columns to Stata StrL\n",
      " |          format. Only available if version is 117.  Storing strings in the\n",
      " |          StrL format can produce smaller dta files if strings have more than\n",
      " |          8 characters and values are repeated.\n",
      " |      compression : str or dict, default 'infer'\n",
      " |          For on-the-fly compression of the output data. If 'infer' and 'path' is\n",
      " |          path-like, then detect compression from the following extensions: '.gz',\n",
      " |          '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
      " |          (otherwise no compression).\n",
      " |          Set to ``None`` for no compression.\n",
      " |          Can also be a dict with key ``'method'`` set\n",
      " |          to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n",
      " |          other key-value pairs are forwarded to\n",
      " |          ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      " |          ``bz2.BZ2File``, ``zstandard.ZstdCompressor``, ``lzma.LZMAFile`` or\n",
      " |          ``tarfile.TarFile``, respectively.\n",
      " |          As an example, the following could be passed for faster compression and to create\n",
      " |          a reproducible gzip archive:\n",
      " |          ``compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}``.\n",
      " |\n",
      " |          .. versionadded:: 1.5.0\n",
      " |              Added support for `.tar` files.\n",
      " |\n",
      " |          .. versionchanged:: 1.4.0 Zstandard support.\n",
      " |\n",
      " |      storage_options : dict, optional\n",
      " |          Extra options that make sense for a particular storage connection, e.g.\n",
      " |          host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      " |          are forwarded to ``urllib.request.Request`` as header options. For other\n",
      " |          URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      " |          forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      " |          details, and for more examples on storage options refer `here\n",
      " |          <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      " |          highlight=storage_options#reading-writing-remote-files>`_.\n",
      " |\n",
      " |      value_labels : dict of dicts\n",
      " |          Dictionary containing columns as keys and dictionaries of column value\n",
      " |          to labels as values. Labels for a single variable must be 32,000\n",
      " |          characters or smaller.\n",
      " |\n",
      " |          .. versionadded:: 1.4.0\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      NotImplementedError\n",
      " |          * If datetimes contain timezone information\n",
      " |          * Column dtype is not representable in Stata\n",
      " |      ValueError\n",
      " |          * Columns listed in convert_dates are neither datetime64[ns]\n",
      " |            or datetime.datetime\n",
      " |          * Column listed in convert_dates is not in DataFrame\n",
      " |          * Categorical label contains more than 32,000 characters\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      read_stata : Import Stata data files.\n",
      " |      io.stata.StataWriter : Low-level writer for Stata data files.\n",
      " |      io.stata.StataWriter117 : Low-level writer for version 117 files.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'animal': ['falcon', 'parrot', 'falcon',\n",
      " |      ...                               'parrot'],\n",
      " |      ...                    'speed': [350, 18, 361, 15]})\n",
      " |      >>> df.to_stata('animals.dta')  # doctest: +SKIP\n",
      " |\n",
      " |  to_string(self, buf: 'FilePath | WriteBuffer[str] | None' = None, *, columns: 'Axes | None' = None, col_space: 'int | list[int] | dict[Hashable, int] | None' = None, header: 'bool | SequenceNotStr[str]' = True, index: 'bool' = True, na_rep: 'str' = 'NaN', formatters: 'fmt.FormattersType | None' = None, float_format: 'fmt.FloatFormatType | None' = None, sparsify: 'bool | None' = None, index_names: 'bool' = True, justify: 'str | None' = None, max_rows: 'int | None' = None, max_cols: 'int | None' = None, show_dimensions: 'bool' = False, decimal: 'str' = '.', line_width: 'int | None' = None, min_rows: 'int | None' = None, max_colwidth: 'int | None' = None, encoding: 'str | None' = None) -> 'str | None'\n",
      " |      Render a DataFrame to a console-friendly tabular output.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      buf : str, Path or StringIO-like, optional, default None\n",
      " |          Buffer to write to. If None, the output is returned as a string.\n",
      " |      columns : array-like, optional, default None\n",
      " |          The subset of columns to write. Writes all columns by default.\n",
      " |      col_space : int, list or dict of int, optional\n",
      " |          The minimum width of each column. If a list of ints is given every integers corresponds with one column. If a dict is given, the key references the column, while the value defines the space to use..\n",
      " |      header : bool or list of str, optional\n",
      " |          Write out the column names. If a list of columns is given, it is assumed to be aliases for the column names.\n",
      " |      index : bool, optional, default True\n",
      " |          Whether to print index (row) labels.\n",
      " |      na_rep : str, optional, default 'NaN'\n",
      " |          String representation of ``NaN`` to use.\n",
      " |      formatters : list, tuple or dict of one-param. functions, optional\n",
      " |          Formatter functions to apply to columns' elements by position or\n",
      " |          name.\n",
      " |          The result of each function must be a unicode string.\n",
      " |          List/tuple must be of length equal to the number of columns.\n",
      " |      float_format : one-parameter function, optional, default None\n",
      " |          Formatter function to apply to columns' elements if they are\n",
      " |          floats. This function must return a unicode string and will be\n",
      " |          applied only to the non-``NaN`` elements, with ``NaN`` being\n",
      " |          handled by ``na_rep``.\n",
      " |      sparsify : bool, optional, default True\n",
      " |          Set to False for a DataFrame with a hierarchical index to print\n",
      " |          every multiindex key at each row.\n",
      " |      index_names : bool, optional, default True\n",
      " |          Prints the names of the indexes.\n",
      " |      justify : str, default None\n",
      " |          How to justify the column labels. If None uses the option from\n",
      " |          the print configuration (controlled by set_option), 'right' out\n",
      " |          of the box. Valid values are\n",
      " |\n",
      " |          * left\n",
      " |          * right\n",
      " |          * center\n",
      " |          * justify\n",
      " |          * justify-all\n",
      " |          * start\n",
      " |          * end\n",
      " |          * inherit\n",
      " |          * match-parent\n",
      " |          * initial\n",
      " |          * unset.\n",
      " |      max_rows : int, optional\n",
      " |          Maximum number of rows to display in the console.\n",
      " |      max_cols : int, optional\n",
      " |          Maximum number of columns to display in the console.\n",
      " |      show_dimensions : bool, default False\n",
      " |          Display DataFrame dimensions (number of rows by number of columns).\n",
      " |      decimal : str, default '.'\n",
      " |          Character recognized as decimal separator, e.g. ',' in Europe.\n",
      " |\n",
      " |      line_width : int, optional\n",
      " |          Width to wrap a line in characters.\n",
      " |      min_rows : int, optional\n",
      " |          The number of rows to display in the console in a truncated repr\n",
      " |          (when number of rows is above `max_rows`).\n",
      " |      max_colwidth : int, optional\n",
      " |          Max width to truncate each column in characters. By default, no limit.\n",
      " |      encoding : str, default \"utf-8\"\n",
      " |          Set character encoding.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      str or None\n",
      " |          If buf is None, returns the result as a string. Otherwise returns\n",
      " |          None.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      to_html : Convert DataFrame to HTML.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> d = {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\n",
      " |      >>> df = pd.DataFrame(d)\n",
      " |      >>> print(df.to_string())\n",
      " |         col1  col2\n",
      " |      0     1     4\n",
      " |      1     2     5\n",
      " |      2     3     6\n",
      " |\n",
      " |  to_timestamp(self, freq: 'Frequency | None' = None, how: 'ToTimestampHow' = 'start', axis: 'Axis' = 0, copy: 'bool | None' = None) -> 'DataFrame'\n",
      " |      Cast to DatetimeIndex of timestamps, at *beginning* of period.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      freq : str, default frequency of PeriodIndex\n",
      " |          Desired frequency.\n",
      " |      how : {'s', 'e', 'start', 'end'}\n",
      " |          Convention for converting period to timestamp; start of period\n",
      " |          vs. end.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The axis to convert (the index by default).\n",
      " |      copy : bool, default True\n",
      " |          If False then underlying input data is not copied.\n",
      " |\n",
      " |          .. note::\n",
      " |              The `copy` keyword will change behavior in pandas 3.0.\n",
      " |              `Copy-on-Write\n",
      " |              <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n",
      " |              will be enabled by default, which means that all methods with a\n",
      " |              `copy` keyword will use a lazy copy mechanism to defer the copy and\n",
      " |              ignore the `copy` keyword. The `copy` keyword will be removed in a\n",
      " |              future version of pandas.\n",
      " |\n",
      " |              You can already get the future behavior and improvements through\n",
      " |              enabling copy on write ``pd.options.mode.copy_on_write = True``\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          The DataFrame has a DatetimeIndex.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> idx = pd.PeriodIndex(['2023', '2024'], freq='Y')\n",
      " |      >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n",
      " |      >>> df1 = pd.DataFrame(data=d, index=idx)\n",
      " |      >>> df1\n",
      " |            col1   col2\n",
      " |      2023     1      3\n",
      " |      2024     2      4\n",
      " |\n",
      " |      The resulting timestamps will be at the beginning of the year in this case\n",
      " |\n",
      " |      >>> df1 = df1.to_timestamp()\n",
      " |      >>> df1\n",
      " |                  col1   col2\n",
      " |      2023-01-01     1      3\n",
      " |      2024-01-01     2      4\n",
      " |      >>> df1.index\n",
      " |      DatetimeIndex(['2023-01-01', '2024-01-01'], dtype='datetime64[ns]', freq=None)\n",
      " |\n",
      " |      Using `freq` which is the offset that the Timestamps will have\n",
      " |\n",
      " |      >>> df2 = pd.DataFrame(data=d, index=idx)\n",
      " |      >>> df2 = df2.to_timestamp(freq='M')\n",
      " |      >>> df2\n",
      " |                  col1   col2\n",
      " |      2023-01-31     1      3\n",
      " |      2024-01-31     2      4\n",
      " |      >>> df2.index\n",
      " |      DatetimeIndex(['2023-01-31', '2024-01-31'], dtype='datetime64[ns]', freq=None)\n",
      " |\n",
      " |  to_xml(self, path_or_buffer: 'FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None' = None, *, index: 'bool' = True, root_name: 'str | None' = 'data', row_name: 'str | None' = 'row', na_rep: 'str | None' = None, attr_cols: 'list[str] | None' = None, elem_cols: 'list[str] | None' = None, namespaces: 'dict[str | None, str] | None' = None, prefix: 'str | None' = None, encoding: 'str' = 'utf-8', xml_declaration: 'bool | None' = True, pretty_print: 'bool | None' = True, parser: 'XMLParsers | None' = 'lxml', stylesheet: 'FilePath | ReadBuffer[str] | ReadBuffer[bytes] | None' = None, compression: 'CompressionOptions' = 'infer', storage_options: 'StorageOptions | None' = None) -> 'str | None'\n",
      " |      Render a DataFrame to an XML document.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path_or_buffer : str, path object, file-like object, or None, default None\n",
      " |          String, path object (implementing ``os.PathLike[str]``), or file-like\n",
      " |          object implementing a ``write()`` function. If None, the result is returned\n",
      " |          as a string.\n",
      " |      index : bool, default True\n",
      " |          Whether to include index in XML document.\n",
      " |      root_name : str, default 'data'\n",
      " |          The name of root element in XML document.\n",
      " |      row_name : str, default 'row'\n",
      " |          The name of row element in XML document.\n",
      " |      na_rep : str, optional\n",
      " |          Missing data representation.\n",
      " |      attr_cols : list-like, optional\n",
      " |          List of columns to write as attributes in row element.\n",
      " |          Hierarchical columns will be flattened with underscore\n",
      " |          delimiting the different levels.\n",
      " |      elem_cols : list-like, optional\n",
      " |          List of columns to write as children in row element. By default,\n",
      " |          all columns output as children of row element. Hierarchical\n",
      " |          columns will be flattened with underscore delimiting the\n",
      " |          different levels.\n",
      " |      namespaces : dict, optional\n",
      " |          All namespaces to be defined in root element. Keys of dict\n",
      " |          should be prefix names and values of dict corresponding URIs.\n",
      " |          Default namespaces should be given empty string key. For\n",
      " |          example, ::\n",
      " |\n",
      " |              namespaces = {\"\": \"https://example.com\"}\n",
      " |\n",
      " |      prefix : str, optional\n",
      " |          Namespace prefix to be used for every element and/or attribute\n",
      " |          in document. This should be one of the keys in ``namespaces``\n",
      " |          dict.\n",
      " |      encoding : str, default 'utf-8'\n",
      " |          Encoding of the resulting document.\n",
      " |      xml_declaration : bool, default True\n",
      " |          Whether to include the XML declaration at start of document.\n",
      " |      pretty_print : bool, default True\n",
      " |          Whether output should be pretty printed with indentation and\n",
      " |          line breaks.\n",
      " |      parser : {'lxml','etree'}, default 'lxml'\n",
      " |          Parser module to use for building of tree. Only 'lxml' and\n",
      " |          'etree' are supported. With 'lxml', the ability to use XSLT\n",
      " |          stylesheet is supported.\n",
      " |      stylesheet : str, path object or file-like object, optional\n",
      " |          A URL, file-like object, or a raw string containing an XSLT\n",
      " |          script used to transform the raw XML output. Script should use\n",
      " |          layout of elements and attributes from original output. This\n",
      " |          argument requires ``lxml`` to be installed. Only XSLT 1.0\n",
      " |          scripts and not later versions is currently supported.\n",
      " |      compression : str or dict, default 'infer'\n",
      " |          For on-the-fly compression of the output data. If 'infer' and 'path_or_buffer' is\n",
      " |          path-like, then detect compression from the following extensions: '.gz',\n",
      " |          '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
      " |          (otherwise no compression).\n",
      " |          Set to ``None`` for no compression.\n",
      " |          Can also be a dict with key ``'method'`` set\n",
      " |          to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n",
      " |          other key-value pairs are forwarded to\n",
      " |          ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      " |          ``bz2.BZ2File``, ``zstandard.ZstdCompressor``, ``lzma.LZMAFile`` or\n",
      " |          ``tarfile.TarFile``, respectively.\n",
      " |          As an example, the following could be passed for faster compression and to create\n",
      " |          a reproducible gzip archive:\n",
      " |          ``compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}``.\n",
      " |\n",
      " |          .. versionadded:: 1.5.0\n",
      " |              Added support for `.tar` files.\n",
      " |\n",
      " |          .. versionchanged:: 1.4.0 Zstandard support.\n",
      " |\n",
      " |      storage_options : dict, optional\n",
      " |          Extra options that make sense for a particular storage connection, e.g.\n",
      " |          host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      " |          are forwarded to ``urllib.request.Request`` as header options. For other\n",
      " |          URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      " |          forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      " |          details, and for more examples on storage options refer `here\n",
      " |          <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      " |          highlight=storage_options#reading-writing-remote-files>`_.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      None or str\n",
      " |          If ``io`` is None, returns the resulting XML format as a\n",
      " |          string. Otherwise returns None.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      to_json : Convert the pandas object to a JSON string.\n",
      " |      to_html : Convert DataFrame to a html.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'shape': ['square', 'circle', 'triangle'],\n",
      " |      ...                    'degrees': [360, 360, 180],\n",
      " |      ...                    'sides': [4, np.nan, 3]})\n",
      " |\n",
      " |      >>> df.to_xml()  # doctest: +SKIP\n",
      " |      <?xml version='1.0' encoding='utf-8'?>\n",
      " |      <data>\n",
      " |        <row>\n",
      " |          <index>0</index>\n",
      " |          <shape>square</shape>\n",
      " |          <degrees>360</degrees>\n",
      " |          <sides>4.0</sides>\n",
      " |        </row>\n",
      " |        <row>\n",
      " |          <index>1</index>\n",
      " |          <shape>circle</shape>\n",
      " |          <degrees>360</degrees>\n",
      " |          <sides/>\n",
      " |        </row>\n",
      " |        <row>\n",
      " |          <index>2</index>\n",
      " |          <shape>triangle</shape>\n",
      " |          <degrees>180</degrees>\n",
      " |          <sides>3.0</sides>\n",
      " |        </row>\n",
      " |      </data>\n",
      " |\n",
      " |      >>> df.to_xml(attr_cols=[\n",
      " |      ...           'index', 'shape', 'degrees', 'sides'\n",
      " |      ...           ])  # doctest: +SKIP\n",
      " |      <?xml version='1.0' encoding='utf-8'?>\n",
      " |      <data>\n",
      " |        <row index=\"0\" shape=\"square\" degrees=\"360\" sides=\"4.0\"/>\n",
      " |        <row index=\"1\" shape=\"circle\" degrees=\"360\"/>\n",
      " |        <row index=\"2\" shape=\"triangle\" degrees=\"180\" sides=\"3.0\"/>\n",
      " |      </data>\n",
      " |\n",
      " |      >>> df.to_xml(namespaces={\"doc\": \"https://example.com\"},\n",
      " |      ...           prefix=\"doc\")  # doctest: +SKIP\n",
      " |      <?xml version='1.0' encoding='utf-8'?>\n",
      " |      <doc:data xmlns:doc=\"https://example.com\">\n",
      " |        <doc:row>\n",
      " |          <doc:index>0</doc:index>\n",
      " |          <doc:shape>square</doc:shape>\n",
      " |          <doc:degrees>360</doc:degrees>\n",
      " |          <doc:sides>4.0</doc:sides>\n",
      " |        </doc:row>\n",
      " |        <doc:row>\n",
      " |          <doc:index>1</doc:index>\n",
      " |          <doc:shape>circle</doc:shape>\n",
      " |          <doc:degrees>360</doc:degrees>\n",
      " |          <doc:sides/>\n",
      " |        </doc:row>\n",
      " |        <doc:row>\n",
      " |          <doc:index>2</doc:index>\n",
      " |          <doc:shape>triangle</doc:shape>\n",
      " |          <doc:degrees>180</doc:degrees>\n",
      " |          <doc:sides>3.0</doc:sides>\n",
      " |        </doc:row>\n",
      " |      </doc:data>\n",
      " |\n",
      " |  transform(self, func: 'AggFuncType', axis: 'Axis' = 0, *args, **kwargs) -> 'DataFrame'\n",
      " |      Call ``func`` on self producing a DataFrame with the same axis shape as self.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function, str, list-like or dict-like\n",
      " |          Function to use for transforming the data. If a function, must either\n",
      " |          work when passed a DataFrame or when passed to DataFrame.apply. If func\n",
      " |          is both list-like and dict-like, dict-like behavior takes precedence.\n",
      " |\n",
      " |          Accepted combinations are:\n",
      " |\n",
      " |          - function\n",
      " |          - string function name\n",
      " |          - list-like of functions and/or function names, e.g. ``[np.exp, 'sqrt']``\n",
      " |          - dict-like of axis labels -> functions, function names or list-like of such.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |              If 0 or 'index': apply function to each column.\n",
      " |              If 1 or 'columns': apply function to each row.\n",
      " |      *args\n",
      " |          Positional arguments to pass to `func`.\n",
      " |      **kwargs\n",
      " |          Keyword arguments to pass to `func`.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          A DataFrame that must have the same length as self.\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      ValueError : If the returned DataFrame has a different length than self.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.agg : Only perform aggregating type operations.\n",
      " |      DataFrame.apply : Invoke function on a DataFrame.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Functions that mutate the passed object can produce unexpected\n",
      " |      behavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\n",
      " |      for more details.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'A': range(3), 'B': range(1, 4)})\n",
      " |      >>> df\n",
      " |         A  B\n",
      " |      0  0  1\n",
      " |      1  1  2\n",
      " |      2  2  3\n",
      " |      >>> df.transform(lambda x: x + 1)\n",
      " |         A  B\n",
      " |      0  1  2\n",
      " |      1  2  3\n",
      " |      2  3  4\n",
      " |\n",
      " |      Even though the resulting DataFrame must have the same length as the\n",
      " |      input DataFrame, it is possible to provide several input functions:\n",
      " |\n",
      " |      >>> s = pd.Series(range(3))\n",
      " |      >>> s\n",
      " |      0    0\n",
      " |      1    1\n",
      " |      2    2\n",
      " |      dtype: int64\n",
      " |      >>> s.transform([np.sqrt, np.exp])\n",
      " |             sqrt        exp\n",
      " |      0  0.000000   1.000000\n",
      " |      1  1.000000   2.718282\n",
      " |      2  1.414214   7.389056\n",
      " |\n",
      " |      You can call transform on a GroupBy object:\n",
      " |\n",
      " |      >>> df = pd.DataFrame({\n",
      " |      ...     \"Date\": [\n",
      " |      ...         \"2015-05-08\", \"2015-05-07\", \"2015-05-06\", \"2015-05-05\",\n",
      " |      ...         \"2015-05-08\", \"2015-05-07\", \"2015-05-06\", \"2015-05-05\"],\n",
      " |      ...     \"Data\": [5, 8, 6, 1, 50, 100, 60, 120],\n",
      " |      ... })\n",
      " |      >>> df\n",
      " |               Date  Data\n",
      " |      0  2015-05-08     5\n",
      " |      1  2015-05-07     8\n",
      " |      2  2015-05-06     6\n",
      " |      3  2015-05-05     1\n",
      " |      4  2015-05-08    50\n",
      " |      5  2015-05-07   100\n",
      " |      6  2015-05-06    60\n",
      " |      7  2015-05-05   120\n",
      " |      >>> df.groupby('Date')['Data'].transform('sum')\n",
      " |      0     55\n",
      " |      1    108\n",
      " |      2     66\n",
      " |      3    121\n",
      " |      4     55\n",
      " |      5    108\n",
      " |      6     66\n",
      " |      7    121\n",
      " |      Name: Data, dtype: int64\n",
      " |\n",
      " |      >>> df = pd.DataFrame({\n",
      " |      ...     \"c\": [1, 1, 1, 2, 2, 2, 2],\n",
      " |      ...     \"type\": [\"m\", \"n\", \"o\", \"m\", \"m\", \"n\", \"n\"]\n",
      " |      ... })\n",
      " |      >>> df\n",
      " |         c type\n",
      " |      0  1    m\n",
      " |      1  1    n\n",
      " |      2  1    o\n",
      " |      3  2    m\n",
      " |      4  2    m\n",
      " |      5  2    n\n",
      " |      6  2    n\n",
      " |      >>> df['size'] = df.groupby('c')['type'].transform(len)\n",
      " |      >>> df\n",
      " |         c type size\n",
      " |      0  1    m    3\n",
      " |      1  1    n    3\n",
      " |      2  1    o    3\n",
      " |      3  2    m    4\n",
      " |      4  2    m    4\n",
      " |      5  2    n    4\n",
      " |      6  2    n    4\n",
      " |\n",
      " |  transpose(self, *args, copy: 'bool' = False) -> 'DataFrame'\n",
      " |      Transpose index and columns.\n",
      " |\n",
      " |      Reflect the DataFrame over its main diagonal by writing rows as columns\n",
      " |      and vice-versa. The property :attr:`.T` is an accessor to the method\n",
      " |      :meth:`transpose`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      *args : tuple, optional\n",
      " |          Accepted for compatibility with NumPy.\n",
      " |      copy : bool, default False\n",
      " |          Whether to copy the data after transposing, even for DataFrames\n",
      " |          with a single dtype.\n",
      " |\n",
      " |          Note that a copy is always required for mixed dtype DataFrames,\n",
      " |          or for DataFrames with any extension types.\n",
      " |\n",
      " |          .. note::\n",
      " |              The `copy` keyword will change behavior in pandas 3.0.\n",
      " |              `Copy-on-Write\n",
      " |              <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n",
      " |              will be enabled by default, which means that all methods with a\n",
      " |              `copy` keyword will use a lazy copy mechanism to defer the copy and\n",
      " |              ignore the `copy` keyword. The `copy` keyword will be removed in a\n",
      " |              future version of pandas.\n",
      " |\n",
      " |              You can already get the future behavior and improvements through\n",
      " |              enabling copy on write ``pd.options.mode.copy_on_write = True``\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          The transposed DataFrame.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.transpose : Permute the dimensions of a given array.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Transposing a DataFrame with mixed dtypes will result in a homogeneous\n",
      " |      DataFrame with the `object` dtype. In such a case, a copy of the data\n",
      " |      is always made.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      **Square DataFrame with homogeneous dtype**\n",
      " |\n",
      " |      >>> d1 = {'col1': [1, 2], 'col2': [3, 4]}\n",
      " |      >>> df1 = pd.DataFrame(data=d1)\n",
      " |      >>> df1\n",
      " |         col1  col2\n",
      " |      0     1     3\n",
      " |      1     2     4\n",
      " |\n",
      " |      >>> df1_transposed = df1.T  # or df1.transpose()\n",
      " |      >>> df1_transposed\n",
      " |            0  1\n",
      " |      col1  1  2\n",
      " |      col2  3  4\n",
      " |\n",
      " |      When the dtype is homogeneous in the original DataFrame, we get a\n",
      " |      transposed DataFrame with the same dtype:\n",
      " |\n",
      " |      >>> df1.dtypes\n",
      " |      col1    int64\n",
      " |      col2    int64\n",
      " |      dtype: object\n",
      " |      >>> df1_transposed.dtypes\n",
      " |      0    int64\n",
      " |      1    int64\n",
      " |      dtype: object\n",
      " |\n",
      " |      **Non-square DataFrame with mixed dtypes**\n",
      " |\n",
      " |      >>> d2 = {'name': ['Alice', 'Bob'],\n",
      " |      ...       'score': [9.5, 8],\n",
      " |      ...       'employed': [False, True],\n",
      " |      ...       'kids': [0, 0]}\n",
      " |      >>> df2 = pd.DataFrame(data=d2)\n",
      " |      >>> df2\n",
      " |          name  score  employed  kids\n",
      " |      0  Alice    9.5     False     0\n",
      " |      1    Bob    8.0      True     0\n",
      " |\n",
      " |      >>> df2_transposed = df2.T  # or df2.transpose()\n",
      " |      >>> df2_transposed\n",
      " |                    0     1\n",
      " |      name      Alice   Bob\n",
      " |      score       9.5   8.0\n",
      " |      employed  False  True\n",
      " |      kids          0     0\n",
      " |\n",
      " |      When the DataFrame has mixed dtypes, we get a transposed DataFrame with\n",
      " |      the `object` dtype:\n",
      " |\n",
      " |      >>> df2.dtypes\n",
      " |      name         object\n",
      " |      score       float64\n",
      " |      employed       bool\n",
      " |      kids          int64\n",
      " |      dtype: object\n",
      " |      >>> df2_transposed.dtypes\n",
      " |      0    object\n",
      " |      1    object\n",
      " |      dtype: object\n",
      " |\n",
      " |  truediv(self, other, axis: 'Axis' = 'columns', level=None, fill_value=None) -> 'DataFrame'\n",
      " |      Get Floating division of dataframe and other, element-wise (binary operator `truediv`).\n",
      " |\n",
      " |      Equivalent to ``dataframe / other``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `rtruediv`.\n",
      " |\n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `floordiv`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, dict or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns.\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |\n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |\n",
      " |      >>> df + 1\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      >>> df.add(1)\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |\n",
      " |      Divide by constant with reverse version.\n",
      " |\n",
      " |      >>> df.div(10)\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |\n",
      " |      >>> df.rdiv(10)\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |\n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |\n",
      " |      >>> df - [1, 2]\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub([1, 2], axis='columns')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |\n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |\n",
      " |      Multiply a dictionary by axis.\n",
      " |\n",
      " |      >>> df.mul({'angles': 0, 'degrees': 2})\n",
      " |                  angles  degrees\n",
      " |      circle           0      720\n",
      " |      triangle         0      360\n",
      " |      rectangle        0      720\n",
      " |\n",
      " |      >>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n",
      " |                  angles  degrees\n",
      " |      circle           0        0\n",
      " |      triangle         6      360\n",
      " |      rectangle       12     1080\n",
      " |\n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |\n",
      " |      >>> df * other\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |\n",
      " |      >>> df.mul(other, fill_value=0)\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |\n",
      " |      Divide by a MultiIndex by level.\n",
      " |\n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |\n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |\n",
      " |  unstack(self, level: 'IndexLabel' = -1, fill_value=None, sort: 'bool' = True)\n",
      " |      Pivot a level of the (necessarily hierarchical) index labels.\n",
      " |\n",
      " |      Returns a DataFrame having a new level of column labels whose inner-most level\n",
      " |      consists of the pivoted index labels.\n",
      " |\n",
      " |      If the index is not a MultiIndex, the output will be a Series\n",
      " |      (the analogue of stack when the columns are not a MultiIndex).\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      level : int, str, or list of these, default -1 (last level)\n",
      " |          Level(s) of index to unstack, can pass level name.\n",
      " |      fill_value : int, str or dict\n",
      " |          Replace NaN with this value if the unstack produces missing values.\n",
      " |      sort : bool, default True\n",
      " |          Sort the level(s) in the resulting MultiIndex columns.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.pivot : Pivot a table based on column values.\n",
      " |      DataFrame.stack : Pivot a level of the column labels (inverse operation\n",
      " |          from `unstack`).\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Reference :ref:`the user guide <reshaping.stacking>` for more examples.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),\n",
      " |      ...                                    ('two', 'a'), ('two', 'b')])\n",
      " |      >>> s = pd.Series(np.arange(1.0, 5.0), index=index)\n",
      " |      >>> s\n",
      " |      one  a   1.0\n",
      " |           b   2.0\n",
      " |      two  a   3.0\n",
      " |           b   4.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |      >>> s.unstack(level=-1)\n",
      " |           a   b\n",
      " |      one  1.0  2.0\n",
      " |      two  3.0  4.0\n",
      " |\n",
      " |      >>> s.unstack(level=0)\n",
      " |         one  two\n",
      " |      a  1.0   3.0\n",
      " |      b  2.0   4.0\n",
      " |\n",
      " |      >>> df = s.unstack(level=0)\n",
      " |      >>> df.unstack()\n",
      " |      one  a  1.0\n",
      " |           b  2.0\n",
      " |      two  a  3.0\n",
      " |           b  4.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |  update(self, other, join: 'UpdateJoin' = 'left', overwrite: 'bool' = True, filter_func=None, errors: 'IgnoreRaise' = 'ignore') -> 'None'\n",
      " |      Modify in place using non-NA values from another DataFrame.\n",
      " |\n",
      " |      Aligns on indices. There is no return value.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : DataFrame, or object coercible into a DataFrame\n",
      " |          Should have at least one matching index/column label\n",
      " |          with the original DataFrame. If a Series is passed,\n",
      " |          its name attribute must be set, and that will be\n",
      " |          used as the column name to align with the original DataFrame.\n",
      " |      join : {'left'}, default 'left'\n",
      " |          Only left join is implemented, keeping the index and columns of the\n",
      " |          original object.\n",
      " |      overwrite : bool, default True\n",
      " |          How to handle non-NA values for overlapping keys:\n",
      " |\n",
      " |          * True: overwrite original DataFrame's values\n",
      " |            with values from `other`.\n",
      " |          * False: only update values that are NA in\n",
      " |            the original DataFrame.\n",
      " |\n",
      " |      filter_func : callable(1d-array) -> bool 1d-array, optional\n",
      " |          Can choose to replace values other than NA. Return True for values\n",
      " |          that should be updated.\n",
      " |      errors : {'raise', 'ignore'}, default 'ignore'\n",
      " |          If 'raise', will raise a ValueError if the DataFrame and `other`\n",
      " |          both contain non-NA data in the same place.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |          This method directly changes calling object.\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      ValueError\n",
      " |          * When `errors='raise'` and there's overlapping non-NA data.\n",
      " |          * When `errors` is not either `'ignore'` or `'raise'`\n",
      " |      NotImplementedError\n",
      " |          * If `join != 'left'`\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      dict.update : Similar method for dictionaries.\n",
      " |      DataFrame.merge : For column(s)-on-column(s) operations.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'A': [1, 2, 3],\n",
      " |      ...                    'B': [400, 500, 600]})\n",
      " |      >>> new_df = pd.DataFrame({'B': [4, 5, 6],\n",
      " |      ...                        'C': [7, 8, 9]})\n",
      " |      >>> df.update(new_df)\n",
      " |      >>> df\n",
      " |         A  B\n",
      " |      0  1  4\n",
      " |      1  2  5\n",
      " |      2  3  6\n",
      " |\n",
      " |      The DataFrame's length does not increase as a result of the update,\n",
      " |      only values at matching index/column labels are updated.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n",
      " |      ...                    'B': ['x', 'y', 'z']})\n",
      " |      >>> new_df = pd.DataFrame({'B': ['d', 'e', 'f', 'g', 'h', 'i']})\n",
      " |      >>> df.update(new_df)\n",
      " |      >>> df\n",
      " |         A  B\n",
      " |      0  a  d\n",
      " |      1  b  e\n",
      " |      2  c  f\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n",
      " |      ...                    'B': ['x', 'y', 'z']})\n",
      " |      >>> new_df = pd.DataFrame({'B': ['d', 'f']}, index=[0, 2])\n",
      " |      >>> df.update(new_df)\n",
      " |      >>> df\n",
      " |         A  B\n",
      " |      0  a  d\n",
      " |      1  b  y\n",
      " |      2  c  f\n",
      " |\n",
      " |      For Series, its name attribute must be set.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n",
      " |      ...                    'B': ['x', 'y', 'z']})\n",
      " |      >>> new_column = pd.Series(['d', 'e', 'f'], name='B')\n",
      " |      >>> df.update(new_column)\n",
      " |      >>> df\n",
      " |         A  B\n",
      " |      0  a  d\n",
      " |      1  b  e\n",
      " |      2  c  f\n",
      " |\n",
      " |      If `other` contains NaNs the corresponding values are not updated\n",
      " |      in the original dataframe.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'A': [1, 2, 3],\n",
      " |      ...                    'B': [400., 500., 600.]})\n",
      " |      >>> new_df = pd.DataFrame({'B': [4, np.nan, 6]})\n",
      " |      >>> df.update(new_df)\n",
      " |      >>> df\n",
      " |         A      B\n",
      " |      0  1    4.0\n",
      " |      1  2  500.0\n",
      " |      2  3    6.0\n",
      " |\n",
      " |  value_counts(self, subset: 'IndexLabel | None' = None, normalize: 'bool' = False, sort: 'bool' = True, ascending: 'bool' = False, dropna: 'bool' = True) -> 'Series'\n",
      " |      Return a Series containing the frequency of each distinct row in the Dataframe.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      subset : label or list of labels, optional\n",
      " |          Columns to use when counting unique combinations.\n",
      " |      normalize : bool, default False\n",
      " |          Return proportions rather than frequencies.\n",
      " |      sort : bool, default True\n",
      " |          Sort by frequencies when True. Sort by DataFrame column values when False.\n",
      " |      ascending : bool, default False\n",
      " |          Sort in ascending order.\n",
      " |      dropna : bool, default True\n",
      " |          Don't include counts of rows that contain NA values.\n",
      " |\n",
      " |          .. versionadded:: 1.3.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.value_counts: Equivalent method on Series.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      The returned Series will have a MultiIndex with one level per input\n",
      " |      column but an Index (non-multi) for a single label. By default, rows\n",
      " |      that contain any NA values are omitted from the result. By default,\n",
      " |      the resulting Series will be in descending order so that the first\n",
      " |      element is the most frequently-occurring row.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'num_legs': [2, 4, 4, 6],\n",
      " |      ...                    'num_wings': [2, 0, 0, 0]},\n",
      " |      ...                   index=['falcon', 'dog', 'cat', 'ant'])\n",
      " |      >>> df\n",
      " |              num_legs  num_wings\n",
      " |      falcon         2          2\n",
      " |      dog            4          0\n",
      " |      cat            4          0\n",
      " |      ant            6          0\n",
      " |\n",
      " |      >>> df.value_counts()\n",
      " |      num_legs  num_wings\n",
      " |      4         0            2\n",
      " |      2         2            1\n",
      " |      6         0            1\n",
      " |      Name: count, dtype: int64\n",
      " |\n",
      " |      >>> df.value_counts(sort=False)\n",
      " |      num_legs  num_wings\n",
      " |      2         2            1\n",
      " |      4         0            2\n",
      " |      6         0            1\n",
      " |      Name: count, dtype: int64\n",
      " |\n",
      " |      >>> df.value_counts(ascending=True)\n",
      " |      num_legs  num_wings\n",
      " |      2         2            1\n",
      " |      6         0            1\n",
      " |      4         0            2\n",
      " |      Name: count, dtype: int64\n",
      " |\n",
      " |      >>> df.value_counts(normalize=True)\n",
      " |      num_legs  num_wings\n",
      " |      4         0            0.50\n",
      " |      2         2            0.25\n",
      " |      6         0            0.25\n",
      " |      Name: proportion, dtype: float64\n",
      " |\n",
      " |      With `dropna` set to `False` we can also count rows with NA values.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'first_name': ['John', 'Anne', 'John', 'Beth'],\n",
      " |      ...                    'middle_name': ['Smith', pd.NA, pd.NA, 'Louise']})\n",
      " |      >>> df\n",
      " |        first_name middle_name\n",
      " |      0       John       Smith\n",
      " |      1       Anne        <NA>\n",
      " |      2       John        <NA>\n",
      " |      3       Beth      Louise\n",
      " |\n",
      " |      >>> df.value_counts()\n",
      " |      first_name  middle_name\n",
      " |      Beth        Louise         1\n",
      " |      John        Smith          1\n",
      " |      Name: count, dtype: int64\n",
      " |\n",
      " |      >>> df.value_counts(dropna=False)\n",
      " |      first_name  middle_name\n",
      " |      Anne        NaN            1\n",
      " |      Beth        Louise         1\n",
      " |      John        Smith          1\n",
      " |                  NaN            1\n",
      " |      Name: count, dtype: int64\n",
      " |\n",
      " |      >>> df.value_counts(\"first_name\")\n",
      " |      first_name\n",
      " |      John    2\n",
      " |      Anne    1\n",
      " |      Beth    1\n",
      " |      Name: count, dtype: int64\n",
      " |\n",
      " |  var(self, axis: 'Axis | None' = 0, skipna: 'bool' = True, ddof: 'int' = 1, numeric_only: 'bool' = False, **kwargs)\n",
      " |      Return unbiased variance over requested axis.\n",
      " |\n",
      " |      Normalized by N-1 by default. This can be changed using the ddof argument.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {index (0), columns (1)}\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |\n",
      " |          .. warning::\n",
      " |\n",
      " |              The behavior of DataFrame.var with ``axis=None`` is deprecated,\n",
      " |              in a future version this will reduce over both axes and return a scalar\n",
      " |              To retain the old behavior, pass axis=0 (or do not pass axis).\n",
      " |\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If an entire row/column is NA, the result\n",
      " |          will be NA.\n",
      " |      ddof : int, default 1\n",
      " |          Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\n",
      " |          where N represents the number of elements.\n",
      " |      numeric_only : bool, default False\n",
      " |          Include only float, int, boolean columns. Not implemented for Series.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame (if level specified)\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'person_id': [0, 1, 2, 3],\n",
      " |      ...                    'age': [21, 25, 62, 43],\n",
      " |      ...                    'height': [1.61, 1.87, 1.49, 2.01]}\n",
      " |      ...                   ).set_index('person_id')\n",
      " |      >>> df\n",
      " |                 age  height\n",
      " |      person_id\n",
      " |      0           21    1.61\n",
      " |      1           25    1.87\n",
      " |      2           62    1.49\n",
      " |      3           43    2.01\n",
      " |\n",
      " |      >>> df.var()\n",
      " |      age       352.916667\n",
      " |      height      0.056367\n",
      " |      dtype: float64\n",
      " |\n",
      " |      Alternatively, ``ddof=0`` can be set to normalize by N instead of N-1:\n",
      " |\n",
      " |      >>> df.var(ddof=0)\n",
      " |      age       264.687500\n",
      " |      height      0.042275\n",
      " |      dtype: float64\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pandas.core.frame.DataFrame:\n",
      " |\n",
      " |  from_records(data, index=None, exclude=None, columns=None, coerce_float: 'bool' = False, nrows: 'int | None' = None) -> 'DataFrame'\n",
      " |      Convert structured or record ndarray to DataFrame.\n",
      " |\n",
      " |      Creates a DataFrame object from a structured ndarray, sequence of\n",
      " |      tuples or dicts, or DataFrame.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : structured ndarray, sequence of tuples or dicts, or DataFrame\n",
      " |          Structured input data.\n",
      " |\n",
      " |          .. deprecated:: 2.1.0\n",
      " |              Passing a DataFrame is deprecated.\n",
      " |      index : str, list of fields, array-like\n",
      " |          Field of array to use as the index, alternately a specific set of\n",
      " |          input labels to use.\n",
      " |      exclude : sequence, default None\n",
      " |          Columns or fields to exclude.\n",
      " |      columns : sequence, default None\n",
      " |          Column names to use. If the passed data do not have names\n",
      " |          associated with them, this argument provides names for the\n",
      " |          columns. Otherwise this argument indicates the order of the columns\n",
      " |          in the result (any names not found in the data will become all-NA\n",
      " |          columns).\n",
      " |      coerce_float : bool, default False\n",
      " |          Attempt to convert values of non-string, non-numeric objects (like\n",
      " |          decimal.Decimal) to floating point, useful for SQL result sets.\n",
      " |      nrows : int, default None\n",
      " |          Number of rows to read if data is an iterator.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.from_dict : DataFrame from dict of array-like or dicts.\n",
      " |      DataFrame : DataFrame object creation using constructor.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Data can be provided as a structured ndarray:\n",
      " |\n",
      " |      >>> data = np.array([(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')],\n",
      " |      ...                 dtype=[('col_1', 'i4'), ('col_2', 'U1')])\n",
      " |      >>> pd.DataFrame.from_records(data)\n",
      " |         col_1 col_2\n",
      " |      0      3     a\n",
      " |      1      2     b\n",
      " |      2      1     c\n",
      " |      3      0     d\n",
      " |\n",
      " |      Data can be provided as a list of dicts:\n",
      " |\n",
      " |      >>> data = [{'col_1': 3, 'col_2': 'a'},\n",
      " |      ...         {'col_1': 2, 'col_2': 'b'},\n",
      " |      ...         {'col_1': 1, 'col_2': 'c'},\n",
      " |      ...         {'col_1': 0, 'col_2': 'd'}]\n",
      " |      >>> pd.DataFrame.from_records(data)\n",
      " |         col_1 col_2\n",
      " |      0      3     a\n",
      " |      1      2     b\n",
      " |      2      1     c\n",
      " |      3      0     d\n",
      " |\n",
      " |      Data can be provided as a list of tuples with corresponding columns:\n",
      " |\n",
      " |      >>> data = [(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')]\n",
      " |      >>> pd.DataFrame.from_records(data, columns=['col_1', 'col_2'])\n",
      " |         col_1 col_2\n",
      " |      0      3     a\n",
      " |      1      2     b\n",
      " |      2      1     c\n",
      " |      3      0     d\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pandas.core.frame.DataFrame:\n",
      " |\n",
      " |  T\n",
      " |      The transpose of the DataFrame.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          The transposed DataFrame.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.transpose : Transpose index and columns.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n",
      " |      >>> df\n",
      " |         col1  col2\n",
      " |      0     1     3\n",
      " |      1     2     4\n",
      " |\n",
      " |      >>> df.T\n",
      " |            0  1\n",
      " |      col1  1  2\n",
      " |      col2  3  4\n",
      " |\n",
      " |  axes\n",
      " |      Return a list representing the axes of the DataFrame.\n",
      " |\n",
      " |      It has the row axis labels and column axis labels as the only members.\n",
      " |      They are returned in that order.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n",
      " |      >>> df.axes\n",
      " |      [RangeIndex(start=0, stop=2, step=1), Index(['col1', 'col2'],\n",
      " |      dtype='object')]\n",
      " |\n",
      " |  shape\n",
      " |      Return a tuple representing the dimensionality of the DataFrame.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      ndarray.shape : Tuple of array dimensions.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n",
      " |      >>> df.shape\n",
      " |      (2, 2)\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4],\n",
      " |      ...                    'col3': [5, 6]})\n",
      " |      >>> df.shape\n",
      " |      (2, 3)\n",
      " |\n",
      " |  style\n",
      " |      Returns a Styler object.\n",
      " |\n",
      " |      Contains methods for building a styled HTML representation of the DataFrame.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      io.formats.style.Styler : Helps style a DataFrame or Series according to the\n",
      " |          data with HTML and CSS.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'A': [1, 2, 3]})\n",
      " |      >>> df.style  # doctest: +SKIP\n",
      " |\n",
      " |      Please see\n",
      " |      `Table Visualization <../../user_guide/style.ipynb>`_ for more examples.\n",
      " |\n",
      " |  values\n",
      " |      Return a Numpy representation of the DataFrame.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |         We recommend using :meth:`DataFrame.to_numpy` instead.\n",
      " |\n",
      " |      Only the values in the DataFrame will be returned, the axes labels\n",
      " |      will be removed.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          The values of the DataFrame.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.to_numpy : Recommended alternative to this method.\n",
      " |      DataFrame.index : Retrieve the index labels.\n",
      " |      DataFrame.columns : Retrieving the column names.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      The dtype will be a lower-common-denominator dtype (implicit\n",
      " |      upcasting); that is to say if the dtypes (even of numeric types)\n",
      " |      are mixed, the one that accommodates all will be chosen. Use this\n",
      " |      with care if you are not dealing with the blocks.\n",
      " |\n",
      " |      e.g. If the dtypes are float16 and float32, dtype will be upcast to\n",
      " |      float32.  If dtypes are int32 and uint8, dtype will be upcast to\n",
      " |      int32. By :func:`numpy.find_common_type` convention, mixing int64\n",
      " |      and uint64 will result in a float64 dtype.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      A DataFrame where all columns are the same type (e.g., int64) results\n",
      " |      in an array of the same type.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'age':    [ 3,  29],\n",
      " |      ...                    'height': [94, 170],\n",
      " |      ...                    'weight': [31, 115]})\n",
      " |      >>> df\n",
      " |         age  height  weight\n",
      " |      0    3      94      31\n",
      " |      1   29     170     115\n",
      " |      >>> df.dtypes\n",
      " |      age       int64\n",
      " |      height    int64\n",
      " |      weight    int64\n",
      " |      dtype: object\n",
      " |      >>> df.values\n",
      " |      array([[  3,  94,  31],\n",
      " |             [ 29, 170, 115]])\n",
      " |\n",
      " |      A DataFrame with mixed type columns(e.g., str/object, int64, float32)\n",
      " |      results in an ndarray of the broadest type that accommodates these\n",
      " |      mixed types (e.g., object).\n",
      " |\n",
      " |      >>> df2 = pd.DataFrame([('parrot',   24.0, 'second'),\n",
      " |      ...                     ('lion',     80.5, 1),\n",
      " |      ...                     ('monkey', np.nan, None)],\n",
      " |      ...                   columns=('name', 'max_speed', 'rank'))\n",
      " |      >>> df2.dtypes\n",
      " |      name          object\n",
      " |      max_speed    float64\n",
      " |      rank          object\n",
      " |      dtype: object\n",
      " |      >>> df2.values\n",
      " |      array([['parrot', 24.0, 'second'],\n",
      " |             ['lion', 80.5, 1],\n",
      " |             ['monkey', nan, None]], dtype=object)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pandas.core.frame.DataFrame:\n",
      " |\n",
      " |  columns\n",
      " |      The column labels of the DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
      " |      >>> df\n",
      " |           A  B\n",
      " |      0    1  3\n",
      " |      1    2  4\n",
      " |      >>> df.columns\n",
      " |      Index(['A', 'B'], dtype='object')\n",
      " |\n",
      " |  index\n",
      " |      The index (row labels) of the DataFrame.\n",
      " |\n",
      " |      The index of a DataFrame is a series of labels that identify each row.\n",
      " |      The labels can be integers, strings, or any other hashable type. The index\n",
      " |      is used for label-based access and alignment, and can be accessed or\n",
      " |      modified using this attribute.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      pandas.Index\n",
      " |          The index labels of the DataFrame.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.columns : The column labels of the DataFrame.\n",
      " |      DataFrame.to_numpy : Convert the DataFrame to a NumPy array.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'Name': ['Alice', 'Bob', 'Aritra'],\n",
      " |      ...                    'Age': [25, 30, 35],\n",
      " |      ...                    'Location': ['Seattle', 'New York', 'Kona']},\n",
      " |      ...                   index=([10, 20, 30]))\n",
      " |      >>> df.index\n",
      " |      Index([10, 20, 30], dtype='int64')\n",
      " |\n",
      " |      In this example, we create a DataFrame with 3 rows and 3 columns,\n",
      " |      including Name, Age, and Location information. We set the index labels to\n",
      " |      be the integers 10, 20, and 30. We then access the `index` attribute of the\n",
      " |      DataFrame, which returns an `Index` object containing the index labels.\n",
      " |\n",
      " |      >>> df.index = [100, 200, 300]\n",
      " |      >>> df\n",
      " |          Name  Age Location\n",
      " |      100  Alice   25  Seattle\n",
      " |      200    Bob   30 New York\n",
      " |      300  Aritra  35    Kona\n",
      " |\n",
      " |      In this example, we modify the index labels of the DataFrame by assigning\n",
      " |      a new list of labels to the `index` attribute. The DataFrame is then\n",
      " |      updated with the new labels, and the output shows the modified DataFrame.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pandas.core.frame.DataFrame:\n",
      " |\n",
      " |  __pandas_priority__ = 4000\n",
      " |\n",
      " |  sparse = <class 'pandas.core.arrays.sparse.accessor.SparseFrameAccesso...\n",
      " |      DataFrame accessor for sparse data.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({\"a\": [1, 2, 0, 0],\n",
      " |      ...                   \"b\": [3, 0, 0, 4]}, dtype=\"Sparse[int]\")\n",
      " |      >>> df.sparse.density\n",
      " |      0.5\n",
      " |\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pandas.core.generic.NDFrame:\n",
      " |\n",
      " |  __abs__(self) -> 'Self'\n",
      " |\n",
      " |  __array__(self, dtype: 'npt.DTypeLike | None' = None, copy: 'bool_t | None' = None) -> 'np.ndarray'\n",
      " |\n",
      " |  __array_ufunc__(self, ufunc: 'np.ufunc', method: 'str', *inputs: 'Any', **kwargs: 'Any')\n",
      " |\n",
      " |  __bool__ = __nonzero__(self) -> 'NoReturn'\n",
      " |\n",
      " |  __contains__(self, key) -> 'bool_t'\n",
      " |      True if the key is in the info axis\n",
      " |\n",
      " |  __copy__(self, deep: 'bool_t' = True) -> 'Self'\n",
      " |\n",
      " |  __deepcopy__(self, memo=None) -> 'Self'\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      memo, default None\n",
      " |          Standard signature. Unused\n",
      " |\n",
      " |  __delitem__(self, key) -> 'None'\n",
      " |      Delete item\n",
      " |\n",
      " |  __getattr__(self, name: 'str')\n",
      " |      After regular attribute access, try looking up the name\n",
      " |      This allows simpler access to columns for interactive use.\n",
      " |\n",
      " |  __getstate__(self) -> 'dict[str, Any]'\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __iadd__(self, other) -> 'Self'\n",
      " |\n",
      " |  __iand__(self, other) -> 'Self'\n",
      " |\n",
      " |  __ifloordiv__(self, other) -> 'Self'\n",
      " |\n",
      " |  __imod__(self, other) -> 'Self'\n",
      " |\n",
      " |  __imul__(self, other) -> 'Self'\n",
      " |\n",
      " |  __invert__(self) -> 'Self'\n",
      " |\n",
      " |  __ior__(self, other) -> 'Self'\n",
      " |\n",
      " |  __ipow__(self, other) -> 'Self'\n",
      " |\n",
      " |  __isub__(self, other) -> 'Self'\n",
      " |\n",
      " |  __iter__(self) -> 'Iterator'\n",
      " |      Iterate over info axis.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      iterator\n",
      " |          Info axis as iterator.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
      " |      >>> for x in df:\n",
      " |      ...     print(x)\n",
      " |      A\n",
      " |      B\n",
      " |\n",
      " |  __itruediv__(self, other) -> 'Self'\n",
      " |\n",
      " |  __ixor__(self, other) -> 'Self'\n",
      " |\n",
      " |  __neg__(self) -> 'Self'\n",
      " |\n",
      " |  __nonzero__(self) -> 'NoReturn'\n",
      " |\n",
      " |  __pos__(self) -> 'Self'\n",
      " |\n",
      " |  __round__(self, decimals: 'int' = 0) -> 'Self'\n",
      " |\n",
      " |  abs(self) -> 'Self'\n",
      " |      Return a Series/DataFrame with absolute numeric value of each element.\n",
      " |\n",
      " |      This function only applies to elements that are all numeric.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      abs\n",
      " |          Series/DataFrame containing the absolute value of each element.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.absolute : Calculate the absolute value element-wise.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      For ``complex`` inputs, ``1.2 + 1j``, the absolute value is\n",
      " |      :math:`\\sqrt{ a^2 + b^2 }`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Absolute numeric values in a Series.\n",
      " |\n",
      " |      >>> s = pd.Series([-1.10, 2, -3.33, 4])\n",
      " |      >>> s.abs()\n",
      " |      0    1.10\n",
      " |      1    2.00\n",
      " |      2    3.33\n",
      " |      3    4.00\n",
      " |      dtype: float64\n",
      " |\n",
      " |      Absolute numeric values in a Series with complex numbers.\n",
      " |\n",
      " |      >>> s = pd.Series([1.2 + 1j])\n",
      " |      >>> s.abs()\n",
      " |      0    1.56205\n",
      " |      dtype: float64\n",
      " |\n",
      " |      Absolute numeric values in a Series with a Timedelta element.\n",
      " |\n",
      " |      >>> s = pd.Series([pd.Timedelta('1 days')])\n",
      " |      >>> s.abs()\n",
      " |      0   1 days\n",
      " |      dtype: timedelta64[ns]\n",
      " |\n",
      " |      Select rows with data closest to certain value using argsort (from\n",
      " |      `StackOverflow <https://stackoverflow.com/a/17758115>`__).\n",
      " |\n",
      " |      >>> df = pd.DataFrame({\n",
      " |      ...     'a': [4, 5, 6, 7],\n",
      " |      ...     'b': [10, 20, 30, 40],\n",
      " |      ...     'c': [100, 50, -30, -50]\n",
      " |      ... })\n",
      " |      >>> df\n",
      " |           a    b    c\n",
      " |      0    4   10  100\n",
      " |      1    5   20   50\n",
      " |      2    6   30  -30\n",
      " |      3    7   40  -50\n",
      " |      >>> df.loc[(df.c - 43).abs().argsort()]\n",
      " |           a    b    c\n",
      " |      1    5   20   50\n",
      " |      0    4   10  100\n",
      " |      2    6   30  -30\n",
      " |      3    7   40  -50\n",
      " |\n",
      " |  add_prefix(self, prefix: 'str', axis: 'Axis | None' = None) -> 'Self'\n",
      " |      Prefix labels with string `prefix`.\n",
      " |\n",
      " |      For Series, the row labels are prefixed.\n",
      " |      For DataFrame, the column labels are prefixed.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      prefix : str\n",
      " |          The string to add before each label.\n",
      " |      axis : {0 or 'index', 1 or 'columns', None}, default None\n",
      " |          Axis to add prefix on\n",
      " |\n",
      " |          .. versionadded:: 2.0.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          New Series or DataFrame with updated labels.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.add_suffix: Suffix row labels with string `suffix`.\n",
      " |      DataFrame.add_suffix: Suffix column labels with string `suffix`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> s = pd.Series([1, 2, 3, 4])\n",
      " |      >>> s\n",
      " |      0    1\n",
      " |      1    2\n",
      " |      2    3\n",
      " |      3    4\n",
      " |      dtype: int64\n",
      " |\n",
      " |      >>> s.add_prefix('item_')\n",
      " |      item_0    1\n",
      " |      item_1    2\n",
      " |      item_2    3\n",
      " |      item_3    4\n",
      " |      dtype: int64\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})\n",
      " |      >>> df\n",
      " |         A  B\n",
      " |      0  1  3\n",
      " |      1  2  4\n",
      " |      2  3  5\n",
      " |      3  4  6\n",
      " |\n",
      " |      >>> df.add_prefix('col_')\n",
      " |           col_A  col_B\n",
      " |      0       1       3\n",
      " |      1       2       4\n",
      " |      2       3       5\n",
      " |      3       4       6\n",
      " |\n",
      " |  add_suffix(self, suffix: 'str', axis: 'Axis | None' = None) -> 'Self'\n",
      " |      Suffix labels with string `suffix`.\n",
      " |\n",
      " |      For Series, the row labels are suffixed.\n",
      " |      For DataFrame, the column labels are suffixed.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      suffix : str\n",
      " |          The string to add after each label.\n",
      " |      axis : {0 or 'index', 1 or 'columns', None}, default None\n",
      " |          Axis to add suffix on\n",
      " |\n",
      " |          .. versionadded:: 2.0.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          New Series or DataFrame with updated labels.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.add_prefix: Prefix row labels with string `prefix`.\n",
      " |      DataFrame.add_prefix: Prefix column labels with string `prefix`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> s = pd.Series([1, 2, 3, 4])\n",
      " |      >>> s\n",
      " |      0    1\n",
      " |      1    2\n",
      " |      2    3\n",
      " |      3    4\n",
      " |      dtype: int64\n",
      " |\n",
      " |      >>> s.add_suffix('_item')\n",
      " |      0_item    1\n",
      " |      1_item    2\n",
      " |      2_item    3\n",
      " |      3_item    4\n",
      " |      dtype: int64\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})\n",
      " |      >>> df\n",
      " |         A  B\n",
      " |      0  1  3\n",
      " |      1  2  4\n",
      " |      2  3  5\n",
      " |      3  4  6\n",
      " |\n",
      " |      >>> df.add_suffix('_col')\n",
      " |           A_col  B_col\n",
      " |      0       1       3\n",
      " |      1       2       4\n",
      " |      2       3       5\n",
      " |      3       4       6\n",
      " |\n",
      " |  align(self, other: 'NDFrameT', join: 'AlignJoin' = 'outer', axis: 'Axis | None' = None, level: 'Level | None' = None, copy: 'bool_t | None' = None, fill_value: 'Hashable | None' = None, method: 'FillnaOptions | None | lib.NoDefault' = <no_default>, limit: 'int | None | lib.NoDefault' = <no_default>, fill_axis: 'Axis | lib.NoDefault' = <no_default>, broadcast_axis: 'Axis | None | lib.NoDefault' = <no_default>) -> 'tuple[Self, NDFrameT]'\n",
      " |      Align two objects on their axes with the specified join method.\n",
      " |\n",
      " |      Join method is specified for each axis Index.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : DataFrame or Series\n",
      " |      join : {'outer', 'inner', 'left', 'right'}, default 'outer'\n",
      " |          Type of alignment to be performed.\n",
      " |\n",
      " |          * left: use only keys from left frame, preserve key order.\n",
      " |          * right: use only keys from right frame, preserve key order.\n",
      " |          * outer: use union of keys from both frames, sort keys lexicographically.\n",
      " |          * inner: use intersection of keys from both frames,\n",
      " |            preserve the order of the left keys.\n",
      " |\n",
      " |      axis : allowed axis of the other object, default None\n",
      " |          Align on index (0), columns (1), or both (None).\n",
      " |      level : int or level name, default None\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      copy : bool, default True\n",
      " |          Always returns new objects. If copy=False and no reindexing is\n",
      " |          required then original objects are returned.\n",
      " |\n",
      " |          .. note::\n",
      " |              The `copy` keyword will change behavior in pandas 3.0.\n",
      " |              `Copy-on-Write\n",
      " |              <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n",
      " |              will be enabled by default, which means that all methods with a\n",
      " |              `copy` keyword will use a lazy copy mechanism to defer the copy and\n",
      " |              ignore the `copy` keyword. The `copy` keyword will be removed in a\n",
      " |              future version of pandas.\n",
      " |\n",
      " |              You can already get the future behavior and improvements through\n",
      " |              enabling copy on write ``pd.options.mode.copy_on_write = True``\n",
      " |      fill_value : scalar, default np.nan\n",
      " |          Value to use for missing values. Defaults to NaN, but can be any\n",
      " |          \"compatible\" value.\n",
      " |      method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n",
      " |          Method to use for filling holes in reindexed Series:\n",
      " |\n",
      " |          - pad / ffill: propagate last valid observation forward to next valid.\n",
      " |          - backfill / bfill: use NEXT valid observation to fill gap.\n",
      " |\n",
      " |          .. deprecated:: 2.1\n",
      " |\n",
      " |      limit : int, default None\n",
      " |          If method is specified, this is the maximum number of consecutive\n",
      " |          NaN values to forward/backward fill. In other words, if there is\n",
      " |          a gap with more than this number of consecutive NaNs, it will only\n",
      " |          be partially filled. If method is not specified, this is the\n",
      " |          maximum number of entries along the entire axis where NaNs will be\n",
      " |          filled. Must be greater than 0 if not None.\n",
      " |\n",
      " |          .. deprecated:: 2.1\n",
      " |\n",
      " |      fill_axis : {0 or 'index'} for Series, {0 or 'index', 1 or 'columns'} for DataFrame, default 0\n",
      " |          Filling axis, method and limit.\n",
      " |\n",
      " |          .. deprecated:: 2.1\n",
      " |\n",
      " |      broadcast_axis : {0 or 'index'} for Series, {0 or 'index', 1 or 'columns'} for DataFrame, default None\n",
      " |          Broadcast values along this axis, if aligning two objects of\n",
      " |          different dimensions.\n",
      " |\n",
      " |          .. deprecated:: 2.1\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      tuple of (Series/DataFrame, type of other)\n",
      " |          Aligned objects.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame(\n",
      " |      ...     [[1, 2, 3, 4], [6, 7, 8, 9]], columns=[\"D\", \"B\", \"E\", \"A\"], index=[1, 2]\n",
      " |      ... )\n",
      " |      >>> other = pd.DataFrame(\n",
      " |      ...     [[10, 20, 30, 40], [60, 70, 80, 90], [600, 700, 800, 900]],\n",
      " |      ...     columns=[\"A\", \"B\", \"C\", \"D\"],\n",
      " |      ...     index=[2, 3, 4],\n",
      " |      ... )\n",
      " |      >>> df\n",
      " |         D  B  E  A\n",
      " |      1  1  2  3  4\n",
      " |      2  6  7  8  9\n",
      " |      >>> other\n",
      " |          A    B    C    D\n",
      " |      2   10   20   30   40\n",
      " |      3   60   70   80   90\n",
      " |      4  600  700  800  900\n",
      " |\n",
      " |      Align on columns:\n",
      " |\n",
      " |      >>> left, right = df.align(other, join=\"outer\", axis=1)\n",
      " |      >>> left\n",
      " |         A  B   C  D  E\n",
      " |      1  4  2 NaN  1  3\n",
      " |      2  9  7 NaN  6  8\n",
      " |      >>> right\n",
      " |          A    B    C    D   E\n",
      " |      2   10   20   30   40 NaN\n",
      " |      3   60   70   80   90 NaN\n",
      " |      4  600  700  800  900 NaN\n",
      " |\n",
      " |      We can also align on the index:\n",
      " |\n",
      " |      >>> left, right = df.align(other, join=\"outer\", axis=0)\n",
      " |      >>> left\n",
      " |          D    B    E    A\n",
      " |      1  1.0  2.0  3.0  4.0\n",
      " |      2  6.0  7.0  8.0  9.0\n",
      " |      3  NaN  NaN  NaN  NaN\n",
      " |      4  NaN  NaN  NaN  NaN\n",
      " |      >>> right\n",
      " |          A      B      C      D\n",
      " |      1    NaN    NaN    NaN    NaN\n",
      " |      2   10.0   20.0   30.0   40.0\n",
      " |      3   60.0   70.0   80.0   90.0\n",
      " |      4  600.0  700.0  800.0  900.0\n",
      " |\n",
      " |      Finally, the default `axis=None` will align on both index and columns:\n",
      " |\n",
      " |      >>> left, right = df.align(other, join=\"outer\", axis=None)\n",
      " |      >>> left\n",
      " |           A    B   C    D    E\n",
      " |      1  4.0  2.0 NaN  1.0  3.0\n",
      " |      2  9.0  7.0 NaN  6.0  8.0\n",
      " |      3  NaN  NaN NaN  NaN  NaN\n",
      " |      4  NaN  NaN NaN  NaN  NaN\n",
      " |      >>> right\n",
      " |             A      B      C      D   E\n",
      " |      1    NaN    NaN    NaN    NaN NaN\n",
      " |      2   10.0   20.0   30.0   40.0 NaN\n",
      " |      3   60.0   70.0   80.0   90.0 NaN\n",
      " |      4  600.0  700.0  800.0  900.0 NaN\n",
      " |\n",
      " |  asfreq(self, freq: 'Frequency', method: 'FillnaOptions | None' = None, how: \"Literal['start', 'end'] | None\" = None, normalize: 'bool_t' = False, fill_value: 'Hashable | None' = None) -> 'Self'\n",
      " |      Convert time series to specified frequency.\n",
      " |\n",
      " |      Returns the original data conformed to a new index with the specified\n",
      " |      frequency.\n",
      " |\n",
      " |      If the index of this Series/DataFrame is a :class:`~pandas.PeriodIndex`, the new index\n",
      " |      is the result of transforming the original index with\n",
      " |      :meth:`PeriodIndex.asfreq <pandas.PeriodIndex.asfreq>` (so the original index\n",
      " |      will map one-to-one to the new index).\n",
      " |\n",
      " |      Otherwise, the new index will be equivalent to ``pd.date_range(start, end,\n",
      " |      freq=freq)`` where ``start`` and ``end`` are, respectively, the first and\n",
      " |      last entries in the original index (see :func:`pandas.date_range`). The\n",
      " |      values corresponding to any timesteps in the new index which were not present\n",
      " |      in the original index will be null (``NaN``), unless a method for filling\n",
      " |      such unknowns is provided (see the ``method`` parameter below).\n",
      " |\n",
      " |      The :meth:`resample` method is more appropriate if an operation on each group of\n",
      " |      timesteps (such as an aggregate) is necessary to represent the data at the new\n",
      " |      frequency.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      freq : DateOffset or str\n",
      " |          Frequency DateOffset or string.\n",
      " |      method : {'backfill'/'bfill', 'pad'/'ffill'}, default None\n",
      " |          Method to use for filling holes in reindexed Series (note this\n",
      " |          does not fill NaNs that already were present):\n",
      " |\n",
      " |          * 'pad' / 'ffill': propagate last valid observation forward to next\n",
      " |            valid\n",
      " |          * 'backfill' / 'bfill': use NEXT valid observation to fill.\n",
      " |      how : {'start', 'end'}, default end\n",
      " |          For PeriodIndex only (see PeriodIndex.asfreq).\n",
      " |      normalize : bool, default False\n",
      " |          Whether to reset output index to midnight.\n",
      " |      fill_value : scalar, optional\n",
      " |          Value to use for missing values, applied during upsampling (note\n",
      " |          this does not fill NaNs that already were present).\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series/DataFrame\n",
      " |          Series/DataFrame object reindexed to the specified frequency.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      reindex : Conform DataFrame to new index with optional filling logic.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      To learn more about the frequency strings, please see `this link\n",
      " |      <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Start by creating a series with 4 one minute timestamps.\n",
      " |\n",
      " |      >>> index = pd.date_range('1/1/2000', periods=4, freq='min')\n",
      " |      >>> series = pd.Series([0.0, None, 2.0, 3.0], index=index)\n",
      " |      >>> df = pd.DataFrame({'s': series})\n",
      " |      >>> df\n",
      " |                             s\n",
      " |      2000-01-01 00:00:00    0.0\n",
      " |      2000-01-01 00:01:00    NaN\n",
      " |      2000-01-01 00:02:00    2.0\n",
      " |      2000-01-01 00:03:00    3.0\n",
      " |\n",
      " |      Upsample the series into 30 second bins.\n",
      " |\n",
      " |      >>> df.asfreq(freq='30s')\n",
      " |                             s\n",
      " |      2000-01-01 00:00:00    0.0\n",
      " |      2000-01-01 00:00:30    NaN\n",
      " |      2000-01-01 00:01:00    NaN\n",
      " |      2000-01-01 00:01:30    NaN\n",
      " |      2000-01-01 00:02:00    2.0\n",
      " |      2000-01-01 00:02:30    NaN\n",
      " |      2000-01-01 00:03:00    3.0\n",
      " |\n",
      " |      Upsample again, providing a ``fill value``.\n",
      " |\n",
      " |      >>> df.asfreq(freq='30s', fill_value=9.0)\n",
      " |                             s\n",
      " |      2000-01-01 00:00:00    0.0\n",
      " |      2000-01-01 00:00:30    9.0\n",
      " |      2000-01-01 00:01:00    NaN\n",
      " |      2000-01-01 00:01:30    9.0\n",
      " |      2000-01-01 00:02:00    2.0\n",
      " |      2000-01-01 00:02:30    9.0\n",
      " |      2000-01-01 00:03:00    3.0\n",
      " |\n",
      " |      Upsample again, providing a ``method``.\n",
      " |\n",
      " |      >>> df.asfreq(freq='30s', method='bfill')\n",
      " |                             s\n",
      " |      2000-01-01 00:00:00    0.0\n",
      " |      2000-01-01 00:00:30    NaN\n",
      " |      2000-01-01 00:01:00    NaN\n",
      " |      2000-01-01 00:01:30    2.0\n",
      " |      2000-01-01 00:02:00    2.0\n",
      " |      2000-01-01 00:02:30    3.0\n",
      " |      2000-01-01 00:03:00    3.0\n",
      " |\n",
      " |  asof(self, where, subset=None)\n",
      " |      Return the last row(s) without any NaNs before `where`.\n",
      " |\n",
      " |      The last row (for each element in `where`, if list) without any\n",
      " |      NaN is taken.\n",
      " |      In case of a :class:`~pandas.DataFrame`, the last row without NaN\n",
      " |      considering only the subset of columns (if not `None`)\n",
      " |\n",
      " |      If there is no good value, NaN is returned for a Series or\n",
      " |      a Series of NaN values for a DataFrame\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      where : date or array-like of dates\n",
      " |          Date(s) before which the last row(s) are returned.\n",
      " |      subset : str or array-like of str, default `None`\n",
      " |          For DataFrame, if not `None`, only use these columns to\n",
      " |          check for NaNs.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      scalar, Series, or DataFrame\n",
      " |\n",
      " |          The return can be:\n",
      " |\n",
      " |          * scalar : when `self` is a Series and `where` is a scalar\n",
      " |          * Series: when `self` is a Series and `where` is an array-like,\n",
      " |            or when `self` is a DataFrame and `where` is a scalar\n",
      " |          * DataFrame : when `self` is a DataFrame and `where` is an\n",
      " |            array-like\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      merge_asof : Perform an asof merge. Similar to left join.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Dates are assumed to be sorted. Raises if this is not the case.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      A Series and a scalar `where`.\n",
      " |\n",
      " |      >>> s = pd.Series([1, 2, np.nan, 4], index=[10, 20, 30, 40])\n",
      " |      >>> s\n",
      " |      10    1.0\n",
      " |      20    2.0\n",
      " |      30    NaN\n",
      " |      40    4.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |      >>> s.asof(20)\n",
      " |      2.0\n",
      " |\n",
      " |      For a sequence `where`, a Series is returned. The first value is\n",
      " |      NaN, because the first element of `where` is before the first\n",
      " |      index value.\n",
      " |\n",
      " |      >>> s.asof([5, 20])\n",
      " |      5     NaN\n",
      " |      20    2.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |      Missing values are not considered. The following is ``2.0``, not\n",
      " |      NaN, even though NaN is at the index location for ``30``.\n",
      " |\n",
      " |      >>> s.asof(30)\n",
      " |      2.0\n",
      " |\n",
      " |      Take all columns into consideration\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'a': [10., 20., 30., 40., 50.],\n",
      " |      ...                    'b': [None, None, None, None, 500]},\n",
      " |      ...                   index=pd.DatetimeIndex(['2018-02-27 09:01:00',\n",
      " |      ...                                           '2018-02-27 09:02:00',\n",
      " |      ...                                           '2018-02-27 09:03:00',\n",
      " |      ...                                           '2018-02-27 09:04:00',\n",
      " |      ...                                           '2018-02-27 09:05:00']))\n",
      " |      >>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',\n",
      " |      ...                           '2018-02-27 09:04:30']))\n",
      " |                            a   b\n",
      " |      2018-02-27 09:03:30 NaN NaN\n",
      " |      2018-02-27 09:04:30 NaN NaN\n",
      " |\n",
      " |      Take a single column into consideration\n",
      " |\n",
      " |      >>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',\n",
      " |      ...                           '2018-02-27 09:04:30']),\n",
      " |      ...         subset=['a'])\n",
      " |                              a   b\n",
      " |      2018-02-27 09:03:30  30.0 NaN\n",
      " |      2018-02-27 09:04:30  40.0 NaN\n",
      " |\n",
      " |  at_time(self, time, asof: 'bool_t' = False, axis: 'Axis | None' = None) -> 'Self'\n",
      " |      Select values at particular time of day (e.g., 9:30AM).\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      time : datetime.time or str\n",
      " |          The values to select.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      TypeError\n",
      " |          If the index is not  a :class:`DatetimeIndex`\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      between_time : Select values between particular times of the day.\n",
      " |      first : Select initial periods of time series based on a date offset.\n",
      " |      last : Select final periods of time series based on a date offset.\n",
      " |      DatetimeIndex.indexer_at_time : Get just the index locations for\n",
      " |          values at particular time of the day.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> i = pd.date_range('2018-04-09', periods=4, freq='12h')\n",
      " |      >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n",
      " |      >>> ts\n",
      " |                           A\n",
      " |      2018-04-09 00:00:00  1\n",
      " |      2018-04-09 12:00:00  2\n",
      " |      2018-04-10 00:00:00  3\n",
      " |      2018-04-10 12:00:00  4\n",
      " |\n",
      " |      >>> ts.at_time('12:00')\n",
      " |                           A\n",
      " |      2018-04-09 12:00:00  2\n",
      " |      2018-04-10 12:00:00  4\n",
      " |\n",
      " |  backfill(self, *, axis: 'None | Axis' = None, inplace: 'bool_t' = False, limit: 'None | int' = None, downcast: 'dict | None | lib.NoDefault' = <no_default>) -> 'Self | None'\n",
      " |      Fill NA/NaN values by using the next valid observation to fill the gap.\n",
      " |\n",
      " |      .. deprecated:: 2.0\n",
      " |\n",
      " |          Series/DataFrame.backfill is deprecated. Use Series/DataFrame.bfill instead.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series/DataFrame or None\n",
      " |          Object with missing values filled or None if ``inplace=True``.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Please see examples for :meth:`DataFrame.bfill` or :meth:`Series.bfill`.\n",
      " |\n",
      " |  between_time(self, start_time, end_time, inclusive: 'IntervalClosedType' = 'both', axis: 'Axis | None' = None) -> 'Self'\n",
      " |      Select values between particular times of the day (e.g., 9:00-9:30 AM).\n",
      " |\n",
      " |      By setting ``start_time`` to be later than ``end_time``,\n",
      " |      you can get the times that are *not* between the two times.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      start_time : datetime.time or str\n",
      " |          Initial time as a time filter limit.\n",
      " |      end_time : datetime.time or str\n",
      " |          End time as a time filter limit.\n",
      " |      inclusive : {\"both\", \"neither\", \"left\", \"right\"}, default \"both\"\n",
      " |          Include boundaries; whether to set each bound as closed or open.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          Determine range time on index or columns value.\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          Data from the original object filtered to the specified dates range.\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      TypeError\n",
      " |          If the index is not  a :class:`DatetimeIndex`\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      at_time : Select values at a particular time of the day.\n",
      " |      first : Select initial periods of time series based on a date offset.\n",
      " |      last : Select final periods of time series based on a date offset.\n",
      " |      DatetimeIndex.indexer_between_time : Get just the index locations for\n",
      " |          values between particular times of the day.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> i = pd.date_range('2018-04-09', periods=4, freq='1D20min')\n",
      " |      >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n",
      " |      >>> ts\n",
      " |                           A\n",
      " |      2018-04-09 00:00:00  1\n",
      " |      2018-04-10 00:20:00  2\n",
      " |      2018-04-11 00:40:00  3\n",
      " |      2018-04-12 01:00:00  4\n",
      " |\n",
      " |      >>> ts.between_time('0:15', '0:45')\n",
      " |                           A\n",
      " |      2018-04-10 00:20:00  2\n",
      " |      2018-04-11 00:40:00  3\n",
      " |\n",
      " |      You get the times that are *not* between two times by setting\n",
      " |      ``start_time`` later than ``end_time``:\n",
      " |\n",
      " |      >>> ts.between_time('0:45', '0:15')\n",
      " |                           A\n",
      " |      2018-04-09 00:00:00  1\n",
      " |      2018-04-12 01:00:00  4\n",
      " |\n",
      " |  bfill(self, *, axis: 'None | Axis' = None, inplace: 'bool_t' = False, limit: 'None | int' = None, limit_area: \"Literal['inside', 'outside'] | None\" = None, downcast: 'dict | None | lib.NoDefault' = <no_default>) -> 'Self | None'\n",
      " |      Fill NA/NaN values by using the next valid observation to fill the gap.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index'} for Series, {0 or 'index', 1 or 'columns'} for DataFrame\n",
      " |          Axis along which to fill missing values. For `Series`\n",
      " |          this parameter is unused and defaults to 0.\n",
      " |      inplace : bool, default False\n",
      " |          If True, fill in-place. Note: this will modify any\n",
      " |          other views on this object (e.g., a no-copy slice for a column in a\n",
      " |          DataFrame).\n",
      " |      limit : int, default None\n",
      " |          If method is specified, this is the maximum number of consecutive\n",
      " |          NaN values to forward/backward fill. In other words, if there is\n",
      " |          a gap with more than this number of consecutive NaNs, it will only\n",
      " |          be partially filled. If method is not specified, this is the\n",
      " |          maximum number of entries along the entire axis where NaNs will be\n",
      " |          filled. Must be greater than 0 if not None.\n",
      " |      limit_area : {`None`, 'inside', 'outside'}, default None\n",
      " |          If limit is specified, consecutive NaNs will be filled with this\n",
      " |          restriction.\n",
      " |\n",
      " |          * ``None``: No fill restriction.\n",
      " |          * 'inside': Only fill NaNs surrounded by valid values\n",
      " |            (interpolate).\n",
      " |          * 'outside': Only fill NaNs outside valid values (extrapolate).\n",
      " |\n",
      " |          .. versionadded:: 2.2.0\n",
      " |\n",
      " |      downcast : dict, default is None\n",
      " |          A dict of item->dtype of what to downcast if possible,\n",
      " |          or the string 'infer' which will try to downcast to an appropriate\n",
      " |          equal type (e.g. float64 to int64 if possible).\n",
      " |\n",
      " |          .. deprecated:: 2.2.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series/DataFrame or None\n",
      " |          Object with missing values filled or None if ``inplace=True``.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      For Series:\n",
      " |\n",
      " |      >>> s = pd.Series([1, None, None, 2])\n",
      " |      >>> s.bfill()\n",
      " |      0    1.0\n",
      " |      1    2.0\n",
      " |      2    2.0\n",
      " |      3    2.0\n",
      " |      dtype: float64\n",
      " |      >>> s.bfill(limit=1)\n",
      " |      0    1.0\n",
      " |      1    NaN\n",
      " |      2    2.0\n",
      " |      3    2.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |      With DataFrame:\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'A': [1, None, None, 4], 'B': [None, 5, None, 7]})\n",
      " |      >>> df\n",
      " |            A     B\n",
      " |      0   1.0   NaN\n",
      " |      1   NaN   5.0\n",
      " |      2   NaN   NaN\n",
      " |      3   4.0   7.0\n",
      " |      >>> df.bfill()\n",
      " |            A     B\n",
      " |      0   1.0   5.0\n",
      " |      1   4.0   5.0\n",
      " |      2   4.0   7.0\n",
      " |      3   4.0   7.0\n",
      " |      >>> df.bfill(limit=1)\n",
      " |            A     B\n",
      " |      0   1.0   5.0\n",
      " |      1   NaN   5.0\n",
      " |      2   4.0   7.0\n",
      " |      3   4.0   7.0\n",
      " |\n",
      " |  bool(self) -> 'bool_t'\n",
      " |      Return the bool of a single element Series or DataFrame.\n",
      " |\n",
      " |      .. deprecated:: 2.1.0\n",
      " |\n",
      " |         bool is deprecated and will be removed in future version of pandas.\n",
      " |         For ``Series`` use ``pandas.Series.item``.\n",
      " |\n",
      " |      This must be a boolean scalar value, either True or False. It will raise a\n",
      " |      ValueError if the Series or DataFrame does not have exactly 1 element, or that\n",
      " |      element is not boolean (integer values 0 and 1 will also raise an exception).\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          The value in the Series or DataFrame.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.astype : Change the data type of a Series, including to boolean.\n",
      " |      DataFrame.astype : Change the data type of a DataFrame, including to boolean.\n",
      " |      numpy.bool_ : NumPy boolean data type, used by pandas for boolean values.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      The method will only work for single element objects with a boolean value:\n",
      " |\n",
      " |      >>> pd.Series([True]).bool()  # doctest: +SKIP\n",
      " |      True\n",
      " |      >>> pd.Series([False]).bool()  # doctest: +SKIP\n",
      " |      False\n",
      " |\n",
      " |      >>> pd.DataFrame({'col': [True]}).bool()  # doctest: +SKIP\n",
      " |      True\n",
      " |      >>> pd.DataFrame({'col': [False]}).bool()  # doctest: +SKIP\n",
      " |      False\n",
      " |\n",
      " |      This is an alternative method and will only work\n",
      " |      for single element objects with a boolean value:\n",
      " |\n",
      " |      >>> pd.Series([True]).item()  # doctest: +SKIP\n",
      " |      True\n",
      " |      >>> pd.Series([False]).item()  # doctest: +SKIP\n",
      " |      False\n",
      " |\n",
      " |  describe(self, percentiles=None, include=None, exclude=None) -> 'Self'\n",
      " |      Generate descriptive statistics.\n",
      " |\n",
      " |      Descriptive statistics include those that summarize the central\n",
      " |      tendency, dispersion and shape of a\n",
      " |      dataset's distribution, excluding ``NaN`` values.\n",
      " |\n",
      " |      Analyzes both numeric and object series, as well\n",
      " |      as ``DataFrame`` column sets of mixed data types. The output\n",
      " |      will vary depending on what is provided. Refer to the notes\n",
      " |      below for more detail.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      percentiles : list-like of numbers, optional\n",
      " |          The percentiles to include in the output. All should\n",
      " |          fall between 0 and 1. The default is\n",
      " |          ``[.25, .5, .75]``, which returns the 25th, 50th, and\n",
      " |          75th percentiles.\n",
      " |      include : 'all', list-like of dtypes or None (default), optional\n",
      " |          A white list of data types to include in the result. Ignored\n",
      " |          for ``Series``. Here are the options:\n",
      " |\n",
      " |          - 'all' : All columns of the input will be included in the output.\n",
      " |          - A list-like of dtypes : Limits the results to the\n",
      " |            provided data types.\n",
      " |            To limit the result to numeric types submit\n",
      " |            ``numpy.number``. To limit it instead to object columns submit\n",
      " |            the ``numpy.object`` data type. Strings\n",
      " |            can also be used in the style of\n",
      " |            ``select_dtypes`` (e.g. ``df.describe(include=['O'])``). To\n",
      " |            select pandas categorical columns, use ``'category'``\n",
      " |          - None (default) : The result will include all numeric columns.\n",
      " |      exclude : list-like of dtypes or None (default), optional,\n",
      " |          A black list of data types to omit from the result. Ignored\n",
      " |          for ``Series``. Here are the options:\n",
      " |\n",
      " |          - A list-like of dtypes : Excludes the provided data types\n",
      " |            from the result. To exclude numeric types submit\n",
      " |            ``numpy.number``. To exclude object columns submit the data\n",
      " |            type ``numpy.object``. Strings can also be used in the style of\n",
      " |            ``select_dtypes`` (e.g. ``df.describe(exclude=['O'])``). To\n",
      " |            exclude pandas categorical columns, use ``'category'``\n",
      " |          - None (default) : The result will exclude nothing.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          Summary statistics of the Series or Dataframe provided.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.count: Count number of non-NA/null observations.\n",
      " |      DataFrame.max: Maximum of the values in the object.\n",
      " |      DataFrame.min: Minimum of the values in the object.\n",
      " |      DataFrame.mean: Mean of the values.\n",
      " |      DataFrame.std: Standard deviation of the observations.\n",
      " |      DataFrame.select_dtypes: Subset of a DataFrame including/excluding\n",
      " |          columns based on their dtype.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      For numeric data, the result's index will include ``count``,\n",
      " |      ``mean``, ``std``, ``min``, ``max`` as well as lower, ``50`` and\n",
      " |      upper percentiles. By default the lower percentile is ``25`` and the\n",
      " |      upper percentile is ``75``. The ``50`` percentile is the\n",
      " |      same as the median.\n",
      " |\n",
      " |      For object data (e.g. strings or timestamps), the result's index\n",
      " |      will include ``count``, ``unique``, ``top``, and ``freq``. The ``top``\n",
      " |      is the most common value. The ``freq`` is the most common value's\n",
      " |      frequency. Timestamps also include the ``first`` and ``last`` items.\n",
      " |\n",
      " |      If multiple object values have the highest count, then the\n",
      " |      ``count`` and ``top`` results will be arbitrarily chosen from\n",
      " |      among those with the highest count.\n",
      " |\n",
      " |      For mixed data types provided via a ``DataFrame``, the default is to\n",
      " |      return only an analysis of numeric columns. If the dataframe consists\n",
      " |      only of object and categorical data without any numeric columns, the\n",
      " |      default is to return an analysis of both the object and categorical\n",
      " |      columns. If ``include='all'`` is provided as an option, the result\n",
      " |      will include a union of attributes of each type.\n",
      " |\n",
      " |      The `include` and `exclude` parameters can be used to limit\n",
      " |      which columns in a ``DataFrame`` are analyzed for the output.\n",
      " |      The parameters are ignored when analyzing a ``Series``.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Describing a numeric ``Series``.\n",
      " |\n",
      " |      >>> s = pd.Series([1, 2, 3])\n",
      " |      >>> s.describe()\n",
      " |      count    3.0\n",
      " |      mean     2.0\n",
      " |      std      1.0\n",
      " |      min      1.0\n",
      " |      25%      1.5\n",
      " |      50%      2.0\n",
      " |      75%      2.5\n",
      " |      max      3.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |      Describing a categorical ``Series``.\n",
      " |\n",
      " |      >>> s = pd.Series(['a', 'a', 'b', 'c'])\n",
      " |      >>> s.describe()\n",
      " |      count     4\n",
      " |      unique    3\n",
      " |      top       a\n",
      " |      freq      2\n",
      " |      dtype: object\n",
      " |\n",
      " |      Describing a timestamp ``Series``.\n",
      " |\n",
      " |      >>> s = pd.Series([\n",
      " |      ...     np.datetime64(\"2000-01-01\"),\n",
      " |      ...     np.datetime64(\"2010-01-01\"),\n",
      " |      ...     np.datetime64(\"2010-01-01\")\n",
      " |      ... ])\n",
      " |      >>> s.describe()\n",
      " |      count                      3\n",
      " |      mean     2006-09-01 08:00:00\n",
      " |      min      2000-01-01 00:00:00\n",
      " |      25%      2004-12-31 12:00:00\n",
      " |      50%      2010-01-01 00:00:00\n",
      " |      75%      2010-01-01 00:00:00\n",
      " |      max      2010-01-01 00:00:00\n",
      " |      dtype: object\n",
      " |\n",
      " |      Describing a ``DataFrame``. By default only numeric fields\n",
      " |      are returned.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'categorical': pd.Categorical(['d', 'e', 'f']),\n",
      " |      ...                    'numeric': [1, 2, 3],\n",
      " |      ...                    'object': ['a', 'b', 'c']\n",
      " |      ...                    })\n",
      " |      >>> df.describe()\n",
      " |             numeric\n",
      " |      count      3.0\n",
      " |      mean       2.0\n",
      " |      std        1.0\n",
      " |      min        1.0\n",
      " |      25%        1.5\n",
      " |      50%        2.0\n",
      " |      75%        2.5\n",
      " |      max        3.0\n",
      " |\n",
      " |      Describing all columns of a ``DataFrame`` regardless of data type.\n",
      " |\n",
      " |      >>> df.describe(include='all')  # doctest: +SKIP\n",
      " |             categorical  numeric object\n",
      " |      count            3      3.0      3\n",
      " |      unique           3      NaN      3\n",
      " |      top              f      NaN      a\n",
      " |      freq             1      NaN      1\n",
      " |      mean           NaN      2.0    NaN\n",
      " |      std            NaN      1.0    NaN\n",
      " |      min            NaN      1.0    NaN\n",
      " |      25%            NaN      1.5    NaN\n",
      " |      50%            NaN      2.0    NaN\n",
      " |      75%            NaN      2.5    NaN\n",
      " |      max            NaN      3.0    NaN\n",
      " |\n",
      " |      Describing a column from a ``DataFrame`` by accessing it as\n",
      " |      an attribute.\n",
      " |\n",
      " |      >>> df.numeric.describe()\n",
      " |      count    3.0\n",
      " |      mean     2.0\n",
      " |      std      1.0\n",
      " |      min      1.0\n",
      " |      25%      1.5\n",
      " |      50%      2.0\n",
      " |      75%      2.5\n",
      " |      max      3.0\n",
      " |      Name: numeric, dtype: float64\n",
      " |\n",
      " |      Including only numeric columns in a ``DataFrame`` description.\n",
      " |\n",
      " |      >>> df.describe(include=[np.number])\n",
      " |             numeric\n",
      " |      count      3.0\n",
      " |      mean       2.0\n",
      " |      std        1.0\n",
      " |      min        1.0\n",
      " |      25%        1.5\n",
      " |      50%        2.0\n",
      " |      75%        2.5\n",
      " |      max        3.0\n",
      " |\n",
      " |      Including only string columns in a ``DataFrame`` description.\n",
      " |\n",
      " |      >>> df.describe(include=[object])  # doctest: +SKIP\n",
      " |             object\n",
      " |      count       3\n",
      " |      unique      3\n",
      " |      top         a\n",
      " |      freq        1\n",
      " |\n",
      " |      Including only categorical columns from a ``DataFrame`` description.\n",
      " |\n",
      " |      >>> df.describe(include=['category'])\n",
      " |             categorical\n",
      " |      count            3\n",
      " |      unique           3\n",
      " |      top              d\n",
      " |      freq             1\n",
      " |\n",
      " |      Excluding numeric columns from a ``DataFrame`` description.\n",
      " |\n",
      " |      >>> df.describe(exclude=[np.number])  # doctest: +SKIP\n",
      " |             categorical object\n",
      " |      count            3      3\n",
      " |      unique           3      3\n",
      " |      top              f      a\n",
      " |      freq             1      1\n",
      " |\n",
      " |      Excluding object columns from a ``DataFrame`` description.\n",
      " |\n",
      " |      >>> df.describe(exclude=[object])  # doctest: +SKIP\n",
      " |             categorical  numeric\n",
      " |      count            3      3.0\n",
      " |      unique           3      NaN\n",
      " |      top              f      NaN\n",
      " |      freq             1      NaN\n",
      " |      mean           NaN      2.0\n",
      " |      std            NaN      1.0\n",
      " |      min            NaN      1.0\n",
      " |      25%            NaN      1.5\n",
      " |      50%            NaN      2.0\n",
      " |      75%            NaN      2.5\n",
      " |      max            NaN      3.0\n",
      " |\n",
      " |  droplevel(self, level: 'IndexLabel', axis: 'Axis' = 0) -> 'Self'\n",
      " |      Return Series/DataFrame with requested index / column level(s) removed.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      level : int, str, or list-like\n",
      " |          If a string is given, must be the name of a level\n",
      " |          If list-like, elements must be names or positional indexes\n",
      " |          of levels.\n",
      " |\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          Axis along which the level(s) is removed:\n",
      " |\n",
      " |          * 0 or 'index': remove level(s) in column.\n",
      " |          * 1 or 'columns': remove level(s) in row.\n",
      " |\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series/DataFrame\n",
      " |          Series/DataFrame with requested index / column level(s) removed.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([\n",
      " |      ...     [1, 2, 3, 4],\n",
      " |      ...     [5, 6, 7, 8],\n",
      " |      ...     [9, 10, 11, 12]\n",
      " |      ... ]).set_index([0, 1]).rename_axis(['a', 'b'])\n",
      " |\n",
      " |      >>> df.columns = pd.MultiIndex.from_tuples([\n",
      " |      ...     ('c', 'e'), ('d', 'f')\n",
      " |      ... ], names=['level_1', 'level_2'])\n",
      " |\n",
      " |      >>> df\n",
      " |      level_1   c   d\n",
      " |      level_2   e   f\n",
      " |      a b\n",
      " |      1 2      3   4\n",
      " |      5 6      7   8\n",
      " |      9 10    11  12\n",
      " |\n",
      " |      >>> df.droplevel('a')\n",
      " |      level_1   c   d\n",
      " |      level_2   e   f\n",
      " |      b\n",
      " |      2        3   4\n",
      " |      6        7   8\n",
      " |      10      11  12\n",
      " |\n",
      " |      >>> df.droplevel('level_2', axis=1)\n",
      " |      level_1   c   d\n",
      " |      a b\n",
      " |      1 2      3   4\n",
      " |      5 6      7   8\n",
      " |      9 10    11  12\n",
      " |\n",
      " |  equals(self, other: 'object') -> 'bool_t'\n",
      " |      Test whether two objects contain the same elements.\n",
      " |\n",
      " |      This function allows two Series or DataFrames to be compared against\n",
      " |      each other to see if they have the same shape and elements. NaNs in\n",
      " |      the same location are considered equal.\n",
      " |\n",
      " |      The row/column index do not need to have the same type, as long\n",
      " |      as the values are considered equal. Corresponding columns and\n",
      " |      index must be of the same dtype.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Series or DataFrame\n",
      " |          The other Series or DataFrame to be compared with the first.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          True if all elements are the same in both objects, False\n",
      " |          otherwise.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.eq : Compare two Series objects of the same length\n",
      " |          and return a Series where each element is True if the element\n",
      " |          in each Series is equal, False otherwise.\n",
      " |      DataFrame.eq : Compare two DataFrame objects of the same shape and\n",
      " |          return a DataFrame where each element is True if the respective\n",
      " |          element in each DataFrame is equal, False otherwise.\n",
      " |      testing.assert_series_equal : Raises an AssertionError if left and\n",
      " |          right are not equal. Provides an easy interface to ignore\n",
      " |          inequality in dtypes, indexes and precision among others.\n",
      " |      testing.assert_frame_equal : Like assert_series_equal, but targets\n",
      " |          DataFrames.\n",
      " |      numpy.array_equal : Return True if two arrays have the same shape\n",
      " |          and elements, False otherwise.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({1: [10], 2: [20]})\n",
      " |      >>> df\n",
      " |          1   2\n",
      " |      0  10  20\n",
      " |\n",
      " |      DataFrames df and exactly_equal have the same types and values for\n",
      " |      their elements and column labels, which will return True.\n",
      " |\n",
      " |      >>> exactly_equal = pd.DataFrame({1: [10], 2: [20]})\n",
      " |      >>> exactly_equal\n",
      " |          1   2\n",
      " |      0  10  20\n",
      " |      >>> df.equals(exactly_equal)\n",
      " |      True\n",
      " |\n",
      " |      DataFrames df and different_column_type have the same element\n",
      " |      types and values, but have different types for the column labels,\n",
      " |      which will still return True.\n",
      " |\n",
      " |      >>> different_column_type = pd.DataFrame({1.0: [10], 2.0: [20]})\n",
      " |      >>> different_column_type\n",
      " |         1.0  2.0\n",
      " |      0   10   20\n",
      " |      >>> df.equals(different_column_type)\n",
      " |      True\n",
      " |\n",
      " |      DataFrames df and different_data_type have different types for the\n",
      " |      same values for their elements, and will return False even though\n",
      " |      their column labels are the same values and types.\n",
      " |\n",
      " |      >>> different_data_type = pd.DataFrame({1: [10.0], 2: [20.0]})\n",
      " |      >>> different_data_type\n",
      " |            1     2\n",
      " |      0  10.0  20.0\n",
      " |      >>> df.equals(different_data_type)\n",
      " |      False\n",
      " |\n",
      " |  ewm(self, com: 'float | None' = None, span: 'float | None' = None, halflife: 'float | TimedeltaConvertibleTypes | None' = None, alpha: 'float | None' = None, min_periods: 'int | None' = 0, adjust: 'bool_t' = True, ignore_na: 'bool_t' = False, axis: 'Axis | lib.NoDefault' = <no_default>, times: 'np.ndarray | DataFrame | Series | None' = None, method: \"Literal['single', 'table']\" = 'single') -> 'ExponentialMovingWindow'\n",
      " |      Provide exponentially weighted (EW) calculations.\n",
      " |\n",
      " |      Exactly one of ``com``, ``span``, ``halflife``, or ``alpha`` must be\n",
      " |      provided if ``times`` is not provided. If ``times`` is provided,\n",
      " |      ``halflife`` and one of ``com``, ``span`` or ``alpha`` may be provided.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      com : float, optional\n",
      " |          Specify decay in terms of center of mass\n",
      " |\n",
      " |          :math:`\\alpha = 1 / (1 + com)`, for :math:`com \\geq 0`.\n",
      " |\n",
      " |      span : float, optional\n",
      " |          Specify decay in terms of span\n",
      " |\n",
      " |          :math:`\\alpha = 2 / (span + 1)`, for :math:`span \\geq 1`.\n",
      " |\n",
      " |      halflife : float, str, timedelta, optional\n",
      " |          Specify decay in terms of half-life\n",
      " |\n",
      " |          :math:`\\alpha = 1 - \\exp\\left(-\\ln(2) / halflife\\right)`, for\n",
      " |          :math:`halflife > 0`.\n",
      " |\n",
      " |          If ``times`` is specified, a timedelta convertible unit over which an\n",
      " |          observation decays to half its value. Only applicable to ``mean()``,\n",
      " |          and halflife value will not apply to the other functions.\n",
      " |\n",
      " |      alpha : float, optional\n",
      " |          Specify smoothing factor :math:`\\alpha` directly\n",
      " |\n",
      " |          :math:`0 < \\alpha \\leq 1`.\n",
      " |\n",
      " |      min_periods : int, default 0\n",
      " |          Minimum number of observations in window required to have a value;\n",
      " |          otherwise, result is ``np.nan``.\n",
      " |\n",
      " |      adjust : bool, default True\n",
      " |          Divide by decaying adjustment factor in beginning periods to account\n",
      " |          for imbalance in relative weightings (viewing EWMA as a moving average).\n",
      " |\n",
      " |          - When ``adjust=True`` (default), the EW function is calculated using weights\n",
      " |            :math:`w_i = (1 - \\alpha)^i`. For example, the EW moving average of the series\n",
      " |            [:math:`x_0, x_1, ..., x_t`] would be:\n",
      " |\n",
      " |          .. math::\n",
      " |              y_t = \\frac{x_t + (1 - \\alpha)x_{t-1} + (1 - \\alpha)^2 x_{t-2} + ... + (1 -\n",
      " |              \\alpha)^t x_0}{1 + (1 - \\alpha) + (1 - \\alpha)^2 + ... + (1 - \\alpha)^t}\n",
      " |\n",
      " |          - When ``adjust=False``, the exponentially weighted function is calculated\n",
      " |            recursively:\n",
      " |\n",
      " |          .. math::\n",
      " |              \\begin{split}\n",
      " |                  y_0 &= x_0\\\\\n",
      " |                  y_t &= (1 - \\alpha) y_{t-1} + \\alpha x_t,\n",
      " |              \\end{split}\n",
      " |      ignore_na : bool, default False\n",
      " |          Ignore missing values when calculating weights.\n",
      " |\n",
      " |          - When ``ignore_na=False`` (default), weights are based on absolute positions.\n",
      " |            For example, the weights of :math:`x_0` and :math:`x_2` used in calculating\n",
      " |            the final weighted average of [:math:`x_0`, None, :math:`x_2`] are\n",
      " |            :math:`(1-\\alpha)^2` and :math:`1` if ``adjust=True``, and\n",
      " |            :math:`(1-\\alpha)^2` and :math:`\\alpha` if ``adjust=False``.\n",
      " |\n",
      " |          - When ``ignore_na=True``, weights are based\n",
      " |            on relative positions. For example, the weights of :math:`x_0` and :math:`x_2`\n",
      " |            used in calculating the final weighted average of\n",
      " |            [:math:`x_0`, None, :math:`x_2`] are :math:`1-\\alpha` and :math:`1` if\n",
      " |            ``adjust=True``, and :math:`1-\\alpha` and :math:`\\alpha` if ``adjust=False``.\n",
      " |\n",
      " |      axis : {0, 1}, default 0\n",
      " |          If ``0`` or ``'index'``, calculate across the rows.\n",
      " |\n",
      " |          If ``1`` or ``'columns'``, calculate across the columns.\n",
      " |\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |\n",
      " |      times : np.ndarray, Series, default None\n",
      " |\n",
      " |          Only applicable to ``mean()``.\n",
      " |\n",
      " |          Times corresponding to the observations. Must be monotonically increasing and\n",
      " |          ``datetime64[ns]`` dtype.\n",
      " |\n",
      " |          If 1-D array like, a sequence with the same shape as the observations.\n",
      " |\n",
      " |      method : str {'single', 'table'}, default 'single'\n",
      " |          .. versionadded:: 1.4.0\n",
      " |\n",
      " |          Execute the rolling operation per single column or row (``'single'``)\n",
      " |          or over the entire object (``'table'``).\n",
      " |\n",
      " |          This argument is only implemented when specifying ``engine='numba'``\n",
      " |          in the method call.\n",
      " |\n",
      " |          Only applicable to ``mean()``\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      pandas.api.typing.ExponentialMovingWindow\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      rolling : Provides rolling window calculations.\n",
      " |      expanding : Provides expanding transformations.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      See :ref:`Windowing Operations <window.exponentially_weighted>`\n",
      " |      for further usage details and examples.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]})\n",
      " |      >>> df\n",
      " |           B\n",
      " |      0  0.0\n",
      " |      1  1.0\n",
      " |      2  2.0\n",
      " |      3  NaN\n",
      " |      4  4.0\n",
      " |\n",
      " |      >>> df.ewm(com=0.5).mean()\n",
      " |                B\n",
      " |      0  0.000000\n",
      " |      1  0.750000\n",
      " |      2  1.615385\n",
      " |      3  1.615385\n",
      " |      4  3.670213\n",
      " |      >>> df.ewm(alpha=2 / 3).mean()\n",
      " |                B\n",
      " |      0  0.000000\n",
      " |      1  0.750000\n",
      " |      2  1.615385\n",
      " |      3  1.615385\n",
      " |      4  3.670213\n",
      " |\n",
      " |      **adjust**\n",
      " |\n",
      " |      >>> df.ewm(com=0.5, adjust=True).mean()\n",
      " |                B\n",
      " |      0  0.000000\n",
      " |      1  0.750000\n",
      " |      2  1.615385\n",
      " |      3  1.615385\n",
      " |      4  3.670213\n",
      " |      >>> df.ewm(com=0.5, adjust=False).mean()\n",
      " |                B\n",
      " |      0  0.000000\n",
      " |      1  0.666667\n",
      " |      2  1.555556\n",
      " |      3  1.555556\n",
      " |      4  3.650794\n",
      " |\n",
      " |      **ignore_na**\n",
      " |\n",
      " |      >>> df.ewm(com=0.5, ignore_na=True).mean()\n",
      " |                B\n",
      " |      0  0.000000\n",
      " |      1  0.750000\n",
      " |      2  1.615385\n",
      " |      3  1.615385\n",
      " |      4  3.225000\n",
      " |      >>> df.ewm(com=0.5, ignore_na=False).mean()\n",
      " |                B\n",
      " |      0  0.000000\n",
      " |      1  0.750000\n",
      " |      2  1.615385\n",
      " |      3  1.615385\n",
      " |      4  3.670213\n",
      " |\n",
      " |      **times**\n",
      " |\n",
      " |      Exponentially weighted mean with weights calculated with a timedelta ``halflife``\n",
      " |      relative to ``times``.\n",
      " |\n",
      " |      >>> times = ['2020-01-01', '2020-01-03', '2020-01-10', '2020-01-15', '2020-01-17']\n",
      " |      >>> df.ewm(halflife='4 days', times=pd.DatetimeIndex(times)).mean()\n",
      " |                B\n",
      " |      0  0.000000\n",
      " |      1  0.585786\n",
      " |      2  1.523889\n",
      " |      3  1.523889\n",
      " |      4  3.233686\n",
      " |\n",
      " |  expanding(self, min_periods: 'int' = 1, axis: 'Axis | lib.NoDefault' = <no_default>, method: \"Literal['single', 'table']\" = 'single') -> 'Expanding'\n",
      " |      Provide expanding window calculations.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      min_periods : int, default 1\n",
      " |          Minimum number of observations in window required to have a value;\n",
      " |          otherwise, result is ``np.nan``.\n",
      " |\n",
      " |      axis : int or str, default 0\n",
      " |          If ``0`` or ``'index'``, roll across the rows.\n",
      " |\n",
      " |          If ``1`` or ``'columns'``, roll across the columns.\n",
      " |\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |\n",
      " |      method : str {'single', 'table'}, default 'single'\n",
      " |          Execute the rolling operation per single column or row (``'single'``)\n",
      " |          or over the entire object (``'table'``).\n",
      " |\n",
      " |          This argument is only implemented when specifying ``engine='numba'``\n",
      " |          in the method call.\n",
      " |\n",
      " |          .. versionadded:: 1.3.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      pandas.api.typing.Expanding\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      rolling : Provides rolling window calculations.\n",
      " |      ewm : Provides exponential weighted functions.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      See :ref:`Windowing Operations <window.expanding>` for further usage details\n",
      " |      and examples.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({\"B\": [0, 1, 2, np.nan, 4]})\n",
      " |      >>> df\n",
      " |           B\n",
      " |      0  0.0\n",
      " |      1  1.0\n",
      " |      2  2.0\n",
      " |      3  NaN\n",
      " |      4  4.0\n",
      " |\n",
      " |      **min_periods**\n",
      " |\n",
      " |      Expanding sum with 1 vs 3 observations needed to calculate a value.\n",
      " |\n",
      " |      >>> df.expanding(1).sum()\n",
      " |           B\n",
      " |      0  0.0\n",
      " |      1  1.0\n",
      " |      2  3.0\n",
      " |      3  3.0\n",
      " |      4  7.0\n",
      " |      >>> df.expanding(3).sum()\n",
      " |           B\n",
      " |      0  NaN\n",
      " |      1  NaN\n",
      " |      2  3.0\n",
      " |      3  3.0\n",
      " |      4  7.0\n",
      " |\n",
      " |  ffill(self, *, axis: 'None | Axis' = None, inplace: 'bool_t' = False, limit: 'None | int' = None, limit_area: \"Literal['inside', 'outside'] | None\" = None, downcast: 'dict | None | lib.NoDefault' = <no_default>) -> 'Self | None'\n",
      " |      Fill NA/NaN values by propagating the last valid observation to next valid.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index'} for Series, {0 or 'index', 1 or 'columns'} for DataFrame\n",
      " |          Axis along which to fill missing values. For `Series`\n",
      " |          this parameter is unused and defaults to 0.\n",
      " |      inplace : bool, default False\n",
      " |          If True, fill in-place. Note: this will modify any\n",
      " |          other views on this object (e.g., a no-copy slice for a column in a\n",
      " |          DataFrame).\n",
      " |      limit : int, default None\n",
      " |          If method is specified, this is the maximum number of consecutive\n",
      " |          NaN values to forward/backward fill. In other words, if there is\n",
      " |          a gap with more than this number of consecutive NaNs, it will only\n",
      " |          be partially filled. If method is not specified, this is the\n",
      " |          maximum number of entries along the entire axis where NaNs will be\n",
      " |          filled. Must be greater than 0 if not None.\n",
      " |      limit_area : {`None`, 'inside', 'outside'}, default None\n",
      " |          If limit is specified, consecutive NaNs will be filled with this\n",
      " |          restriction.\n",
      " |\n",
      " |          * ``None``: No fill restriction.\n",
      " |          * 'inside': Only fill NaNs surrounded by valid values\n",
      " |            (interpolate).\n",
      " |          * 'outside': Only fill NaNs outside valid values (extrapolate).\n",
      " |\n",
      " |          .. versionadded:: 2.2.0\n",
      " |\n",
      " |      downcast : dict, default is None\n",
      " |          A dict of item->dtype of what to downcast if possible,\n",
      " |          or the string 'infer' which will try to downcast to an appropriate\n",
      " |          equal type (e.g. float64 to int64 if possible).\n",
      " |\n",
      " |          .. deprecated:: 2.2.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series/DataFrame or None\n",
      " |          Object with missing values filled or None if ``inplace=True``.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([[np.nan, 2, np.nan, 0],\n",
      " |      ...                    [3, 4, np.nan, 1],\n",
      " |      ...                    [np.nan, np.nan, np.nan, np.nan],\n",
      " |      ...                    [np.nan, 3, np.nan, 4]],\n",
      " |      ...                   columns=list(\"ABCD\"))\n",
      " |      >>> df\n",
      " |           A    B   C    D\n",
      " |      0  NaN  2.0 NaN  0.0\n",
      " |      1  3.0  4.0 NaN  1.0\n",
      " |      2  NaN  NaN NaN  NaN\n",
      " |      3  NaN  3.0 NaN  4.0\n",
      " |\n",
      " |      >>> df.ffill()\n",
      " |           A    B   C    D\n",
      " |      0  NaN  2.0 NaN  0.0\n",
      " |      1  3.0  4.0 NaN  1.0\n",
      " |      2  3.0  4.0 NaN  1.0\n",
      " |      3  3.0  3.0 NaN  4.0\n",
      " |\n",
      " |      >>> ser = pd.Series([1, np.nan, 2, 3])\n",
      " |      >>> ser.ffill()\n",
      " |      0   1.0\n",
      " |      1   1.0\n",
      " |      2   2.0\n",
      " |      3   3.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |  fillna(self, value: 'Hashable | Mapping | Series | DataFrame | None' = None, *, method: 'FillnaOptions | None' = None, axis: 'Axis | None' = None, inplace: 'bool_t' = False, limit: 'int | None' = None, downcast: 'dict | None | lib.NoDefault' = <no_default>) -> 'Self | None'\n",
      " |      Fill NA/NaN values using the specified method.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value : scalar, dict, Series, or DataFrame\n",
      " |          Value to use to fill holes (e.g. 0), alternately a\n",
      " |          dict/Series/DataFrame of values specifying which value to use for\n",
      " |          each index (for a Series) or column (for a DataFrame).  Values not\n",
      " |          in the dict/Series/DataFrame will not be filled. This value cannot\n",
      " |          be a list.\n",
      " |      method : {'backfill', 'bfill', 'ffill', None}, default None\n",
      " |          Method to use for filling holes in reindexed Series:\n",
      " |\n",
      " |          * ffill: propagate last valid observation forward to next valid.\n",
      " |          * backfill / bfill: use next valid observation to fill gap.\n",
      " |\n",
      " |          .. deprecated:: 2.1.0\n",
      " |              Use ffill or bfill instead.\n",
      " |\n",
      " |      axis : {0 or 'index'} for Series, {0 or 'index', 1 or 'columns'} for DataFrame\n",
      " |          Axis along which to fill missing values. For `Series`\n",
      " |          this parameter is unused and defaults to 0.\n",
      " |      inplace : bool, default False\n",
      " |          If True, fill in-place. Note: this will modify any\n",
      " |          other views on this object (e.g., a no-copy slice for a column in a\n",
      " |          DataFrame).\n",
      " |      limit : int, default None\n",
      " |          If method is specified, this is the maximum number of consecutive\n",
      " |          NaN values to forward/backward fill. In other words, if there is\n",
      " |          a gap with more than this number of consecutive NaNs, it will only\n",
      " |          be partially filled. If method is not specified, this is the\n",
      " |          maximum number of entries along the entire axis where NaNs will be\n",
      " |          filled. Must be greater than 0 if not None.\n",
      " |      downcast : dict, default is None\n",
      " |          A dict of item->dtype of what to downcast if possible,\n",
      " |          or the string 'infer' which will try to downcast to an appropriate\n",
      " |          equal type (e.g. float64 to int64 if possible).\n",
      " |\n",
      " |          .. deprecated:: 2.2.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series/DataFrame or None\n",
      " |          Object with missing values filled or None if ``inplace=True``.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      ffill : Fill values by propagating the last valid observation to next valid.\n",
      " |      bfill : Fill values by using the next valid observation to fill the gap.\n",
      " |      interpolate : Fill NaN values using interpolation.\n",
      " |      reindex : Conform object to new index.\n",
      " |      asfreq : Convert TimeSeries to specified frequency.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([[np.nan, 2, np.nan, 0],\n",
      " |      ...                    [3, 4, np.nan, 1],\n",
      " |      ...                    [np.nan, np.nan, np.nan, np.nan],\n",
      " |      ...                    [np.nan, 3, np.nan, 4]],\n",
      " |      ...                   columns=list(\"ABCD\"))\n",
      " |      >>> df\n",
      " |           A    B   C    D\n",
      " |      0  NaN  2.0 NaN  0.0\n",
      " |      1  3.0  4.0 NaN  1.0\n",
      " |      2  NaN  NaN NaN  NaN\n",
      " |      3  NaN  3.0 NaN  4.0\n",
      " |\n",
      " |      Replace all NaN elements with 0s.\n",
      " |\n",
      " |      >>> df.fillna(0)\n",
      " |           A    B    C    D\n",
      " |      0  0.0  2.0  0.0  0.0\n",
      " |      1  3.0  4.0  0.0  1.0\n",
      " |      2  0.0  0.0  0.0  0.0\n",
      " |      3  0.0  3.0  0.0  4.0\n",
      " |\n",
      " |      Replace all NaN elements in column 'A', 'B', 'C', and 'D', with 0, 1,\n",
      " |      2, and 3 respectively.\n",
      " |\n",
      " |      >>> values = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
      " |      >>> df.fillna(value=values)\n",
      " |           A    B    C    D\n",
      " |      0  0.0  2.0  2.0  0.0\n",
      " |      1  3.0  4.0  2.0  1.0\n",
      " |      2  0.0  1.0  2.0  3.0\n",
      " |      3  0.0  3.0  2.0  4.0\n",
      " |\n",
      " |      Only replace the first NaN element.\n",
      " |\n",
      " |      >>> df.fillna(value=values, limit=1)\n",
      " |           A    B    C    D\n",
      " |      0  0.0  2.0  2.0  0.0\n",
      " |      1  3.0  4.0  NaN  1.0\n",
      " |      2  NaN  1.0  NaN  3.0\n",
      " |      3  NaN  3.0  NaN  4.0\n",
      " |\n",
      " |      When filling using a DataFrame, replacement happens along\n",
      " |      the same column names and same indices\n",
      " |\n",
      " |      >>> df2 = pd.DataFrame(np.zeros((4, 4)), columns=list(\"ABCE\"))\n",
      " |      >>> df.fillna(df2)\n",
      " |           A    B    C    D\n",
      " |      0  0.0  2.0  0.0  0.0\n",
      " |      1  3.0  4.0  0.0  1.0\n",
      " |      2  0.0  0.0  0.0  NaN\n",
      " |      3  0.0  3.0  0.0  4.0\n",
      " |\n",
      " |      Note that column D is not affected since it is not present in df2.\n",
      " |\n",
      " |  filter(self, items=None, like: 'str | None' = None, regex: 'str | None' = None, axis: 'Axis | None' = None) -> 'Self'\n",
      " |      Subset the dataframe rows or columns according to the specified index labels.\n",
      " |\n",
      " |      Note that this routine does not filter a dataframe on its\n",
      " |      contents. The filter is applied to the labels of the index.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      items : list-like\n",
      " |          Keep labels from axis which are in items.\n",
      " |      like : str\n",
      " |          Keep labels from axis for which \"like in label == True\".\n",
      " |      regex : str (regular expression)\n",
      " |          Keep labels from axis for which re.search(regex, label) == True.\n",
      " |      axis : {0 or 'index', 1 or 'columns', None}, default None\n",
      " |          The axis to filter on, expressed either as an index (int)\n",
      " |          or axis name (str). By default this is the info axis, 'columns' for\n",
      " |          DataFrame. For `Series` this parameter is unused and defaults to `None`.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      same type as input object\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.loc : Access a group of rows and columns\n",
      " |          by label(s) or a boolean array.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      The ``items``, ``like``, and ``regex`` parameters are\n",
      " |      enforced to be mutually exclusive.\n",
      " |\n",
      " |      ``axis`` defaults to the info axis that is used when indexing\n",
      " |      with ``[]``.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),\n",
      " |      ...                   index=['mouse', 'rabbit'],\n",
      " |      ...                   columns=['one', 'two', 'three'])\n",
      " |      >>> df\n",
      " |              one  two  three\n",
      " |      mouse     1    2      3\n",
      " |      rabbit    4    5      6\n",
      " |\n",
      " |      >>> # select columns by name\n",
      " |      >>> df.filter(items=['one', 'three'])\n",
      " |               one  three\n",
      " |      mouse     1      3\n",
      " |      rabbit    4      6\n",
      " |\n",
      " |      >>> # select columns by regular expression\n",
      " |      >>> df.filter(regex='e$', axis=1)\n",
      " |               one  three\n",
      " |      mouse     1      3\n",
      " |      rabbit    4      6\n",
      " |\n",
      " |      >>> # select rows containing 'bbi'\n",
      " |      >>> df.filter(like='bbi', axis=0)\n",
      " |               one  two  three\n",
      " |      rabbit    4    5      6\n",
      " |\n",
      " |  first(self, offset) -> 'Self'\n",
      " |      Select initial periods of time series data based on a date offset.\n",
      " |\n",
      " |      .. deprecated:: 2.1\n",
      " |          :meth:`.first` is deprecated and will be removed in a future version.\n",
      " |          Please create a mask and filter using `.loc` instead.\n",
      " |\n",
      " |      For a DataFrame with a sorted DatetimeIndex, this function can\n",
      " |      select the first few rows based on a date offset.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      offset : str, DateOffset or dateutil.relativedelta\n",
      " |          The offset length of the data that will be selected. For instance,\n",
      " |          '1ME' will display all the rows having their index within the first month.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          A subset of the caller.\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      TypeError\n",
      " |          If the index is not  a :class:`DatetimeIndex`\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      last : Select final periods of time series based on a date offset.\n",
      " |      at_time : Select values at a particular time of the day.\n",
      " |      between_time : Select values between particular times of the day.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> i = pd.date_range('2018-04-09', periods=4, freq='2D')\n",
      " |      >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n",
      " |      >>> ts\n",
      " |                  A\n",
      " |      2018-04-09  1\n",
      " |      2018-04-11  2\n",
      " |      2018-04-13  3\n",
      " |      2018-04-15  4\n",
      " |\n",
      " |      Get the rows for the first 3 days:\n",
      " |\n",
      " |      >>> ts.first('3D')\n",
      " |                  A\n",
      " |      2018-04-09  1\n",
      " |      2018-04-11  2\n",
      " |\n",
      " |      Notice the data for 3 first calendar days were returned, not the first\n",
      " |      3 days observed in the dataset, and therefore data for 2018-04-13 was\n",
      " |      not returned.\n",
      " |\n",
      " |  first_valid_index(self) -> 'Hashable | None'\n",
      " |      Return index for first non-NA value or None, if no non-NA value is found.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      type of index\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      For Series:\n",
      " |\n",
      " |      >>> s = pd.Series([None, 3, 4])\n",
      " |      >>> s.first_valid_index()\n",
      " |      1\n",
      " |      >>> s.last_valid_index()\n",
      " |      2\n",
      " |\n",
      " |      >>> s = pd.Series([None, None])\n",
      " |      >>> print(s.first_valid_index())\n",
      " |      None\n",
      " |      >>> print(s.last_valid_index())\n",
      " |      None\n",
      " |\n",
      " |      If all elements in Series are NA/null, returns None.\n",
      " |\n",
      " |      >>> s = pd.Series()\n",
      " |      >>> print(s.first_valid_index())\n",
      " |      None\n",
      " |      >>> print(s.last_valid_index())\n",
      " |      None\n",
      " |\n",
      " |      If Series is empty, returns None.\n",
      " |\n",
      " |      For DataFrame:\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'A': [None, None, 2], 'B': [None, 3, 4]})\n",
      " |      >>> df\n",
      " |           A      B\n",
      " |      0  NaN    NaN\n",
      " |      1  NaN    3.0\n",
      " |      2  2.0    4.0\n",
      " |      >>> df.first_valid_index()\n",
      " |      1\n",
      " |      >>> df.last_valid_index()\n",
      " |      2\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'A': [None, None, None], 'B': [None, None, None]})\n",
      " |      >>> df\n",
      " |           A      B\n",
      " |      0  None   None\n",
      " |      1  None   None\n",
      " |      2  None   None\n",
      " |      >>> print(df.first_valid_index())\n",
      " |      None\n",
      " |      >>> print(df.last_valid_index())\n",
      " |      None\n",
      " |\n",
      " |      If all elements in DataFrame are NA/null, returns None.\n",
      " |\n",
      " |      >>> df = pd.DataFrame()\n",
      " |      >>> df\n",
      " |      Empty DataFrame\n",
      " |      Columns: []\n",
      " |      Index: []\n",
      " |      >>> print(df.first_valid_index())\n",
      " |      None\n",
      " |      >>> print(df.last_valid_index())\n",
      " |      None\n",
      " |\n",
      " |      If DataFrame is empty, returns None.\n",
      " |\n",
      " |  get(self, key, default=None)\n",
      " |      Get item from object for given key (ex: DataFrame column).\n",
      " |\n",
      " |      Returns default value if not found.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : object\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      same type as items contained in object\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame(\n",
      " |      ...     [\n",
      " |      ...         [24.3, 75.7, \"high\"],\n",
      " |      ...         [31, 87.8, \"high\"],\n",
      " |      ...         [22, 71.6, \"medium\"],\n",
      " |      ...         [35, 95, \"medium\"],\n",
      " |      ...     ],\n",
      " |      ...     columns=[\"temp_celsius\", \"temp_fahrenheit\", \"windspeed\"],\n",
      " |      ...     index=pd.date_range(start=\"2014-02-12\", end=\"2014-02-15\", freq=\"D\"),\n",
      " |      ... )\n",
      " |\n",
      " |      >>> df\n",
      " |                  temp_celsius  temp_fahrenheit windspeed\n",
      " |      2014-02-12          24.3             75.7      high\n",
      " |      2014-02-13          31.0             87.8      high\n",
      " |      2014-02-14          22.0             71.6    medium\n",
      " |      2014-02-15          35.0             95.0    medium\n",
      " |\n",
      " |      >>> df.get([\"temp_celsius\", \"windspeed\"])\n",
      " |                  temp_celsius windspeed\n",
      " |      2014-02-12          24.3      high\n",
      " |      2014-02-13          31.0      high\n",
      " |      2014-02-14          22.0    medium\n",
      " |      2014-02-15          35.0    medium\n",
      " |\n",
      " |      >>> ser = df['windspeed']\n",
      " |      >>> ser.get('2014-02-13')\n",
      " |      'high'\n",
      " |\n",
      " |      If the key isn't found, the default value will be used.\n",
      " |\n",
      " |      >>> df.get([\"temp_celsius\", \"temp_kelvin\"], default=\"default_value\")\n",
      " |      'default_value'\n",
      " |\n",
      " |      >>> ser.get('2014-02-10', '[unknown]')\n",
      " |      '[unknown]'\n",
      " |\n",
      " |  head(self, n: 'int' = 5) -> 'Self'\n",
      " |      Return the first `n` rows.\n",
      " |\n",
      " |      This function returns the first `n` rows for the object based\n",
      " |      on position. It is useful for quickly testing if your object\n",
      " |      has the right type of data in it.\n",
      " |\n",
      " |      For negative values of `n`, this function returns all rows except\n",
      " |      the last `|n|` rows, equivalent to ``df[:n]``.\n",
      " |\n",
      " |      If n is larger than the number of rows, this function returns all rows.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int, default 5\n",
      " |          Number of rows to select.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      same type as caller\n",
      " |          The first `n` rows of the caller object.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.tail: Returns the last `n` rows.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion',\n",
      " |      ...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})\n",
      " |      >>> df\n",
      " |            animal\n",
      " |      0  alligator\n",
      " |      1        bee\n",
      " |      2     falcon\n",
      " |      3       lion\n",
      " |      4     monkey\n",
      " |      5     parrot\n",
      " |      6      shark\n",
      " |      7      whale\n",
      " |      8      zebra\n",
      " |\n",
      " |      Viewing the first 5 lines\n",
      " |\n",
      " |      >>> df.head()\n",
      " |            animal\n",
      " |      0  alligator\n",
      " |      1        bee\n",
      " |      2     falcon\n",
      " |      3       lion\n",
      " |      4     monkey\n",
      " |\n",
      " |      Viewing the first `n` lines (three in this case)\n",
      " |\n",
      " |      >>> df.head(3)\n",
      " |            animal\n",
      " |      0  alligator\n",
      " |      1        bee\n",
      " |      2     falcon\n",
      " |\n",
      " |      For negative values of `n`\n",
      " |\n",
      " |      >>> df.head(-3)\n",
      " |            animal\n",
      " |      0  alligator\n",
      " |      1        bee\n",
      " |      2     falcon\n",
      " |      3       lion\n",
      " |      4     monkey\n",
      " |      5     parrot\n",
      " |\n",
      " |  infer_objects(self, copy: 'bool_t | None' = None) -> 'Self'\n",
      " |      Attempt to infer better dtypes for object columns.\n",
      " |\n",
      " |      Attempts soft conversion of object-dtyped\n",
      " |      columns, leaving non-object and unconvertible\n",
      " |      columns unchanged. The inference rules are the\n",
      " |      same as during normal Series/DataFrame construction.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      copy : bool, default True\n",
      " |          Whether to make a copy for non-object or non-inferable columns\n",
      " |          or Series.\n",
      " |\n",
      " |          .. note::\n",
      " |              The `copy` keyword will change behavior in pandas 3.0.\n",
      " |              `Copy-on-Write\n",
      " |              <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n",
      " |              will be enabled by default, which means that all methods with a\n",
      " |              `copy` keyword will use a lazy copy mechanism to defer the copy and\n",
      " |              ignore the `copy` keyword. The `copy` keyword will be removed in a\n",
      " |              future version of pandas.\n",
      " |\n",
      " |              You can already get the future behavior and improvements through\n",
      " |              enabling copy on write ``pd.options.mode.copy_on_write = True``\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      same type as input object\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      to_datetime : Convert argument to datetime.\n",
      " |      to_timedelta : Convert argument to timedelta.\n",
      " |      to_numeric : Convert argument to numeric type.\n",
      " |      convert_dtypes : Convert argument to best possible dtype.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({\"A\": [\"a\", 1, 2, 3]})\n",
      " |      >>> df = df.iloc[1:]\n",
      " |      >>> df\n",
      " |         A\n",
      " |      1  1\n",
      " |      2  2\n",
      " |      3  3\n",
      " |\n",
      " |      >>> df.dtypes\n",
      " |      A    object\n",
      " |      dtype: object\n",
      " |\n",
      " |      >>> df.infer_objects().dtypes\n",
      " |      A    int64\n",
      " |      dtype: object\n",
      " |\n",
      " |  keys(self) -> 'Index'\n",
      " |      Get the 'info axis' (see Indexing for more).\n",
      " |\n",
      " |      This is index for Series, columns for DataFrame.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Index\n",
      " |          Info axis.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> d = pd.DataFrame(data={'A': [1, 2, 3], 'B': [0, 4, 8]},\n",
      " |      ...                  index=['a', 'b', 'c'])\n",
      " |      >>> d\n",
      " |         A  B\n",
      " |      a  1  0\n",
      " |      b  2  4\n",
      " |      c  3  8\n",
      " |      >>> d.keys()\n",
      " |      Index(['A', 'B'], dtype='object')\n",
      " |\n",
      " |  last(self, offset) -> 'Self'\n",
      " |      Select final periods of time series data based on a date offset.\n",
      " |\n",
      " |      .. deprecated:: 2.1\n",
      " |          :meth:`.last` is deprecated and will be removed in a future version.\n",
      " |          Please create a mask and filter using `.loc` instead.\n",
      " |\n",
      " |      For a DataFrame with a sorted DatetimeIndex, this function\n",
      " |      selects the last few rows based on a date offset.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      offset : str, DateOffset, dateutil.relativedelta\n",
      " |          The offset length of the data that will be selected. For instance,\n",
      " |          '3D' will display all the rows having their index within the last 3 days.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          A subset of the caller.\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      TypeError\n",
      " |          If the index is not  a :class:`DatetimeIndex`\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      first : Select initial periods of time series based on a date offset.\n",
      " |      at_time : Select values at a particular time of the day.\n",
      " |      between_time : Select values between particular times of the day.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 2.1.0\n",
      " |          Please create a mask and filter using `.loc` instead\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> i = pd.date_range('2018-04-09', periods=4, freq='2D')\n",
      " |      >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n",
      " |      >>> ts\n",
      " |                  A\n",
      " |      2018-04-09  1\n",
      " |      2018-04-11  2\n",
      " |      2018-04-13  3\n",
      " |      2018-04-15  4\n",
      " |\n",
      " |      Get the rows for the last 3 days:\n",
      " |\n",
      " |      >>> ts.last('3D')  # doctest: +SKIP\n",
      " |                  A\n",
      " |      2018-04-13  3\n",
      " |      2018-04-15  4\n",
      " |\n",
      " |      Notice the data for 3 last calendar days were returned, not the last\n",
      " |      3 observed days in the dataset, and therefore data for 2018-04-11 was\n",
      " |      not returned.\n",
      " |\n",
      " |  last_valid_index(self) -> 'Hashable | None'\n",
      " |      Return index for last non-NA value or None, if no non-NA value is found.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      type of index\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      For Series:\n",
      " |\n",
      " |      >>> s = pd.Series([None, 3, 4])\n",
      " |      >>> s.first_valid_index()\n",
      " |      1\n",
      " |      >>> s.last_valid_index()\n",
      " |      2\n",
      " |\n",
      " |      >>> s = pd.Series([None, None])\n",
      " |      >>> print(s.first_valid_index())\n",
      " |      None\n",
      " |      >>> print(s.last_valid_index())\n",
      " |      None\n",
      " |\n",
      " |      If all elements in Series are NA/null, returns None.\n",
      " |\n",
      " |      >>> s = pd.Series()\n",
      " |      >>> print(s.first_valid_index())\n",
      " |      None\n",
      " |      >>> print(s.last_valid_index())\n",
      " |      None\n",
      " |\n",
      " |      If Series is empty, returns None.\n",
      " |\n",
      " |      For DataFrame:\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'A': [None, None, 2], 'B': [None, 3, 4]})\n",
      " |      >>> df\n",
      " |           A      B\n",
      " |      0  NaN    NaN\n",
      " |      1  NaN    3.0\n",
      " |      2  2.0    4.0\n",
      " |      >>> df.first_valid_index()\n",
      " |      1\n",
      " |      >>> df.last_valid_index()\n",
      " |      2\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'A': [None, None, None], 'B': [None, None, None]})\n",
      " |      >>> df\n",
      " |           A      B\n",
      " |      0  None   None\n",
      " |      1  None   None\n",
      " |      2  None   None\n",
      " |      >>> print(df.first_valid_index())\n",
      " |      None\n",
      " |      >>> print(df.last_valid_index())\n",
      " |      None\n",
      " |\n",
      " |      If all elements in DataFrame are NA/null, returns None.\n",
      " |\n",
      " |      >>> df = pd.DataFrame()\n",
      " |      >>> df\n",
      " |      Empty DataFrame\n",
      " |      Columns: []\n",
      " |      Index: []\n",
      " |      >>> print(df.first_valid_index())\n",
      " |      None\n",
      " |      >>> print(df.last_valid_index())\n",
      " |      None\n",
      " |\n",
      " |      If DataFrame is empty, returns None.\n",
      " |\n",
      " |  mask(self, cond, other=<no_default>, *, inplace: 'bool_t' = False, axis: 'Axis | None' = None, level: 'Level | None' = None) -> 'Self | None'\n",
      " |      Replace values where the condition is True.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cond : bool Series/DataFrame, array-like, or callable\n",
      " |          Where `cond` is False, keep the original value. Where\n",
      " |          True, replace with corresponding value from `other`.\n",
      " |          If `cond` is callable, it is computed on the Series/DataFrame and\n",
      " |          should return boolean Series/DataFrame or array. The callable must\n",
      " |          not change input Series/DataFrame (though pandas doesn't check it).\n",
      " |      other : scalar, Series/DataFrame, or callable\n",
      " |          Entries where `cond` is True are replaced with\n",
      " |          corresponding value from `other`.\n",
      " |          If other is callable, it is computed on the Series/DataFrame and\n",
      " |          should return scalar or Series/DataFrame. The callable must not\n",
      " |          change input Series/DataFrame (though pandas doesn't check it).\n",
      " |          If not specified, entries will be filled with the corresponding\n",
      " |          NULL value (``np.nan`` for numpy dtypes, ``pd.NA`` for extension\n",
      " |          dtypes).\n",
      " |      inplace : bool, default False\n",
      " |          Whether to perform the operation in place on the data.\n",
      " |      axis : int, default None\n",
      " |          Alignment axis if needed. For `Series` this parameter is\n",
      " |          unused and defaults to 0.\n",
      " |      level : int, default None\n",
      " |          Alignment level if needed.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Same type as caller or None if ``inplace=True``.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :func:`DataFrame.where` : Return an object of same shape as\n",
      " |          self.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      The mask method is an application of the if-then idiom. For each\n",
      " |      element in the calling DataFrame, if ``cond`` is ``False`` the\n",
      " |      element is used; otherwise the corresponding element from the DataFrame\n",
      " |      ``other`` is used. If the axis of ``other`` does not align with axis of\n",
      " |      ``cond`` Series/DataFrame, the misaligned index positions will be filled with\n",
      " |      True.\n",
      " |\n",
      " |      The signature for :func:`DataFrame.where` differs from\n",
      " |      :func:`numpy.where`. Roughly ``df1.where(m, df2)`` is equivalent to\n",
      " |      ``np.where(m, df1, df2)``.\n",
      " |\n",
      " |      For further details and examples see the ``mask`` documentation in\n",
      " |      :ref:`indexing <indexing.where_mask>`.\n",
      " |\n",
      " |      The dtype of the object takes precedence. The fill value is casted to\n",
      " |      the object's dtype, if this can be done losslessly.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> s = pd.Series(range(5))\n",
      " |      >>> s.where(s > 0)\n",
      " |      0    NaN\n",
      " |      1    1.0\n",
      " |      2    2.0\n",
      " |      3    3.0\n",
      " |      4    4.0\n",
      " |      dtype: float64\n",
      " |      >>> s.mask(s > 0)\n",
      " |      0    0.0\n",
      " |      1    NaN\n",
      " |      2    NaN\n",
      " |      3    NaN\n",
      " |      4    NaN\n",
      " |      dtype: float64\n",
      " |\n",
      " |      >>> s = pd.Series(range(5))\n",
      " |      >>> t = pd.Series([True, False])\n",
      " |      >>> s.where(t, 99)\n",
      " |      0     0\n",
      " |      1    99\n",
      " |      2    99\n",
      " |      3    99\n",
      " |      4    99\n",
      " |      dtype: int64\n",
      " |      >>> s.mask(t, 99)\n",
      " |      0    99\n",
      " |      1     1\n",
      " |      2    99\n",
      " |      3    99\n",
      " |      4    99\n",
      " |      dtype: int64\n",
      " |\n",
      " |      >>> s.where(s > 1, 10)\n",
      " |      0    10\n",
      " |      1    10\n",
      " |      2    2\n",
      " |      3    3\n",
      " |      4    4\n",
      " |      dtype: int64\n",
      " |      >>> s.mask(s > 1, 10)\n",
      " |      0     0\n",
      " |      1     1\n",
      " |      2    10\n",
      " |      3    10\n",
      " |      4    10\n",
      " |      dtype: int64\n",
      " |\n",
      " |      >>> df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B'])\n",
      " |      >>> df\n",
      " |         A  B\n",
      " |      0  0  1\n",
      " |      1  2  3\n",
      " |      2  4  5\n",
      " |      3  6  7\n",
      " |      4  8  9\n",
      " |      >>> m = df % 3 == 0\n",
      " |      >>> df.where(m, -df)\n",
      " |         A  B\n",
      " |      0  0 -1\n",
      " |      1 -2  3\n",
      " |      2 -4 -5\n",
      " |      3  6 -7\n",
      " |      4 -8  9\n",
      " |      >>> df.where(m, -df) == np.where(m, df, -df)\n",
      " |            A     B\n",
      " |      0  True  True\n",
      " |      1  True  True\n",
      " |      2  True  True\n",
      " |      3  True  True\n",
      " |      4  True  True\n",
      " |      >>> df.where(m, -df) == df.mask(~m, -df)\n",
      " |            A     B\n",
      " |      0  True  True\n",
      " |      1  True  True\n",
      " |      2  True  True\n",
      " |      3  True  True\n",
      " |      4  True  True\n",
      " |\n",
      " |  pad(self, *, axis: 'None | Axis' = None, inplace: 'bool_t' = False, limit: 'None | int' = None, downcast: 'dict | None | lib.NoDefault' = <no_default>) -> 'Self | None'\n",
      " |      Fill NA/NaN values by propagating the last valid observation to next valid.\n",
      " |\n",
      " |      .. deprecated:: 2.0\n",
      " |\n",
      " |          Series/DataFrame.pad is deprecated. Use Series/DataFrame.ffill instead.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series/DataFrame or None\n",
      " |          Object with missing values filled or None if ``inplace=True``.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Please see examples for :meth:`DataFrame.ffill` or :meth:`Series.ffill`.\n",
      " |\n",
      " |  pct_change(self, periods: 'int' = 1, fill_method: 'FillnaOptions | None | lib.NoDefault' = <no_default>, limit: 'int | None | lib.NoDefault' = <no_default>, freq=None, **kwargs) -> 'Self'\n",
      " |      Fractional change between the current and a prior element.\n",
      " |\n",
      " |      Computes the fractional change from the immediately previous row by\n",
      " |      default. This is useful in comparing the fraction of change in a time\n",
      " |      series of elements.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          Despite the name of this method, it calculates fractional change\n",
      " |          (also known as per unit change or relative change) and not\n",
      " |          percentage change. If you need the percentage change, multiply\n",
      " |          these values by 100.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      periods : int, default 1\n",
      " |          Periods to shift for forming percent change.\n",
      " |      fill_method : {'backfill', 'bfill', 'pad', 'ffill', None}, default 'pad'\n",
      " |          How to handle NAs **before** computing percent changes.\n",
      " |\n",
      " |          .. deprecated:: 2.1\n",
      " |              All options of `fill_method` are deprecated except `fill_method=None`.\n",
      " |\n",
      " |      limit : int, default None\n",
      " |          The number of consecutive NAs to fill before stopping.\n",
      " |\n",
      " |          .. deprecated:: 2.1\n",
      " |\n",
      " |      freq : DateOffset, timedelta, or str, optional\n",
      " |          Increment to use from time series API (e.g. 'ME' or BDay()).\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments are passed into\n",
      " |          `DataFrame.shift` or `Series.shift`.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          The same type as the calling object.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.diff : Compute the difference of two elements in a Series.\n",
      " |      DataFrame.diff : Compute the difference of two elements in a DataFrame.\n",
      " |      Series.shift : Shift the index by some number of periods.\n",
      " |      DataFrame.shift : Shift the index by some number of periods.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      **Series**\n",
      " |\n",
      " |      >>> s = pd.Series([90, 91, 85])\n",
      " |      >>> s\n",
      " |      0    90\n",
      " |      1    91\n",
      " |      2    85\n",
      " |      dtype: int64\n",
      " |\n",
      " |      >>> s.pct_change()\n",
      " |      0         NaN\n",
      " |      1    0.011111\n",
      " |      2   -0.065934\n",
      " |      dtype: float64\n",
      " |\n",
      " |      >>> s.pct_change(periods=2)\n",
      " |      0         NaN\n",
      " |      1         NaN\n",
      " |      2   -0.055556\n",
      " |      dtype: float64\n",
      " |\n",
      " |      See the percentage change in a Series where filling NAs with last\n",
      " |      valid observation forward to next valid.\n",
      " |\n",
      " |      >>> s = pd.Series([90, 91, None, 85])\n",
      " |      >>> s\n",
      " |      0    90.0\n",
      " |      1    91.0\n",
      " |      2     NaN\n",
      " |      3    85.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |      >>> s.ffill().pct_change()\n",
      " |      0         NaN\n",
      " |      1    0.011111\n",
      " |      2    0.000000\n",
      " |      3   -0.065934\n",
      " |      dtype: float64\n",
      " |\n",
      " |      **DataFrame**\n",
      " |\n",
      " |      Percentage change in French franc, Deutsche Mark, and Italian lira from\n",
      " |      1980-01-01 to 1980-03-01.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({\n",
      " |      ...     'FR': [4.0405, 4.0963, 4.3149],\n",
      " |      ...     'GR': [1.7246, 1.7482, 1.8519],\n",
      " |      ...     'IT': [804.74, 810.01, 860.13]},\n",
      " |      ...     index=['1980-01-01', '1980-02-01', '1980-03-01'])\n",
      " |      >>> df\n",
      " |                      FR      GR      IT\n",
      " |      1980-01-01  4.0405  1.7246  804.74\n",
      " |      1980-02-01  4.0963  1.7482  810.01\n",
      " |      1980-03-01  4.3149  1.8519  860.13\n",
      " |\n",
      " |      >>> df.pct_change()\n",
      " |                        FR        GR        IT\n",
      " |      1980-01-01       NaN       NaN       NaN\n",
      " |      1980-02-01  0.013810  0.013684  0.006549\n",
      " |      1980-03-01  0.053365  0.059318  0.061876\n",
      " |\n",
      " |      Percentage of change in GOOG and APPL stock volume. Shows computing\n",
      " |      the percentage change between columns.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({\n",
      " |      ...     '2016': [1769950, 30586265],\n",
      " |      ...     '2015': [1500923, 40912316],\n",
      " |      ...     '2014': [1371819, 41403351]},\n",
      " |      ...     index=['GOOG', 'APPL'])\n",
      " |      >>> df\n",
      " |                2016      2015      2014\n",
      " |      GOOG   1769950   1500923   1371819\n",
      " |      APPL  30586265  40912316  41403351\n",
      " |\n",
      " |      >>> df.pct_change(axis='columns', periods=-1)\n",
      " |                2016      2015  2014\n",
      " |      GOOG  0.179241  0.094112   NaN\n",
      " |      APPL -0.252395 -0.011860   NaN\n",
      " |\n",
      " |  pipe(self, func: 'Callable[..., T] | tuple[Callable[..., T], str]', *args, **kwargs) -> 'T'\n",
      " |      Apply chainable functions that expect Series or DataFrames.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          Function to apply to the Series/DataFrame.\n",
      " |          ``args``, and ``kwargs`` are passed into ``func``.\n",
      " |          Alternatively a ``(callable, data_keyword)`` tuple where\n",
      " |          ``data_keyword`` is a string indicating the keyword of\n",
      " |          ``callable`` that expects the Series/DataFrame.\n",
      " |      *args : iterable, optional\n",
      " |          Positional arguments passed into ``func``.\n",
      " |      **kwargs : mapping, optional\n",
      " |          A dictionary of keyword arguments passed into ``func``.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      the return type of ``func``.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.apply : Apply a function along input axis of DataFrame.\n",
      " |      DataFrame.map : Apply a function elementwise on a whole DataFrame.\n",
      " |      Series.map : Apply a mapping correspondence on a\n",
      " |          :class:`~pandas.Series`.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Use ``.pipe`` when chaining together functions that expect\n",
      " |      Series, DataFrames or GroupBy objects.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Constructing a income DataFrame from a dictionary.\n",
      " |\n",
      " |      >>> data = [[8000, 1000], [9500, np.nan], [5000, 2000]]\n",
      " |      >>> df = pd.DataFrame(data, columns=['Salary', 'Others'])\n",
      " |      >>> df\n",
      " |         Salary  Others\n",
      " |      0    8000  1000.0\n",
      " |      1    9500     NaN\n",
      " |      2    5000  2000.0\n",
      " |\n",
      " |      Functions that perform tax reductions on an income DataFrame.\n",
      " |\n",
      " |      >>> def subtract_federal_tax(df):\n",
      " |      ...     return df * 0.9\n",
      " |      >>> def subtract_state_tax(df, rate):\n",
      " |      ...     return df * (1 - rate)\n",
      " |      >>> def subtract_national_insurance(df, rate, rate_increase):\n",
      " |      ...     new_rate = rate + rate_increase\n",
      " |      ...     return df * (1 - new_rate)\n",
      " |\n",
      " |      Instead of writing\n",
      " |\n",
      " |      >>> subtract_national_insurance(\n",
      " |      ...     subtract_state_tax(subtract_federal_tax(df), rate=0.12),\n",
      " |      ...     rate=0.05,\n",
      " |      ...     rate_increase=0.02)  # doctest: +SKIP\n",
      " |\n",
      " |      You can write\n",
      " |\n",
      " |      >>> (\n",
      " |      ...     df.pipe(subtract_federal_tax)\n",
      " |      ...     .pipe(subtract_state_tax, rate=0.12)\n",
      " |      ...     .pipe(subtract_national_insurance, rate=0.05, rate_increase=0.02)\n",
      " |      ... )\n",
      " |          Salary   Others\n",
      " |      0  5892.48   736.56\n",
      " |      1  6997.32      NaN\n",
      " |      2  3682.80  1473.12\n",
      " |\n",
      " |      If you have a function that takes the data as (say) the second\n",
      " |      argument, pass a tuple indicating which keyword expects the\n",
      " |      data. For example, suppose ``national_insurance`` takes its data as ``df``\n",
      " |      in the second argument:\n",
      " |\n",
      " |      >>> def subtract_national_insurance(rate, df, rate_increase):\n",
      " |      ...     new_rate = rate + rate_increase\n",
      " |      ...     return df * (1 - new_rate)\n",
      " |      >>> (\n",
      " |      ...     df.pipe(subtract_federal_tax)\n",
      " |      ...     .pipe(subtract_state_tax, rate=0.12)\n",
      " |      ...     .pipe(\n",
      " |      ...         (subtract_national_insurance, 'df'),\n",
      " |      ...         rate=0.05,\n",
      " |      ...         rate_increase=0.02\n",
      " |      ...     )\n",
      " |      ... )\n",
      " |          Salary   Others\n",
      " |      0  5892.48   736.56\n",
      " |      1  6997.32      NaN\n",
      " |      2  3682.80  1473.12\n",
      " |\n",
      " |  rank(self, axis: 'Axis' = 0, method: \"Literal['average', 'min', 'max', 'first', 'dense']\" = 'average', numeric_only: 'bool_t' = False, na_option: \"Literal['keep', 'top', 'bottom']\" = 'keep', ascending: 'bool_t' = True, pct: 'bool_t' = False) -> 'Self'\n",
      " |      Compute numerical data ranks (1 through n) along axis.\n",
      " |\n",
      " |      By default, equal values are assigned a rank that is the average of the\n",
      " |      ranks of those values.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          Index to direct ranking.\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |      method : {'average', 'min', 'max', 'first', 'dense'}, default 'average'\n",
      " |          How to rank the group of records that have the same value (i.e. ties):\n",
      " |\n",
      " |          * average: average rank of the group\n",
      " |          * min: lowest rank in the group\n",
      " |          * max: highest rank in the group\n",
      " |          * first: ranks assigned in order they appear in the array\n",
      " |          * dense: like 'min', but rank always increases by 1 between groups.\n",
      " |\n",
      " |      numeric_only : bool, default False\n",
      " |          For DataFrame objects, rank only numeric columns if set to True.\n",
      " |\n",
      " |          .. versionchanged:: 2.0.0\n",
      " |              The default value of ``numeric_only`` is now ``False``.\n",
      " |\n",
      " |      na_option : {'keep', 'top', 'bottom'}, default 'keep'\n",
      " |          How to rank NaN values:\n",
      " |\n",
      " |          * keep: assign NaN rank to NaN values\n",
      " |          * top: assign lowest rank to NaN values\n",
      " |          * bottom: assign highest rank to NaN values\n",
      " |\n",
      " |      ascending : bool, default True\n",
      " |          Whether or not the elements should be ranked in ascending order.\n",
      " |      pct : bool, default False\n",
      " |          Whether or not to display the returned rankings in percentile\n",
      " |          form.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      same type as caller\n",
      " |          Return a Series or DataFrame with data ranks as values.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      core.groupby.DataFrameGroupBy.rank : Rank of values within each group.\n",
      " |      core.groupby.SeriesGroupBy.rank : Rank of values within each group.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame(data={'Animal': ['cat', 'penguin', 'dog',\n",
      " |      ...                                    'spider', 'snake'],\n",
      " |      ...                         'Number_legs': [4, 2, 4, 8, np.nan]})\n",
      " |      >>> df\n",
      " |          Animal  Number_legs\n",
      " |      0      cat          4.0\n",
      " |      1  penguin          2.0\n",
      " |      2      dog          4.0\n",
      " |      3   spider          8.0\n",
      " |      4    snake          NaN\n",
      " |\n",
      " |      Ties are assigned the mean of the ranks (by default) for the group.\n",
      " |\n",
      " |      >>> s = pd.Series(range(5), index=list(\"abcde\"))\n",
      " |      >>> s[\"d\"] = s[\"b\"]\n",
      " |      >>> s.rank()\n",
      " |      a    1.0\n",
      " |      b    2.5\n",
      " |      c    4.0\n",
      " |      d    2.5\n",
      " |      e    5.0\n",
      " |      dtype: float64\n",
      " |\n",
      " |      The following example shows how the method behaves with the above\n",
      " |      parameters:\n",
      " |\n",
      " |      * default_rank: this is the default behaviour obtained without using\n",
      " |        any parameter.\n",
      " |      * max_rank: setting ``method = 'max'`` the records that have the\n",
      " |        same values are ranked using the highest rank (e.g.: since 'cat'\n",
      " |        and 'dog' are both in the 2nd and 3rd position, rank 3 is assigned.)\n",
      " |      * NA_bottom: choosing ``na_option = 'bottom'``, if there are records\n",
      " |        with NaN values they are placed at the bottom of the ranking.\n",
      " |      * pct_rank: when setting ``pct = True``, the ranking is expressed as\n",
      " |        percentile rank.\n",
      " |\n",
      " |      >>> df['default_rank'] = df['Number_legs'].rank()\n",
      " |      >>> df['max_rank'] = df['Number_legs'].rank(method='max')\n",
      " |      >>> df['NA_bottom'] = df['Number_legs'].rank(na_option='bottom')\n",
      " |      >>> df['pct_rank'] = df['Number_legs'].rank(pct=True)\n",
      " |      >>> df\n",
      " |          Animal  Number_legs  default_rank  max_rank  NA_bottom  pct_rank\n",
      " |      0      cat          4.0           2.5       3.0        2.5     0.625\n",
      " |      1  penguin          2.0           1.0       1.0        1.0     0.250\n",
      " |      2      dog          4.0           2.5       3.0        2.5     0.625\n",
      " |      3   spider          8.0           4.0       4.0        4.0     1.000\n",
      " |      4    snake          NaN           NaN       NaN        5.0       NaN\n",
      " |\n",
      " |  reindex_like(self, other, method: \"Literal['backfill', 'bfill', 'pad', 'ffill', 'nearest'] | None\" = None, copy: 'bool_t | None' = None, limit: 'int | None' = None, tolerance=None) -> 'Self'\n",
      " |      Return an object with matching indices as other object.\n",
      " |\n",
      " |      Conform the object to the same index on all axes. Optional\n",
      " |      filling logic, placing NaN in locations having no value\n",
      " |      in the previous index. A new object is produced unless the\n",
      " |      new index is equivalent to the current one and copy=False.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Object of the same data type\n",
      " |          Its row and column indices are used to define the new indices\n",
      " |          of this object.\n",
      " |      method : {None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'}\n",
      " |          Method to use for filling holes in reindexed DataFrame.\n",
      " |          Please note: this is only applicable to DataFrames/Series with a\n",
      " |          monotonically increasing/decreasing index.\n",
      " |\n",
      " |          * None (default): don't fill gaps\n",
      " |          * pad / ffill: propagate last valid observation forward to next\n",
      " |            valid\n",
      " |          * backfill / bfill: use next valid observation to fill gap\n",
      " |          * nearest: use nearest valid observations to fill gap.\n",
      " |\n",
      " |      copy : bool, default True\n",
      " |          Return a new object, even if the passed indexes are the same.\n",
      " |\n",
      " |          .. note::\n",
      " |              The `copy` keyword will change behavior in pandas 3.0.\n",
      " |              `Copy-on-Write\n",
      " |              <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n",
      " |              will be enabled by default, which means that all methods with a\n",
      " |              `copy` keyword will use a lazy copy mechanism to defer the copy and\n",
      " |              ignore the `copy` keyword. The `copy` keyword will be removed in a\n",
      " |              future version of pandas.\n",
      " |\n",
      " |              You can already get the future behavior and improvements through\n",
      " |              enabling copy on write ``pd.options.mode.copy_on_write = True``\n",
      " |      limit : int, default None\n",
      " |          Maximum number of consecutive labels to fill for inexact matches.\n",
      " |      tolerance : optional\n",
      " |          Maximum distance between original and new labels for inexact\n",
      " |          matches. The values of the index at the matching locations must\n",
      " |          satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n",
      " |\n",
      " |          Tolerance may be a scalar value, which applies the same tolerance\n",
      " |          to all values, or list-like, which applies variable tolerance per\n",
      " |          element. List-like includes list, tuple, array, Series, and must be\n",
      " |          the same size as the index and its dtype must exactly match the\n",
      " |          index's type.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          Same type as caller, but with changed indices on each axis.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.set_index : Set row labels.\n",
      " |      DataFrame.reset_index : Remove row labels or move them to new columns.\n",
      " |      DataFrame.reindex : Change to new indices or expand indices.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Same as calling\n",
      " |      ``.reindex(index=other.index, columns=other.columns,...)``.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = pd.DataFrame([[24.3, 75.7, 'high'],\n",
      " |      ...                     [31, 87.8, 'high'],\n",
      " |      ...                     [22, 71.6, 'medium'],\n",
      " |      ...                     [35, 95, 'medium']],\n",
      " |      ...                    columns=['temp_celsius', 'temp_fahrenheit',\n",
      " |      ...                             'windspeed'],\n",
      " |      ...                    index=pd.date_range(start='2014-02-12',\n",
      " |      ...                                        end='2014-02-15', freq='D'))\n",
      " |\n",
      " |      >>> df1\n",
      " |                  temp_celsius  temp_fahrenheit windspeed\n",
      " |      2014-02-12          24.3             75.7      high\n",
      " |      2014-02-13          31.0             87.8      high\n",
      " |      2014-02-14          22.0             71.6    medium\n",
      " |      2014-02-15          35.0             95.0    medium\n",
      " |\n",
      " |      >>> df2 = pd.DataFrame([[28, 'low'],\n",
      " |      ...                     [30, 'low'],\n",
      " |      ...                     [35.1, 'medium']],\n",
      " |      ...                    columns=['temp_celsius', 'windspeed'],\n",
      " |      ...                    index=pd.DatetimeIndex(['2014-02-12', '2014-02-13',\n",
      " |      ...                                            '2014-02-15']))\n",
      " |\n",
      " |      >>> df2\n",
      " |                  temp_celsius windspeed\n",
      " |      2014-02-12          28.0       low\n",
      " |      2014-02-13          30.0       low\n",
      " |      2014-02-15          35.1    medium\n",
      " |\n",
      " |      >>> df2.reindex_like(df1)\n",
      " |                  temp_celsius  temp_fahrenheit windspeed\n",
      " |      2014-02-12          28.0              NaN       low\n",
      " |      2014-02-13          30.0              NaN       low\n",
      " |      2014-02-14           NaN              NaN       NaN\n",
      " |      2014-02-15          35.1              NaN    medium\n",
      " |\n",
      " |  rename_axis(self, mapper: 'IndexLabel | lib.NoDefault' = <no_default>, *, index=<no_default>, columns=<no_default>, axis: 'Axis' = 0, copy: 'bool_t | None' = None, inplace: 'bool_t' = False) -> 'Self | None'\n",
      " |      Set the name of the axis for the index or columns.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      mapper : scalar, list-like, optional\n",
      " |          Value to set the axis name attribute.\n",
      " |      index, columns : scalar, list-like, dict-like or function, optional\n",
      " |          A scalar, list-like, dict-like or functions transformations to\n",
      " |          apply to that axis' values.\n",
      " |          Note that the ``columns`` parameter is not allowed if the\n",
      " |          object is a Series. This parameter only apply for DataFrame\n",
      " |          type objects.\n",
      " |\n",
      " |          Use either ``mapper`` and ``axis`` to\n",
      " |          specify the axis to target with ``mapper``, or ``index``\n",
      " |          and/or ``columns``.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The axis to rename. For `Series` this parameter is unused and defaults to 0.\n",
      " |      copy : bool, default None\n",
      " |          Also copy underlying data.\n",
      " |\n",
      " |          .. note::\n",
      " |              The `copy` keyword will change behavior in pandas 3.0.\n",
      " |              `Copy-on-Write\n",
      " |              <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n",
      " |              will be enabled by default, which means that all methods with a\n",
      " |              `copy` keyword will use a lazy copy mechanism to defer the copy and\n",
      " |              ignore the `copy` keyword. The `copy` keyword will be removed in a\n",
      " |              future version of pandas.\n",
      " |\n",
      " |              You can already get the future behavior and improvements through\n",
      " |              enabling copy on write ``pd.options.mode.copy_on_write = True``\n",
      " |      inplace : bool, default False\n",
      " |          Modifies the object directly, instead of creating a new Series\n",
      " |          or DataFrame.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series, DataFrame, or None\n",
      " |          The same type as the caller or None if ``inplace=True``.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.rename : Alter Series index labels or name.\n",
      " |      DataFrame.rename : Alter DataFrame index labels or name.\n",
      " |      Index.rename : Set new names on index.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      ``DataFrame.rename_axis`` supports two calling conventions\n",
      " |\n",
      " |      * ``(index=index_mapper, columns=columns_mapper, ...)``\n",
      " |      * ``(mapper, axis={'index', 'columns'}, ...)``\n",
      " |\n",
      " |      The first calling convention will only modify the names of\n",
      " |      the index and/or the names of the Index object that is the columns.\n",
      " |      In this case, the parameter ``copy`` is ignored.\n",
      " |\n",
      " |      The second calling convention will modify the names of the\n",
      " |      corresponding index if mapper is a list or a scalar.\n",
      " |      However, if mapper is dict-like or a function, it will use the\n",
      " |      deprecated behavior of modifying the axis *labels*.\n",
      " |\n",
      " |      We *highly* recommend using keyword arguments to clarify your\n",
      " |      intent.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      **Series**\n",
      " |\n",
      " |      >>> s = pd.Series([\"dog\", \"cat\", \"monkey\"])\n",
      " |      >>> s\n",
      " |      0       dog\n",
      " |      1       cat\n",
      " |      2    monkey\n",
      " |      dtype: object\n",
      " |      >>> s.rename_axis(\"animal\")\n",
      " |      animal\n",
      " |      0    dog\n",
      " |      1    cat\n",
      " |      2    monkey\n",
      " |      dtype: object\n",
      " |\n",
      " |      **DataFrame**\n",
      " |\n",
      " |      >>> df = pd.DataFrame({\"num_legs\": [4, 4, 2],\n",
      " |      ...                    \"num_arms\": [0, 0, 2]},\n",
      " |      ...                   [\"dog\", \"cat\", \"monkey\"])\n",
      " |      >>> df\n",
      " |              num_legs  num_arms\n",
      " |      dog            4         0\n",
      " |      cat            4         0\n",
      " |      monkey         2         2\n",
      " |      >>> df = df.rename_axis(\"animal\")\n",
      " |      >>> df\n",
      " |              num_legs  num_arms\n",
      " |      animal\n",
      " |      dog            4         0\n",
      " |      cat            4         0\n",
      " |      monkey         2         2\n",
      " |      >>> df = df.rename_axis(\"limbs\", axis=\"columns\")\n",
      " |      >>> df\n",
      " |      limbs   num_legs  num_arms\n",
      " |      animal\n",
      " |      dog            4         0\n",
      " |      cat            4         0\n",
      " |      monkey         2         2\n",
      " |\n",
      " |      **MultiIndex**\n",
      " |\n",
      " |      >>> df.index = pd.MultiIndex.from_product([['mammal'],\n",
      " |      ...                                        ['dog', 'cat', 'monkey']],\n",
      " |      ...                                       names=['type', 'name'])\n",
      " |      >>> df\n",
      " |      limbs          num_legs  num_arms\n",
      " |      type   name\n",
      " |      mammal dog            4         0\n",
      " |             cat            4         0\n",
      " |             monkey         2         2\n",
      " |\n",
      " |      >>> df.rename_axis(index={'type': 'class'})\n",
      " |      limbs          num_legs  num_arms\n",
      " |      class  name\n",
      " |      mammal dog            4         0\n",
      " |             cat            4         0\n",
      " |             monkey         2         2\n",
      " |\n",
      " |      >>> df.rename_axis(columns=str.upper)\n",
      " |      LIMBS          num_legs  num_arms\n",
      " |      type   name\n",
      " |      mammal dog            4         0\n",
      " |             cat            4         0\n",
      " |             monkey         2         2\n",
      " |\n",
      " |  replace(self, to_replace=None, value=<no_default>, *, inplace: 'bool_t' = False, limit: 'int | None' = None, regex: 'bool_t' = False, method: \"Literal['pad', 'ffill', 'bfill'] | lib.NoDefault\" = <no_default>) -> 'Self | None'\n",
      " |      Replace values given in `to_replace` with `value`.\n",
      " |\n",
      " |      Values of the Series/DataFrame are replaced with other values dynamically.\n",
      " |      This differs from updating with ``.loc`` or ``.iloc``, which require\n",
      " |      you to specify a location to update with some value.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      to_replace : str, regex, list, dict, Series, int, float, or None\n",
      " |          How to find the values that will be replaced.\n",
      " |\n",
      " |          * numeric, str or regex:\n",
      " |\n",
      " |              - numeric: numeric values equal to `to_replace` will be\n",
      " |                replaced with `value`\n",
      " |              - str: string exactly matching `to_replace` will be replaced\n",
      " |                with `value`\n",
      " |              - regex: regexs matching `to_replace` will be replaced with\n",
      " |                `value`\n",
      " |\n",
      " |          * list of str, regex, or numeric:\n",
      " |\n",
      " |              - First, if `to_replace` and `value` are both lists, they\n",
      " |                **must** be the same length.\n",
      " |              - Second, if ``regex=True`` then all of the strings in **both**\n",
      " |                lists will be interpreted as regexs otherwise they will match\n",
      " |                directly. This doesn't matter much for `value` since there\n",
      " |                are only a few possible substitution regexes you can use.\n",
      " |              - str, regex and numeric rules apply as above.\n",
      " |\n",
      " |          * dict:\n",
      " |\n",
      " |              - Dicts can be used to specify different replacement values\n",
      " |                for different existing values. For example,\n",
      " |                ``{'a': 'b', 'y': 'z'}`` replaces the value 'a' with 'b' and\n",
      " |                'y' with 'z'. To use a dict in this way, the optional `value`\n",
      " |                parameter should not be given.\n",
      " |              - For a DataFrame a dict can specify that different values\n",
      " |                should be replaced in different columns. For example,\n",
      " |                ``{'a': 1, 'b': 'z'}`` looks for the value 1 in column 'a'\n",
      " |                and the value 'z' in column 'b' and replaces these values\n",
      " |                with whatever is specified in `value`. The `value` parameter\n",
      " |                should not be ``None`` in this case. You can treat this as a\n",
      " |                special case of passing two lists except that you are\n",
      " |                specifying the column to search in.\n",
      " |              - For a DataFrame nested dictionaries, e.g.,\n",
      " |                ``{'a': {'b': np.nan}}``, are read as follows: look in column\n",
      " |                'a' for the value 'b' and replace it with NaN. The optional `value`\n",
      " |                parameter should not be specified to use a nested dict in this\n",
      " |                way. You can nest regular expressions as well. Note that\n",
      " |                column names (the top-level dictionary keys in a nested\n",
      " |                dictionary) **cannot** be regular expressions.\n",
      " |\n",
      " |          * None:\n",
      " |\n",
      " |              - This means that the `regex` argument must be a string,\n",
      " |                compiled regular expression, or list, dict, ndarray or\n",
      " |                Series of such elements. If `value` is also ``None`` then\n",
      " |                this **must** be a nested dictionary or Series.\n",
      " |\n",
      " |          See the examples section for examples of each of these.\n",
      " |      value : scalar, dict, list, str, regex, default None\n",
      " |          Value to replace any values matching `to_replace` with.\n",
      " |          For a DataFrame a dict of values can be used to specify which\n",
      " |          value to use for each column (columns not in the dict will not be\n",
      " |          filled). Regular expressions, strings and lists or dicts of such\n",
      " |          objects are also allowed.\n",
      " |\n",
      " |      inplace : bool, default False\n",
      " |          If True, performs operation inplace and returns None.\n",
      " |      limit : int, default None\n",
      " |          Maximum size gap to forward or backward fill.\n",
      " |\n",
      " |          .. deprecated:: 2.1.0\n",
      " |      regex : bool or same types as `to_replace`, default False\n",
      " |          Whether to interpret `to_replace` and/or `value` as regular\n",
      " |          expressions. Alternatively, this could be a regular expression or a\n",
      " |          list, dict, or array of regular expressions in which case\n",
      " |          `to_replace` must be ``None``.\n",
      " |      method : {'pad', 'ffill', 'bfill'}\n",
      " |          The method to use when for replacement, when `to_replace` is a\n",
      " |          scalar, list or tuple and `value` is ``None``.\n",
      " |\n",
      " |          .. deprecated:: 2.1.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series/DataFrame\n",
      " |          Object after replacement.\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      AssertionError\n",
      " |          * If `regex` is not a ``bool`` and `to_replace` is not\n",
      " |            ``None``.\n",
      " |\n",
      " |      TypeError\n",
      " |          * If `to_replace` is not a scalar, array-like, ``dict``, or ``None``\n",
      " |          * If `to_replace` is a ``dict`` and `value` is not a ``list``,\n",
      " |            ``dict``, ``ndarray``, or ``Series``\n",
      " |          * If `to_replace` is ``None`` and `regex` is not compilable\n",
      " |            into a regular expression or is a list, dict, ndarray, or\n",
      " |            Series.\n",
      " |          * When replacing multiple ``bool`` or ``datetime64`` objects and\n",
      " |            the arguments to `to_replace` does not match the type of the\n",
      " |            value being replaced\n",
      " |\n",
      " |      ValueError\n",
      " |          * If a ``list`` or an ``ndarray`` is passed to `to_replace` and\n",
      " |            `value` but they are not the same length.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.fillna : Fill NA values.\n",
      " |      DataFrame.fillna : Fill NA values.\n",
      " |      Series.where : Replace values based on boolean condition.\n",
      " |      DataFrame.where : Replace values based on boolean condition.\n",
      " |      DataFrame.map: Apply a function to a Dataframe elementwise.\n",
      " |      Series.map: Map values of Series according to an input mapping or function.\n",
      " |      Series.str.replace : Simple string replacement.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      * Regex substitution is performed under the hood with ``re.sub``. The\n",
      " |        rules for substitution for ``re.sub`` are the same.\n",
      " |      * Regular expressions will only substitute on strings, meaning you\n",
      " |        cannot provide, for example, a regular expression matching floating\n",
      " |        point numbers and expect the columns in your frame that have a\n",
      " |        numeric dtype to be matched. However, if those floating point\n",
      " |        numbers *are* strings, then you can do this.\n",
      " |      * This method has *a lot* of options. You are encouraged to experiment\n",
      " |        and play with this method to gain intuition about how it works.\n",
      " |      * When dict is used as the `to_replace` value, it is like\n",
      " |        key(s) in the dict are the to_replace part and\n",
      " |        value(s) in the dict are the value parameter.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      **Scalar `to_replace` and `value`**\n",
      " |\n",
      " |      >>> s = pd.Series([1, 2, 3, 4, 5])\n",
      " |      >>> s.replace(1, 5)\n",
      " |      0    5\n",
      " |      1    2\n",
      " |      2    3\n",
      " |      3    4\n",
      " |      4    5\n",
      " |      dtype: int64\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'A': [0, 1, 2, 3, 4],\n",
      " |      ...                    'B': [5, 6, 7, 8, 9],\n",
      " |      ...                    'C': ['a', 'b', 'c', 'd', 'e']})\n",
      " |      >>> df.replace(0, 5)\n",
      " |          A  B  C\n",
      " |      0  5  5  a\n",
      " |      1  1  6  b\n",
      " |      2  2  7  c\n",
      " |      3  3  8  d\n",
      " |      4  4  9  e\n",
      " |\n",
      " |      **List-like `to_replace`**\n",
      " |\n",
      " |      >>> df.replace([0, 1, 2, 3], 4)\n",
      " |          A  B  C\n",
      " |      0  4  5  a\n",
      " |      1  4  6  b\n",
      " |      2  4  7  c\n",
      " |      3  4  8  d\n",
      " |      4  4  9  e\n",
      " |\n",
      " |      >>> df.replace([0, 1, 2, 3], [4, 3, 2, 1])\n",
      " |          A  B  C\n",
      " |      0  4  5  a\n",
      " |      1  3  6  b\n",
      " |      2  2  7  c\n",
      " |      3  1  8  d\n",
      " |      4  4  9  e\n",
      " |\n",
      " |      >>> s.replace([1, 2], method='bfill')\n",
      " |      0    3\n",
      " |      1    3\n",
      " |      2    3\n",
      " |      3    4\n",
      " |      4    5\n",
      " |      dtype: int64\n",
      " |\n",
      " |      **dict-like `to_replace`**\n",
      " |\n",
      " |      >>> df.replace({0: 10, 1: 100})\n",
      " |              A  B  C\n",
      " |      0   10  5  a\n",
      " |      1  100  6  b\n",
      " |      2    2  7  c\n",
      " |      3    3  8  d\n",
      " |      4    4  9  e\n",
      " |\n",
      " |      >>> df.replace({'A': 0, 'B': 5}, 100)\n",
      " |              A    B  C\n",
      " |      0  100  100  a\n",
      " |      1    1    6  b\n",
      " |      2    2    7  c\n",
      " |      3    3    8  d\n",
      " |      4    4    9  e\n",
      " |\n",
      " |      >>> df.replace({'A': {0: 100, 4: 400}})\n",
      " |              A  B  C\n",
      " |      0  100  5  a\n",
      " |      1    1  6  b\n",
      " |      2    2  7  c\n",
      " |      3    3  8  d\n",
      " |      4  400  9  e\n",
      " |\n",
      " |      **Regular expression `to_replace`**\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'A': ['bat', 'foo', 'bait'],\n",
      " |      ...                    'B': ['abc', 'bar', 'xyz']})\n",
      " |      >>> df.replace(to_replace=r'^ba.$', value='new', regex=True)\n",
      " |              A    B\n",
      " |      0   new  abc\n",
      " |      1   foo  new\n",
      " |      2  bait  xyz\n",
      " |\n",
      " |      >>> df.replace({'A': r'^ba.$'}, {'A': 'new'}, regex=True)\n",
      " |              A    B\n",
      " |      0   new  abc\n",
      " |      1   foo  bar\n",
      " |      2  bait  xyz\n",
      " |\n",
      " |      >>> df.replace(regex=r'^ba.$', value='new')\n",
      " |              A    B\n",
      " |      0   new  abc\n",
      " |      1   foo  new\n",
      " |      2  bait  xyz\n",
      " |\n",
      " |      >>> df.replace(regex={r'^ba.$': 'new', 'foo': 'xyz'})\n",
      " |              A    B\n",
      " |      0   new  abc\n",
      " |      1   xyz  new\n",
      " |      2  bait  xyz\n",
      " |\n",
      " |      >>> df.replace(regex=[r'^ba.$', 'foo'], value='new')\n",
      " |              A    B\n",
      " |      0   new  abc\n",
      " |      1   new  new\n",
      " |      2  bait  xyz\n",
      " |\n",
      " |      Compare the behavior of ``s.replace({'a': None})`` and\n",
      " |      ``s.replace('a', None)`` to understand the peculiarities\n",
      " |      of the `to_replace` parameter:\n",
      " |\n",
      " |      >>> s = pd.Series([10, 'a', 'a', 'b', 'a'])\n",
      " |\n",
      " |      When one uses a dict as the `to_replace` value, it is like the\n",
      " |      value(s) in the dict are equal to the `value` parameter.\n",
      " |      ``s.replace({'a': None})`` is equivalent to\n",
      " |      ``s.replace(to_replace={'a': None}, value=None, method=None)``:\n",
      " |\n",
      " |      >>> s.replace({'a': None})\n",
      " |      0      10\n",
      " |      1    None\n",
      " |      2    None\n",
      " |      3       b\n",
      " |      4    None\n",
      " |      dtype: object\n",
      " |\n",
      " |      When ``value`` is not explicitly passed and `to_replace` is a scalar, list\n",
      " |      or tuple, `replace` uses the method parameter (default 'pad') to do the\n",
      " |      replacement. So this is why the 'a' values are being replaced by 10\n",
      " |      in rows 1 and 2 and 'b' in row 4 in this case.\n",
      " |\n",
      " |      >>> s.replace('a')\n",
      " |      0    10\n",
      " |      1    10\n",
      " |      2    10\n",
      " |      3     b\n",
      " |      4     b\n",
      " |      dtype: object\n",
      " |\n",
      " |          .. deprecated:: 2.1.0\n",
      " |              The 'method' parameter and padding behavior are deprecated.\n",
      " |\n",
      " |      On the other hand, if ``None`` is explicitly passed for ``value``, it will\n",
      " |      be respected:\n",
      " |\n",
      " |      >>> s.replace('a', None)\n",
      " |      0      10\n",
      " |      1    None\n",
      " |      2    None\n",
      " |      3       b\n",
      " |      4    None\n",
      " |      dtype: object\n",
      " |\n",
      " |          .. versionchanged:: 1.4.0\n",
      " |              Previously the explicit ``None`` was silently ignored.\n",
      " |\n",
      " |      When ``regex=True``, ``value`` is not ``None`` and `to_replace` is a string,\n",
      " |      the replacement will be applied in all columns of the DataFrame.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'A': [0, 1, 2, 3, 4],\n",
      " |      ...                    'B': ['a', 'b', 'c', 'd', 'e'],\n",
      " |      ...                    'C': ['f', 'g', 'h', 'i', 'j']})\n",
      " |\n",
      " |      >>> df.replace(to_replace='^[a-g]', value='e', regex=True)\n",
      " |          A  B  C\n",
      " |      0  0  e  e\n",
      " |      1  1  e  e\n",
      " |      2  2  e  h\n",
      " |      3  3  e  i\n",
      " |      4  4  e  j\n",
      " |\n",
      " |      If ``value`` is not ``None`` and `to_replace` is a dictionary, the dictionary\n",
      " |      keys will be the DataFrame columns that the replacement will be applied.\n",
      " |\n",
      " |      >>> df.replace(to_replace={'B': '^[a-c]', 'C': '^[h-j]'}, value='e', regex=True)\n",
      " |          A  B  C\n",
      " |      0  0  e  f\n",
      " |      1  1  e  g\n",
      " |      2  2  e  e\n",
      " |      3  3  d  e\n",
      " |      4  4  e  e\n",
      " |\n",
      " |  resample(self, rule, axis: 'Axis | lib.NoDefault' = <no_default>, closed: \"Literal['right', 'left'] | None\" = None, label: \"Literal['right', 'left'] | None\" = None, convention: \"Literal['start', 'end', 's', 'e'] | lib.NoDefault\" = <no_default>, kind: \"Literal['timestamp', 'period'] | None | lib.NoDefault\" = <no_default>, on: 'Level | None' = None, level: 'Level | None' = None, origin: 'str | TimestampConvertibleTypes' = 'start_day', offset: 'TimedeltaConvertibleTypes | None' = None, group_keys: 'bool_t' = False) -> 'Resampler'\n",
      " |      Resample time-series data.\n",
      " |\n",
      " |      Convenience method for frequency conversion and resampling of time series.\n",
      " |      The object must have a datetime-like index (`DatetimeIndex`, `PeriodIndex`,\n",
      " |      or `TimedeltaIndex`), or the caller must pass the label of a datetime-like\n",
      " |      series/index to the ``on``/``level`` keyword parameter.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      rule : DateOffset, Timedelta or str\n",
      " |          The offset string or object representing target conversion.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          Which axis to use for up- or down-sampling. For `Series` this parameter\n",
      " |          is unused and defaults to 0. Must be\n",
      " |          `DatetimeIndex`, `TimedeltaIndex` or `PeriodIndex`.\n",
      " |\n",
      " |          .. deprecated:: 2.0.0\n",
      " |              Use frame.T.resample(...) instead.\n",
      " |      closed : {'right', 'left'}, default None\n",
      " |          Which side of bin interval is closed. The default is 'left'\n",
      " |          for all frequency offsets except for 'ME', 'YE', 'QE', 'BME',\n",
      " |          'BA', 'BQE', and 'W' which all have a default of 'right'.\n",
      " |      label : {'right', 'left'}, default None\n",
      " |          Which bin edge label to label bucket with. The default is 'left'\n",
      " |          for all frequency offsets except for 'ME', 'YE', 'QE', 'BME',\n",
      " |          'BA', 'BQE', and 'W' which all have a default of 'right'.\n",
      " |      convention : {'start', 'end', 's', 'e'}, default 'start'\n",
      " |          For `PeriodIndex` only, controls whether to use the start or\n",
      " |          end of `rule`.\n",
      " |\n",
      " |          .. deprecated:: 2.2.0\n",
      " |              Convert PeriodIndex to DatetimeIndex before resampling instead.\n",
      " |      kind : {'timestamp', 'period'}, optional, default None\n",
      " |          Pass 'timestamp' to convert the resulting index to a\n",
      " |          `DateTimeIndex` or 'period' to convert it to a `PeriodIndex`.\n",
      " |          By default the input representation is retained.\n",
      " |\n",
      " |          .. deprecated:: 2.2.0\n",
      " |              Convert index to desired type explicitly instead.\n",
      " |\n",
      " |      on : str, optional\n",
      " |          For a DataFrame, column to use instead of index for resampling.\n",
      " |          Column must be datetime-like.\n",
      " |      level : str or int, optional\n",
      " |          For a MultiIndex, level (name or number) to use for\n",
      " |          resampling. `level` must be datetime-like.\n",
      " |      origin : Timestamp or str, default 'start_day'\n",
      " |          The timestamp on which to adjust the grouping. The timezone of origin\n",
      " |          must match the timezone of the index.\n",
      " |          If string, must be one of the following:\n",
      " |\n",
      " |          - 'epoch': `origin` is 1970-01-01\n",
      " |          - 'start': `origin` is the first value of the timeseries\n",
      " |          - 'start_day': `origin` is the first day at midnight of the timeseries\n",
      " |\n",
      " |          - 'end': `origin` is the last value of the timeseries\n",
      " |          - 'end_day': `origin` is the ceiling midnight of the last day\n",
      " |\n",
      " |          .. versionadded:: 1.3.0\n",
      " |\n",
      " |          .. note::\n",
      " |\n",
      " |              Only takes effect for Tick-frequencies (i.e. fixed frequencies like\n",
      " |              days, hours, and minutes, rather than months or quarters).\n",
      " |      offset : Timedelta or str, default is None\n",
      " |          An offset timedelta added to the origin.\n",
      " |\n",
      " |      group_keys : bool, default False\n",
      " |          Whether to include the group keys in the result index when using\n",
      " |          ``.apply()`` on the resampled object.\n",
      " |\n",
      " |          .. versionadded:: 1.5.0\n",
      " |\n",
      " |              Not specifying ``group_keys`` will retain values-dependent behavior\n",
      " |              from pandas 1.4 and earlier (see :ref:`pandas 1.5.0 Release notes\n",
      " |              <whatsnew_150.enhancements.resample_group_keys>` for examples).\n",
      " |\n",
      " |          .. versionchanged:: 2.0.0\n",
      " |\n",
      " |              ``group_keys`` now defaults to ``False``.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      pandas.api.typing.Resampler\n",
      " |          :class:`~pandas.core.Resampler` object.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.resample : Resample a Series.\n",
      " |      DataFrame.resample : Resample a DataFrame.\n",
      " |      groupby : Group Series/DataFrame by mapping, function, label, or list of labels.\n",
      " |      asfreq : Reindex a Series/DataFrame with the given frequency without grouping.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      See the `user guide\n",
      " |      <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#resampling>`__\n",
      " |      for more.\n",
      " |\n",
      " |      To learn more about the offset strings, please see `this link\n",
      " |      <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects>`__.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Start by creating a series with 9 one minute timestamps.\n",
      " |\n",
      " |      >>> index = pd.date_range('1/1/2000', periods=9, freq='min')\n",
      " |      >>> series = pd.Series(range(9), index=index)\n",
      " |      >>> series\n",
      " |      2000-01-01 00:00:00    0\n",
      " |      2000-01-01 00:01:00    1\n",
      " |      2000-01-01 00:02:00    2\n",
      " |      2000-01-01 00:03:00    3\n",
      " |      2000-01-01 00:04:00    4\n",
      " |      2000-01-01 00:05:00    5\n",
      " |      2000-01-01 00:06:00    6\n",
      " |      2000-01-01 00:07:00    7\n",
      " |      2000-01-01 00:08:00    8\n",
      " |      Freq: min, dtype: int64\n",
      " |\n",
      " |      Downsample the series into 3 minute bins and sum the values\n",
      " |      of the timestamps falling into a bin.\n",
      " |\n",
      " |      >>> series.resample('3min').sum()\n",
      " |      2000-01-01 00:00:00     3\n",
      " |      2000-01-01 00:03:00    12\n",
      " |      2000-01-01 00:06:00    21\n",
      " |      Freq: 3min, dtype: int64\n",
      " |\n",
      " |      Downsample the series into 3 minute bins as above, but label each\n",
      " |      bin using the right edge instead of the left. Please note that the\n",
      " |      value in the bucket used as the label is not included in the bucket,\n",
      " |      which it labels. For example, in the original series the\n",
      " |      bucket ``2000-01-01 00:03:00`` contains the value 3, but the summed\n",
      " |      value in the resampled bucket with the label ``2000-01-01 00:03:00``\n",
      " |      does not include 3 (if it did, the summed value would be 6, not 3).\n",
      " |\n",
      " |      >>> series.resample('3min', label='right').sum()\n",
      " |      2000-01-01 00:03:00     3\n",
      " |      2000-01-01 00:06:00    12\n",
      " |      2000-01-01 00:09:00    21\n",
      " |      Freq: 3min, dtype: int64\n",
      " |\n",
      " |      To include this value close the right side of the bin interval,\n",
      " |      as shown below.\n",
      " |\n",
      " |      >>> series.resample('3min', label='right', closed='right').sum()\n",
      " |      2000-01-01 00:00:00     0\n",
      " |      2000-01-01 00:03:00     6\n",
      " |      2000-01-01 00:06:00    15\n",
      " |      2000-01-01 00:09:00    15\n",
      " |      Freq: 3min, dtype: int64\n",
      " |\n",
      " |      Upsample the series into 30 second bins.\n",
      " |\n",
      " |      >>> series.resample('30s').asfreq()[0:5]   # Select first 5 rows\n",
      " |      2000-01-01 00:00:00   0.0\n",
      " |      2000-01-01 00:00:30   NaN\n",
      " |      2000-01-01 00:01:00   1.0\n",
      " |      2000-01-01 00:01:30   NaN\n",
      " |      2000-01-01 00:02:00   2.0\n",
      " |      Freq: 30s, dtype: float64\n",
      " |\n",
      " |      Upsample the series into 30 second bins and fill the ``NaN``\n",
      " |      values using the ``ffill`` method.\n",
      " |\n",
      " |      >>> series.resample('30s').ffill()[0:5]\n",
      " |      2000-01-01 00:00:00    0\n",
      " |      2000-01-01 00:00:30    0\n",
      " |      2000-01-01 00:01:00    1\n",
      " |      2000-01-01 00:01:30    1\n",
      " |      2000-01-01 00:02:00    2\n",
      " |      Freq: 30s, dtype: int64\n",
      " |\n",
      " |      Upsample the series into 30 second bins and fill the\n",
      " |      ``NaN`` values using the ``bfill`` method.\n",
      " |\n",
      " |      >>> series.resample('30s').bfill()[0:5]\n",
      " |      2000-01-01 00:00:00    0\n",
      " |      2000-01-01 00:00:30    1\n",
      " |      2000-01-01 00:01:00    1\n",
      " |      2000-01-01 00:01:30    2\n",
      " |      2000-01-01 00:02:00    2\n",
      " |      Freq: 30s, dtype: int64\n",
      " |\n",
      " |      Pass a custom function via ``apply``\n",
      " |\n",
      " |      >>> def custom_resampler(arraylike):\n",
      " |      ...     return np.sum(arraylike) + 5\n",
      " |      ...\n",
      " |      >>> series.resample('3min').apply(custom_resampler)\n",
      " |      2000-01-01 00:00:00     8\n",
      " |      2000-01-01 00:03:00    17\n",
      " |      2000-01-01 00:06:00    26\n",
      " |      Freq: 3min, dtype: int64\n",
      " |\n",
      " |      For DataFrame objects, the keyword `on` can be used to specify the\n",
      " |      column instead of the index for resampling.\n",
      " |\n",
      " |      >>> d = {'price': [10, 11, 9, 13, 14, 18, 17, 19],\n",
      " |      ...      'volume': [50, 60, 40, 100, 50, 100, 40, 50]}\n",
      " |      >>> df = pd.DataFrame(d)\n",
      " |      >>> df['week_starting'] = pd.date_range('01/01/2018',\n",
      " |      ...                                     periods=8,\n",
      " |      ...                                     freq='W')\n",
      " |      >>> df\n",
      " |         price  volume week_starting\n",
      " |      0     10      50    2018-01-07\n",
      " |      1     11      60    2018-01-14\n",
      " |      2      9      40    2018-01-21\n",
      " |      3     13     100    2018-01-28\n",
      " |      4     14      50    2018-02-04\n",
      " |      5     18     100    2018-02-11\n",
      " |      6     17      40    2018-02-18\n",
      " |      7     19      50    2018-02-25\n",
      " |      >>> df.resample('ME', on='week_starting').mean()\n",
      " |                     price  volume\n",
      " |      week_starting\n",
      " |      2018-01-31     10.75    62.5\n",
      " |      2018-02-28     17.00    60.0\n",
      " |\n",
      " |      For a DataFrame with MultiIndex, the keyword `level` can be used to\n",
      " |      specify on which level the resampling needs to take place.\n",
      " |\n",
      " |      >>> days = pd.date_range('1/1/2000', periods=4, freq='D')\n",
      " |      >>> d2 = {'price': [10, 11, 9, 13, 14, 18, 17, 19],\n",
      " |      ...       'volume': [50, 60, 40, 100, 50, 100, 40, 50]}\n",
      " |      >>> df2 = pd.DataFrame(\n",
      " |      ...     d2,\n",
      " |      ...     index=pd.MultiIndex.from_product(\n",
      " |      ...         [days, ['morning', 'afternoon']]\n",
      " |      ...     )\n",
      " |      ... )\n",
      " |      >>> df2\n",
      " |                            price  volume\n",
      " |      2000-01-01 morning       10      50\n",
      " |                 afternoon     11      60\n",
      " |      2000-01-02 morning        9      40\n",
      " |                 afternoon     13     100\n",
      " |      2000-01-03 morning       14      50\n",
      " |                 afternoon     18     100\n",
      " |      2000-01-04 morning       17      40\n",
      " |                 afternoon     19      50\n",
      " |      >>> df2.resample('D', level=0).sum()\n",
      " |                  price  volume\n",
      " |      2000-01-01     21     110\n",
      " |      2000-01-02     22     140\n",
      " |      2000-01-03     32     150\n",
      " |      2000-01-04     36      90\n",
      " |\n",
      " |      If you want to adjust the start of the bins based on a fixed timestamp:\n",
      " |\n",
      " |      >>> start, end = '2000-10-01 23:30:00', '2000-10-02 00:30:00'\n",
      " |      >>> rng = pd.date_range(start, end, freq='7min')\n",
      " |      >>> ts = pd.Series(np.arange(len(rng)) * 3, index=rng)\n",
      " |      >>> ts\n",
      " |      2000-10-01 23:30:00     0\n",
      " |      2000-10-01 23:37:00     3\n",
      " |      2000-10-01 23:44:00     6\n",
      " |      2000-10-01 23:51:00     9\n",
      " |      2000-10-01 23:58:00    12\n",
      " |      2000-10-02 00:05:00    15\n",
      " |      2000-10-02 00:12:00    18\n",
      " |      2000-10-02 00:19:00    21\n",
      " |      2000-10-02 00:26:00    24\n",
      " |      Freq: 7min, dtype: int64\n",
      " |\n",
      " |      >>> ts.resample('17min').sum()\n",
      " |      2000-10-01 23:14:00     0\n",
      " |      2000-10-01 23:31:00     9\n",
      " |      2000-10-01 23:48:00    21\n",
      " |      2000-10-02 00:05:00    54\n",
      " |      2000-10-02 00:22:00    24\n",
      " |      Freq: 17min, dtype: int64\n",
      " |\n",
      " |      >>> ts.resample('17min', origin='epoch').sum()\n",
      " |      2000-10-01 23:18:00     0\n",
      " |      2000-10-01 23:35:00    18\n",
      " |      2000-10-01 23:52:00    27\n",
      " |      2000-10-02 00:09:00    39\n",
      " |      2000-10-02 00:26:00    24\n",
      " |      Freq: 17min, dtype: int64\n",
      " |\n",
      " |      >>> ts.resample('17min', origin='2000-01-01').sum()\n",
      " |      2000-10-01 23:24:00     3\n",
      " |      2000-10-01 23:41:00    15\n",
      " |      2000-10-01 23:58:00    45\n",
      " |      2000-10-02 00:15:00    45\n",
      " |      Freq: 17min, dtype: int64\n",
      " |\n",
      " |      If you want to adjust the start of the bins with an `offset` Timedelta, the two\n",
      " |      following lines are equivalent:\n",
      " |\n",
      " |      >>> ts.resample('17min', origin='start').sum()\n",
      " |      2000-10-01 23:30:00     9\n",
      " |      2000-10-01 23:47:00    21\n",
      " |      2000-10-02 00:04:00    54\n",
      " |      2000-10-02 00:21:00    24\n",
      " |      Freq: 17min, dtype: int64\n",
      " |\n",
      " |      >>> ts.resample('17min', offset='23h30min').sum()\n",
      " |      2000-10-01 23:30:00     9\n",
      " |      2000-10-01 23:47:00    21\n",
      " |      2000-10-02 00:04:00    54\n",
      " |      2000-10-02 00:21:00    24\n",
      " |      Freq: 17min, dtype: int64\n",
      " |\n",
      " |      If you want to take the largest Timestamp as the end of the bins:\n",
      " |\n",
      " |      >>> ts.resample('17min', origin='end').sum()\n",
      " |      2000-10-01 23:35:00     0\n",
      " |      2000-10-01 23:52:00    18\n",
      " |      2000-10-02 00:09:00    27\n",
      " |      2000-10-02 00:26:00    63\n",
      " |      Freq: 17min, dtype: int64\n",
      " |\n",
      " |      In contrast with the `start_day`, you can use `end_day` to take the ceiling\n",
      " |      midnight of the largest Timestamp as the end of the bins and drop the bins\n",
      " |      not containing data:\n",
      " |\n",
      " |      >>> ts.resample('17min', origin='end_day').sum()\n",
      " |      2000-10-01 23:38:00     3\n",
      " |      2000-10-01 23:55:00    15\n",
      " |      2000-10-02 00:12:00    45\n",
      " |      2000-10-02 00:29:00    45\n",
      " |      Freq: 17min, dtype: int64\n",
      " |\n",
      " |  rolling(self, window: 'int | dt.timedelta | str | BaseOffset | BaseIndexer', min_periods: 'int | None' = None, center: 'bool_t' = False, win_type: 'str | None' = None, on: 'str | None' = None, axis: 'Axis | lib.NoDefault' = <no_default>, closed: 'IntervalClosedType | None' = None, step: 'int | None' = None, method: 'str' = 'single') -> 'Window | Rolling'\n",
      " |      Provide rolling window calculations.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      window : int, timedelta, str, offset, or BaseIndexer subclass\n",
      " |          Size of the moving window.\n",
      " |\n",
      " |          If an integer, the fixed number of observations used for\n",
      " |          each window.\n",
      " |\n",
      " |          If a timedelta, str, or offset, the time period of each window. Each\n",
      " |          window will be a variable sized based on the observations included in\n",
      " |          the time-period. This is only valid for datetimelike indexes.\n",
      " |          To learn more about the offsets & frequency strings, please see `this link\n",
      " |          <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n",
      " |\n",
      " |          If a BaseIndexer subclass, the window boundaries\n",
      " |          based on the defined ``get_window_bounds`` method. Additional rolling\n",
      " |          keyword arguments, namely ``min_periods``, ``center``, ``closed`` and\n",
      " |          ``step`` will be passed to ``get_window_bounds``.\n",
      " |\n",
      " |      min_periods : int, default None\n",
      " |          Minimum number of observations in window required to have a value;\n",
      " |          otherwise, result is ``np.nan``.\n",
      " |\n",
      " |          For a window that is specified by an offset, ``min_periods`` will default to 1.\n",
      " |\n",
      " |          For a window that is specified by an integer, ``min_periods`` will default\n",
      " |          to the size of the window.\n",
      " |\n",
      " |      center : bool, default False\n",
      " |          If False, set the window labels as the right edge of the window index.\n",
      " |\n",
      " |          If True, set the window labels as the center of the window index.\n",
      " |\n",
      " |      win_type : str, default None\n",
      " |          If ``None``, all points are evenly weighted.\n",
      " |\n",
      " |          If a string, it must be a valid `scipy.signal window function\n",
      " |          <https://docs.scipy.org/doc/scipy/reference/signal.windows.html#module-scipy.signal.windows>`__.\n",
      " |\n",
      " |          Certain Scipy window types require additional parameters to be passed\n",
      " |          in the aggregation function. The additional parameters must match\n",
      " |          the keywords specified in the Scipy window type method signature.\n",
      " |\n",
      " |      on : str, optional\n",
      " |          For a DataFrame, a column label or Index level on which\n",
      " |          to calculate the rolling window, rather than the DataFrame's index.\n",
      " |\n",
      " |          Provided integer column is ignored and excluded from result since\n",
      " |          an integer index is not used to calculate the rolling window.\n",
      " |\n",
      " |      axis : int or str, default 0\n",
      " |          If ``0`` or ``'index'``, roll across the rows.\n",
      " |\n",
      " |          If ``1`` or ``'columns'``, roll across the columns.\n",
      " |\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |\n",
      " |          .. deprecated:: 2.1.0\n",
      " |\n",
      " |              The axis keyword is deprecated. For ``axis=1``,\n",
      " |              transpose the DataFrame first instead.\n",
      " |\n",
      " |      closed : str, default None\n",
      " |          If ``'right'``, the first point in the window is excluded from calculations.\n",
      " |\n",
      " |          If ``'left'``, the last point in the window is excluded from calculations.\n",
      " |\n",
      " |          If ``'both'``, the no points in the window are excluded from calculations.\n",
      " |\n",
      " |          If ``'neither'``, the first and last points in the window are excluded\n",
      " |          from calculations.\n",
      " |\n",
      " |          Default ``None`` (``'right'``).\n",
      " |\n",
      " |      step : int, default None\n",
      " |\n",
      " |          .. versionadded:: 1.5.0\n",
      " |\n",
      " |          Evaluate the window at every ``step`` result, equivalent to slicing as\n",
      " |          ``[::step]``. ``window`` must be an integer. Using a step argument other\n",
      " |          than None or 1 will produce a result with a different shape than the input.\n",
      " |\n",
      " |      method : str {'single', 'table'}, default 'single'\n",
      " |\n",
      " |          .. versionadded:: 1.3.0\n",
      " |\n",
      " |          Execute the rolling operation per single column or row (``'single'``)\n",
      " |          or over the entire object (``'table'``).\n",
      " |\n",
      " |          This argument is only implemented when specifying ``engine='numba'``\n",
      " |          in the method call.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      pandas.api.typing.Window or pandas.api.typing.Rolling\n",
      " |          An instance of Window is returned if ``win_type`` is passed. Otherwise,\n",
      " |          an instance of Rolling is returned.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      expanding : Provides expanding transformations.\n",
      " |      ewm : Provides exponential weighted functions.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      See :ref:`Windowing Operations <window.generic>` for further usage details\n",
      " |      and examples.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]})\n",
      " |      >>> df\n",
      " |           B\n",
      " |      0  0.0\n",
      " |      1  1.0\n",
      " |      2  2.0\n",
      " |      3  NaN\n",
      " |      4  4.0\n",
      " |\n",
      " |      **window**\n",
      " |\n",
      " |      Rolling sum with a window length of 2 observations.\n",
      " |\n",
      " |      >>> df.rolling(2).sum()\n",
      " |           B\n",
      " |      0  NaN\n",
      " |      1  1.0\n",
      " |      2  3.0\n",
      " |      3  NaN\n",
      " |      4  NaN\n",
      " |\n",
      " |      Rolling sum with a window span of 2 seconds.\n",
      " |\n",
      " |      >>> df_time = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]},\n",
      " |      ...                        index=[pd.Timestamp('20130101 09:00:00'),\n",
      " |      ...                               pd.Timestamp('20130101 09:00:02'),\n",
      " |      ...                               pd.Timestamp('20130101 09:00:03'),\n",
      " |      ...                               pd.Timestamp('20130101 09:00:05'),\n",
      " |      ...                               pd.Timestamp('20130101 09:00:06')])\n",
      " |\n",
      " |      >>> df_time\n",
      " |                             B\n",
      " |      2013-01-01 09:00:00  0.0\n",
      " |      2013-01-01 09:00:02  1.0\n",
      " |      2013-01-01 09:00:03  2.0\n",
      " |      2013-01-01 09:00:05  NaN\n",
      " |      2013-01-01 09:00:06  4.0\n",
      " |\n",
      " |      >>> df_time.rolling('2s').sum()\n",
      " |                             B\n",
      " |      2013-01-01 09:00:00  0.0\n",
      " |      2013-01-01 09:00:02  1.0\n",
      " |      2013-01-01 09:00:03  3.0\n",
      " |      2013-01-01 09:00:05  NaN\n",
      " |      2013-01-01 09:00:06  4.0\n",
      " |\n",
      " |      Rolling sum with forward looking windows with 2 observations.\n",
      " |\n",
      " |      >>> indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=2)\n",
      " |      >>> df.rolling(window=indexer, min_periods=1).sum()\n",
      " |           B\n",
      " |      0  1.0\n",
      " |      1  3.0\n",
      " |      2  2.0\n",
      " |      3  4.0\n",
      " |      4  4.0\n",
      " |\n",
      " |      **min_periods**\n",
      " |\n",
      " |      Rolling sum with a window length of 2 observations, but only needs a minimum of 1\n",
      " |      observation to calculate a value.\n",
      " |\n",
      " |      >>> df.rolling(2, min_periods=1).sum()\n",
      " |           B\n",
      " |      0  0.0\n",
      " |      1  1.0\n",
      " |      2  3.0\n",
      " |      3  2.0\n",
      " |      4  4.0\n",
      " |\n",
      " |      **center**\n",
      " |\n",
      " |      Rolling sum with the result assigned to the center of the window index.\n",
      " |\n",
      " |      >>> df.rolling(3, min_periods=1, center=True).sum()\n",
      " |           B\n",
      " |      0  1.0\n",
      " |      1  3.0\n",
      " |      2  3.0\n",
      " |      3  6.0\n",
      " |      4  4.0\n",
      " |\n",
      " |      >>> df.rolling(3, min_periods=1, center=False).sum()\n",
      " |           B\n",
      " |      0  0.0\n",
      " |      1  1.0\n",
      " |      2  3.0\n",
      " |      3  3.0\n",
      " |      4  6.0\n",
      " |\n",
      " |      **step**\n",
      " |\n",
      " |      Rolling sum with a window length of 2 observations, minimum of 1 observation to\n",
      " |      calculate a value, and a step of 2.\n",
      " |\n",
      " |      >>> df.rolling(2, min_periods=1, step=2).sum()\n",
      " |           B\n",
      " |      0  0.0\n",
      " |      2  3.0\n",
      " |      4  4.0\n",
      " |\n",
      " |      **win_type**\n",
      " |\n",
      " |      Rolling sum with a window length of 2, using the Scipy ``'gaussian'``\n",
      " |      window type. ``std`` is required in the aggregation function.\n",
      " |\n",
      " |      >>> df.rolling(2, win_type='gaussian').sum(std=3)\n",
      " |                B\n",
      " |      0       NaN\n",
      " |      1  0.986207\n",
      " |      2  2.958621\n",
      " |      3       NaN\n",
      " |      4       NaN\n",
      " |\n",
      " |      **on**\n",
      " |\n",
      " |      Rolling sum with a window length of 2 days.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({\n",
      " |      ...     'A': [pd.to_datetime('2020-01-01'),\n",
      " |      ...           pd.to_datetime('2020-01-01'),\n",
      " |      ...           pd.to_datetime('2020-01-02'),],\n",
      " |      ...     'B': [1, 2, 3], },\n",
      " |      ...     index=pd.date_range('2020', periods=3))\n",
      " |\n",
      " |      >>> df\n",
      " |                          A  B\n",
      " |      2020-01-01 2020-01-01  1\n",
      " |      2020-01-02 2020-01-01  2\n",
      " |      2020-01-03 2020-01-02  3\n",
      " |\n",
      " |      >>> df.rolling('2D', on='A').sum()\n",
      " |                          A    B\n",
      " |      2020-01-01 2020-01-01  1.0\n",
      " |      2020-01-02 2020-01-01  3.0\n",
      " |      2020-01-03 2020-01-02  6.0\n",
      " |\n",
      " |  sample(self, n: 'int | None' = None, frac: 'float | None' = None, replace: 'bool_t' = False, weights=None, random_state: 'RandomState | None' = None, axis: 'Axis | None' = None, ignore_index: 'bool_t' = False) -> 'Self'\n",
      " |      Return a random sample of items from an axis of object.\n",
      " |\n",
      " |      You can use `random_state` for reproducibility.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int, optional\n",
      " |          Number of items from axis to return. Cannot be used with `frac`.\n",
      " |          Default = 1 if `frac` = None.\n",
      " |      frac : float, optional\n",
      " |          Fraction of axis items to return. Cannot be used with `n`.\n",
      " |      replace : bool, default False\n",
      " |          Allow or disallow sampling of the same row more than once.\n",
      " |      weights : str or ndarray-like, optional\n",
      " |          Default 'None' results in equal probability weighting.\n",
      " |          If passed a Series, will align with target object on index. Index\n",
      " |          values in weights not found in sampled object will be ignored and\n",
      " |          index values in sampled object not in weights will be assigned\n",
      " |          weights of zero.\n",
      " |          If called on a DataFrame, will accept the name of a column\n",
      " |          when axis = 0.\n",
      " |          Unless weights are a Series, weights must be same length as axis\n",
      " |          being sampled.\n",
      " |          If weights do not sum to 1, they will be normalized to sum to 1.\n",
      " |          Missing values in the weights column will be treated as zero.\n",
      " |          Infinite values not allowed.\n",
      " |      random_state : int, array-like, BitGenerator, np.random.RandomState, np.random.Generator, optional\n",
      " |          If int, array-like, or BitGenerator, seed for random number generator.\n",
      " |          If np.random.RandomState or np.random.Generator, use as given.\n",
      " |\n",
      " |          .. versionchanged:: 1.4.0\n",
      " |\n",
      " |              np.random.Generator objects now accepted\n",
      " |\n",
      " |      axis : {0 or 'index', 1 or 'columns', None}, default None\n",
      " |          Axis to sample. Accepts axis number or name. Default is stat axis\n",
      " |          for given data type. For `Series` this parameter is unused and defaults to `None`.\n",
      " |      ignore_index : bool, default False\n",
      " |          If True, the resulting index will be labeled 0, 1, …, n - 1.\n",
      " |\n",
      " |          .. versionadded:: 1.3.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          A new object of same type as caller containing `n` items randomly\n",
      " |          sampled from the caller object.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrameGroupBy.sample: Generates random samples from each group of a\n",
      " |          DataFrame object.\n",
      " |      SeriesGroupBy.sample: Generates random samples from each group of a\n",
      " |          Series object.\n",
      " |      numpy.random.choice: Generates a random sample from a given 1-D numpy\n",
      " |          array.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      If `frac` > 1, `replacement` should be set to `True`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'num_legs': [2, 4, 8, 0],\n",
      " |      ...                    'num_wings': [2, 0, 0, 0],\n",
      " |      ...                    'num_specimen_seen': [10, 2, 1, 8]},\n",
      " |      ...                   index=['falcon', 'dog', 'spider', 'fish'])\n",
      " |      >>> df\n",
      " |              num_legs  num_wings  num_specimen_seen\n",
      " |      falcon         2          2                 10\n",
      " |      dog            4          0                  2\n",
      " |      spider         8          0                  1\n",
      " |      fish           0          0                  8\n",
      " |\n",
      " |      Extract 3 random elements from the ``Series`` ``df['num_legs']``:\n",
      " |      Note that we use `random_state` to ensure the reproducibility of\n",
      " |      the examples.\n",
      " |\n",
      " |      >>> df['num_legs'].sample(n=3, random_state=1)\n",
      " |      fish      0\n",
      " |      spider    8\n",
      " |      falcon    2\n",
      " |      Name: num_legs, dtype: int64\n",
      " |\n",
      " |      A random 50% sample of the ``DataFrame`` with replacement:\n",
      " |\n",
      " |      >>> df.sample(frac=0.5, replace=True, random_state=1)\n",
      " |            num_legs  num_wings  num_specimen_seen\n",
      " |      dog          4          0                  2\n",
      " |      fish         0          0                  8\n",
      " |\n",
      " |      An upsample sample of the ``DataFrame`` with replacement:\n",
      " |      Note that `replace` parameter has to be `True` for `frac` parameter > 1.\n",
      " |\n",
      " |      >>> df.sample(frac=2, replace=True, random_state=1)\n",
      " |              num_legs  num_wings  num_specimen_seen\n",
      " |      dog            4          0                  2\n",
      " |      fish           0          0                  8\n",
      " |      falcon         2          2                 10\n",
      " |      falcon         2          2                 10\n",
      " |      fish           0          0                  8\n",
      " |      dog            4          0                  2\n",
      " |      fish           0          0                  8\n",
      " |      dog            4          0                  2\n",
      " |\n",
      " |      Using a DataFrame column as weights. Rows with larger value in the\n",
      " |      `num_specimen_seen` column are more likely to be sampled.\n",
      " |\n",
      " |      >>> df.sample(n=2, weights='num_specimen_seen', random_state=1)\n",
      " |              num_legs  num_wings  num_specimen_seen\n",
      " |      falcon         2          2                 10\n",
      " |      fish           0          0                  8\n",
      " |\n",
      " |  set_flags(self, *, copy: 'bool_t' = False, allows_duplicate_labels: 'bool_t | None' = None) -> 'Self'\n",
      " |      Return a new object with updated flags.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      copy : bool, default False\n",
      " |          Specify if a copy of the object should be made.\n",
      " |\n",
      " |          .. note::\n",
      " |              The `copy` keyword will change behavior in pandas 3.0.\n",
      " |              `Copy-on-Write\n",
      " |              <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n",
      " |              will be enabled by default, which means that all methods with a\n",
      " |              `copy` keyword will use a lazy copy mechanism to defer the copy and\n",
      " |              ignore the `copy` keyword. The `copy` keyword will be removed in a\n",
      " |              future version of pandas.\n",
      " |\n",
      " |              You can already get the future behavior and improvements through\n",
      " |              enabling copy on write ``pd.options.mode.copy_on_write = True``\n",
      " |      allows_duplicate_labels : bool, optional\n",
      " |          Whether the returned object allows duplicate labels.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          The same type as the caller.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.attrs : Global metadata applying to this dataset.\n",
      " |      DataFrame.flags : Global flags applying to this object.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method returns a new object that's a view on the same data\n",
      " |      as the input. Mutating the input or the output values will be reflected\n",
      " |      in the other.\n",
      " |\n",
      " |      This method is intended to be used in method chains.\n",
      " |\n",
      " |      \"Flags\" differ from \"metadata\". Flags reflect properties of the\n",
      " |      pandas object (the Series or DataFrame). Metadata refer to properties\n",
      " |      of the dataset, and should be stored in :attr:`DataFrame.attrs`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({\"A\": [1, 2]})\n",
      " |      >>> df.flags.allows_duplicate_labels\n",
      " |      True\n",
      " |      >>> df2 = df.set_flags(allows_duplicate_labels=False)\n",
      " |      >>> df2.flags.allows_duplicate_labels\n",
      " |      False\n",
      " |\n",
      " |  squeeze(self, axis: 'Axis | None' = None)\n",
      " |      Squeeze 1 dimensional axis objects into scalars.\n",
      " |\n",
      " |      Series or DataFrames with a single element are squeezed to a scalar.\n",
      " |      DataFrames with a single column or a single row are squeezed to a\n",
      " |      Series. Otherwise the object is unchanged.\n",
      " |\n",
      " |      This method is most useful when you don't know if your\n",
      " |      object is a Series or DataFrame, but you do know it has just a single\n",
      " |      column. In that case you can safely call `squeeze` to ensure you have a\n",
      " |      Series.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns', None}, default None\n",
      " |          A specific axis to squeeze. By default, all length-1 axes are\n",
      " |          squeezed. For `Series` this parameter is unused and defaults to `None`.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame, Series, or scalar\n",
      " |          The projection after squeezing `axis` or all the axes.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.iloc : Integer-location based indexing for selecting scalars.\n",
      " |      DataFrame.iloc : Integer-location based indexing for selecting Series.\n",
      " |      Series.to_frame : Inverse of DataFrame.squeeze for a\n",
      " |          single-column DataFrame.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> primes = pd.Series([2, 3, 5, 7])\n",
      " |\n",
      " |      Slicing might produce a Series with a single value:\n",
      " |\n",
      " |      >>> even_primes = primes[primes % 2 == 0]\n",
      " |      >>> even_primes\n",
      " |      0    2\n",
      " |      dtype: int64\n",
      " |\n",
      " |      >>> even_primes.squeeze()\n",
      " |      2\n",
      " |\n",
      " |      Squeezing objects with more than one value in every axis does nothing:\n",
      " |\n",
      " |      >>> odd_primes = primes[primes % 2 == 1]\n",
      " |      >>> odd_primes\n",
      " |      1    3\n",
      " |      2    5\n",
      " |      3    7\n",
      " |      dtype: int64\n",
      " |\n",
      " |      >>> odd_primes.squeeze()\n",
      " |      1    3\n",
      " |      2    5\n",
      " |      3    7\n",
      " |      dtype: int64\n",
      " |\n",
      " |      Squeezing is even more effective when used with DataFrames.\n",
      " |\n",
      " |      >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=['a', 'b'])\n",
      " |      >>> df\n",
      " |         a  b\n",
      " |      0  1  2\n",
      " |      1  3  4\n",
      " |\n",
      " |      Slicing a single column will produce a DataFrame with the columns\n",
      " |      having only one value:\n",
      " |\n",
      " |      >>> df_a = df[['a']]\n",
      " |      >>> df_a\n",
      " |         a\n",
      " |      0  1\n",
      " |      1  3\n",
      " |\n",
      " |      So the columns can be squeezed down, resulting in a Series:\n",
      " |\n",
      " |      >>> df_a.squeeze('columns')\n",
      " |      0    1\n",
      " |      1    3\n",
      " |      Name: a, dtype: int64\n",
      " |\n",
      " |      Slicing a single row from a single column will produce a single\n",
      " |      scalar DataFrame:\n",
      " |\n",
      " |      >>> df_0a = df.loc[df.index < 1, ['a']]\n",
      " |      >>> df_0a\n",
      " |         a\n",
      " |      0  1\n",
      " |\n",
      " |      Squeezing the rows produces a single scalar Series:\n",
      " |\n",
      " |      >>> df_0a.squeeze('rows')\n",
      " |      a    1\n",
      " |      Name: 0, dtype: int64\n",
      " |\n",
      " |      Squeezing all axes will project directly into a scalar:\n",
      " |\n",
      " |      >>> df_0a.squeeze()\n",
      " |      1\n",
      " |\n",
      " |  swapaxes(self, axis1: 'Axis', axis2: 'Axis', copy: 'bool_t | None' = None) -> 'Self'\n",
      " |      Interchange axes and swap values axes appropriately.\n",
      " |\n",
      " |      .. deprecated:: 2.1.0\n",
      " |          ``swapaxes`` is deprecated and will be removed.\n",
      " |          Please use ``transpose`` instead.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      same as input\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Please see examples for :meth:`DataFrame.transpose`.\n",
      " |\n",
      " |  tail(self, n: 'int' = 5) -> 'Self'\n",
      " |      Return the last `n` rows.\n",
      " |\n",
      " |      This function returns last `n` rows from the object based on\n",
      " |      position. It is useful for quickly verifying data, for example,\n",
      " |      after sorting or appending rows.\n",
      " |\n",
      " |      For negative values of `n`, this function returns all rows except\n",
      " |      the first `|n|` rows, equivalent to ``df[|n|:]``.\n",
      " |\n",
      " |      If n is larger than the number of rows, this function returns all rows.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int, default 5\n",
      " |          Number of rows to select.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      type of caller\n",
      " |          The last `n` rows of the caller object.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.head : The first `n` rows of the caller object.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion',\n",
      " |      ...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})\n",
      " |      >>> df\n",
      " |            animal\n",
      " |      0  alligator\n",
      " |      1        bee\n",
      " |      2     falcon\n",
      " |      3       lion\n",
      " |      4     monkey\n",
      " |      5     parrot\n",
      " |      6      shark\n",
      " |      7      whale\n",
      " |      8      zebra\n",
      " |\n",
      " |      Viewing the last 5 lines\n",
      " |\n",
      " |      >>> df.tail()\n",
      " |         animal\n",
      " |      4  monkey\n",
      " |      5  parrot\n",
      " |      6   shark\n",
      " |      7   whale\n",
      " |      8   zebra\n",
      " |\n",
      " |      Viewing the last `n` lines (three in this case)\n",
      " |\n",
      " |      >>> df.tail(3)\n",
      " |        animal\n",
      " |      6  shark\n",
      " |      7  whale\n",
      " |      8  zebra\n",
      " |\n",
      " |      For negative values of `n`\n",
      " |\n",
      " |      >>> df.tail(-3)\n",
      " |         animal\n",
      " |      3    lion\n",
      " |      4  monkey\n",
      " |      5  parrot\n",
      " |      6   shark\n",
      " |      7   whale\n",
      " |      8   zebra\n",
      " |\n",
      " |  take(self, indices, axis: 'Axis' = 0, **kwargs) -> 'Self'\n",
      " |      Return the elements in the given *positional* indices along an axis.\n",
      " |\n",
      " |      This means that we are not indexing according to actual values in\n",
      " |      the index attribute of the object. We are indexing according to the\n",
      " |      actual position of the element in the object.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      indices : array-like\n",
      " |          An array of ints indicating which positions to take.\n",
      " |      axis : {0 or 'index', 1 or 'columns', None}, default 0\n",
      " |          The axis on which to select elements. ``0`` means that we are\n",
      " |          selecting rows, ``1`` means that we are selecting columns.\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |      **kwargs\n",
      " |          For compatibility with :meth:`numpy.take`. Has no effect on the\n",
      " |          output.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      same type as caller\n",
      " |          An array-like containing the elements taken from the object.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.loc : Select a subset of a DataFrame by labels.\n",
      " |      DataFrame.iloc : Select a subset of a DataFrame by positions.\n",
      " |      numpy.take : Take elements from an array along an axis.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([('falcon', 'bird', 389.0),\n",
      " |      ...                    ('parrot', 'bird', 24.0),\n",
      " |      ...                    ('lion', 'mammal', 80.5),\n",
      " |      ...                    ('monkey', 'mammal', np.nan)],\n",
      " |      ...                   columns=['name', 'class', 'max_speed'],\n",
      " |      ...                   index=[0, 2, 3, 1])\n",
      " |      >>> df\n",
      " |           name   class  max_speed\n",
      " |      0  falcon    bird      389.0\n",
      " |      2  parrot    bird       24.0\n",
      " |      3    lion  mammal       80.5\n",
      " |      1  monkey  mammal        NaN\n",
      " |\n",
      " |      Take elements at positions 0 and 3 along the axis 0 (default).\n",
      " |\n",
      " |      Note how the actual indices selected (0 and 1) do not correspond to\n",
      " |      our selected indices 0 and 3. That's because we are selecting the 0th\n",
      " |      and 3rd rows, not rows whose indices equal 0 and 3.\n",
      " |\n",
      " |      >>> df.take([0, 3])\n",
      " |           name   class  max_speed\n",
      " |      0  falcon    bird      389.0\n",
      " |      1  monkey  mammal        NaN\n",
      " |\n",
      " |      Take elements at indices 1 and 2 along the axis 1 (column selection).\n",
      " |\n",
      " |      >>> df.take([1, 2], axis=1)\n",
      " |          class  max_speed\n",
      " |      0    bird      389.0\n",
      " |      2    bird       24.0\n",
      " |      3  mammal       80.5\n",
      " |      1  mammal        NaN\n",
      " |\n",
      " |      We may take elements using negative integers for positive indices,\n",
      " |      starting from the end of the object, just like with Python lists.\n",
      " |\n",
      " |      >>> df.take([-1, -2])\n",
      " |           name   class  max_speed\n",
      " |      1  monkey  mammal        NaN\n",
      " |      3    lion  mammal       80.5\n",
      " |\n",
      " |  to_clipboard(self, *, excel: 'bool_t' = True, sep: 'str | None' = None, **kwargs) -> 'None'\n",
      " |      Copy object to the system clipboard.\n",
      " |\n",
      " |      Write a text representation of object to the system clipboard.\n",
      " |      This can be pasted into Excel, for example.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      excel : bool, default True\n",
      " |          Produce output in a csv format for easy pasting into excel.\n",
      " |\n",
      " |          - True, use the provided separator for csv pasting.\n",
      " |          - False, write a string representation of the object to the clipboard.\n",
      " |\n",
      " |      sep : str, default ``'\\t'``\n",
      " |          Field delimiter.\n",
      " |      **kwargs\n",
      " |          These parameters will be passed to DataFrame.to_csv.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.to_csv : Write a DataFrame to a comma-separated values\n",
      " |          (csv) file.\n",
      " |      read_clipboard : Read text from clipboard and pass to read_csv.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Requirements for your platform.\n",
      " |\n",
      " |        - Linux : `xclip`, or `xsel` (with `PyQt4` modules)\n",
      " |        - Windows : none\n",
      " |        - macOS : none\n",
      " |\n",
      " |      This method uses the processes developed for the package `pyperclip`. A\n",
      " |      solution to render any output string format is given in the examples.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Copy the contents of a DataFrame to the clipboard.\n",
      " |\n",
      " |      >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=['A', 'B', 'C'])\n",
      " |\n",
      " |      >>> df.to_clipboard(sep=',')  # doctest: +SKIP\n",
      " |      ... # Wrote the following to the system clipboard:\n",
      " |      ... # ,A,B,C\n",
      " |      ... # 0,1,2,3\n",
      " |      ... # 1,4,5,6\n",
      " |\n",
      " |      We can omit the index by passing the keyword `index` and setting\n",
      " |      it to false.\n",
      " |\n",
      " |      >>> df.to_clipboard(sep=',', index=False)  # doctest: +SKIP\n",
      " |      ... # Wrote the following to the system clipboard:\n",
      " |      ... # A,B,C\n",
      " |      ... # 1,2,3\n",
      " |      ... # 4,5,6\n",
      " |\n",
      " |      Using the original `pyperclip` package for any string output format.\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |         import pyperclip\n",
      " |         html = df.style.to_html()\n",
      " |         pyperclip.copy(html)\n",
      " |\n",
      " |  to_csv(self, path_or_buf: 'FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None' = None, *, sep: 'str' = ',', na_rep: 'str' = '', float_format: 'str | Callable | None' = None, columns: 'Sequence[Hashable] | None' = None, header: 'bool_t | list[str]' = True, index: 'bool_t' = True, index_label: 'IndexLabel | None' = None, mode: 'str' = 'w', encoding: 'str | None' = None, compression: 'CompressionOptions' = 'infer', quoting: 'int | None' = None, quotechar: 'str' = '\"', lineterminator: 'str | None' = None, chunksize: 'int | None' = None, date_format: 'str | None' = None, doublequote: 'bool_t' = True, escapechar: 'str | None' = None, decimal: 'str' = '.', errors: 'OpenFileErrors' = 'strict', storage_options: 'StorageOptions | None' = None) -> 'str | None'\n",
      " |      Write object to a comma-separated values (csv) file.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path_or_buf : str, path object, file-like object, or None, default None\n",
      " |          String, path object (implementing os.PathLike[str]), or file-like\n",
      " |          object implementing a write() function. If None, the result is\n",
      " |          returned as a string. If a non-binary file object is passed, it should\n",
      " |          be opened with `newline=''`, disabling universal newlines. If a binary\n",
      " |          file object is passed, `mode` might need to contain a `'b'`.\n",
      " |      sep : str, default ','\n",
      " |          String of length 1. Field delimiter for the output file.\n",
      " |      na_rep : str, default ''\n",
      " |          Missing data representation.\n",
      " |      float_format : str, Callable, default None\n",
      " |          Format string for floating point numbers. If a Callable is given, it takes\n",
      " |          precedence over other numeric formatting parameters, like decimal.\n",
      " |      columns : sequence, optional\n",
      " |          Columns to write.\n",
      " |      header : bool or list of str, default True\n",
      " |          Write out the column names. If a list of strings is given it is\n",
      " |          assumed to be aliases for the column names.\n",
      " |      index : bool, default True\n",
      " |          Write row names (index).\n",
      " |      index_label : str or sequence, or False, default None\n",
      " |          Column label for index column(s) if desired. If None is given, and\n",
      " |          `header` and `index` are True, then the index names are used. A\n",
      " |          sequence should be given if the object uses MultiIndex. If\n",
      " |          False do not print fields for index names. Use index_label=False\n",
      " |          for easier importing in R.\n",
      " |      mode : {'w', 'x', 'a'}, default 'w'\n",
      " |          Forwarded to either `open(mode=)` or `fsspec.open(mode=)` to control\n",
      " |          the file opening. Typical values include:\n",
      " |\n",
      " |          - 'w', truncate the file first.\n",
      " |          - 'x', exclusive creation, failing if the file already exists.\n",
      " |          - 'a', append to the end of file if it exists.\n",
      " |\n",
      " |      encoding : str, optional\n",
      " |          A string representing the encoding to use in the output file,\n",
      " |          defaults to 'utf-8'. `encoding` is not supported if `path_or_buf`\n",
      " |          is a non-binary file object.\n",
      " |      compression : str or dict, default 'infer'\n",
      " |          For on-the-fly compression of the output data. If 'infer' and 'path_or_buf' is\n",
      " |          path-like, then detect compression from the following extensions: '.gz',\n",
      " |          '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
      " |          (otherwise no compression).\n",
      " |          Set to ``None`` for no compression.\n",
      " |          Can also be a dict with key ``'method'`` set\n",
      " |          to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n",
      " |          other key-value pairs are forwarded to\n",
      " |          ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      " |          ``bz2.BZ2File``, ``zstandard.ZstdCompressor``, ``lzma.LZMAFile`` or\n",
      " |          ``tarfile.TarFile``, respectively.\n",
      " |          As an example, the following could be passed for faster compression and to create\n",
      " |          a reproducible gzip archive:\n",
      " |          ``compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}``.\n",
      " |\n",
      " |          .. versionadded:: 1.5.0\n",
      " |              Added support for `.tar` files.\n",
      " |\n",
      " |             May be a dict with key 'method' as compression mode\n",
      " |             and other entries as additional compression options if\n",
      " |             compression mode is 'zip'.\n",
      " |\n",
      " |             Passing compression options as keys in dict is\n",
      " |             supported for compression modes 'gzip', 'bz2', 'zstd', and 'zip'.\n",
      " |      quoting : optional constant from csv module\n",
      " |          Defaults to csv.QUOTE_MINIMAL. If you have set a `float_format`\n",
      " |          then floats are converted to strings and thus csv.QUOTE_NONNUMERIC\n",
      " |          will treat them as non-numeric.\n",
      " |      quotechar : str, default '\\\"'\n",
      " |          String of length 1. Character used to quote fields.\n",
      " |      lineterminator : str, optional\n",
      " |          The newline character or character sequence to use in the output\n",
      " |          file. Defaults to `os.linesep`, which depends on the OS in which\n",
      " |          this method is called ('\\\\n' for linux, '\\\\r\\\\n' for Windows, i.e.).\n",
      " |\n",
      " |          .. versionchanged:: 1.5.0\n",
      " |\n",
      " |              Previously was line_terminator, changed for consistency with\n",
      " |              read_csv and the standard library 'csv' module.\n",
      " |\n",
      " |      chunksize : int or None\n",
      " |          Rows to write at a time.\n",
      " |      date_format : str, default None\n",
      " |          Format string for datetime objects.\n",
      " |      doublequote : bool, default True\n",
      " |          Control quoting of `quotechar` inside a field.\n",
      " |      escapechar : str, default None\n",
      " |          String of length 1. Character used to escape `sep` and `quotechar`\n",
      " |          when appropriate.\n",
      " |      decimal : str, default '.'\n",
      " |          Character recognized as decimal separator. E.g. use ',' for\n",
      " |          European data.\n",
      " |      errors : str, default 'strict'\n",
      " |          Specifies how encoding and decoding errors are to be handled.\n",
      " |          See the errors argument for :func:`open` for a full list\n",
      " |          of options.\n",
      " |\n",
      " |      storage_options : dict, optional\n",
      " |          Extra options that make sense for a particular storage connection, e.g.\n",
      " |          host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      " |          are forwarded to ``urllib.request.Request`` as header options. For other\n",
      " |          URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      " |          forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      " |          details, and for more examples on storage options refer `here\n",
      " |          <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      " |          highlight=storage_options#reading-writing-remote-files>`_.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      None or str\n",
      " |          If path_or_buf is None, returns the resulting csv format as a\n",
      " |          string. Otherwise returns None.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      read_csv : Load a CSV file into a DataFrame.\n",
      " |      to_excel : Write DataFrame to an Excel file.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Create 'out.csv' containing 'df' without indices\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n",
      " |      ...                    'mask': ['red', 'purple'],\n",
      " |      ...                    'weapon': ['sai', 'bo staff']})\n",
      " |      >>> df.to_csv('out.csv', index=False)  # doctest: +SKIP\n",
      " |\n",
      " |      Create 'out.zip' containing 'out.csv'\n",
      " |\n",
      " |      >>> df.to_csv(index=False)\n",
      " |      'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'\n",
      " |      >>> compression_opts = dict(method='zip',\n",
      " |      ...                         archive_name='out.csv')  # doctest: +SKIP\n",
      " |      >>> df.to_csv('out.zip', index=False,\n",
      " |      ...           compression=compression_opts)  # doctest: +SKIP\n",
      " |\n",
      " |      To write a csv file to a new folder or nested folder you will first\n",
      " |      need to create it using either Pathlib or os:\n",
      " |\n",
      " |      >>> from pathlib import Path  # doctest: +SKIP\n",
      " |      >>> filepath = Path('folder/subfolder/out.csv')  # doctest: +SKIP\n",
      " |      >>> filepath.parent.mkdir(parents=True, exist_ok=True)  # doctest: +SKIP\n",
      " |      >>> df.to_csv(filepath)  # doctest: +SKIP\n",
      " |\n",
      " |      >>> import os  # doctest: +SKIP\n",
      " |      >>> os.makedirs('folder/subfolder', exist_ok=True)  # doctest: +SKIP\n",
      " |      >>> df.to_csv('folder/subfolder/out.csv')  # doctest: +SKIP\n",
      " |\n",
      " |  to_excel(self, excel_writer: 'FilePath | WriteExcelBuffer | ExcelWriter', *, sheet_name: 'str' = 'Sheet1', na_rep: 'str' = '', float_format: 'str | None' = None, columns: 'Sequence[Hashable] | None' = None, header: 'Sequence[Hashable] | bool_t' = True, index: 'bool_t' = True, index_label: 'IndexLabel | None' = None, startrow: 'int' = 0, startcol: 'int' = 0, engine: \"Literal['openpyxl', 'xlsxwriter'] | None\" = None, merge_cells: 'bool_t' = True, inf_rep: 'str' = 'inf', freeze_panes: 'tuple[int, int] | None' = None, storage_options: 'StorageOptions | None' = None, engine_kwargs: 'dict[str, Any] | None' = None) -> 'None'\n",
      " |      Write object to an Excel sheet.\n",
      " |\n",
      " |      To write a single object to an Excel .xlsx file it is only necessary to\n",
      " |      specify a target file name. To write to multiple sheets it is necessary to\n",
      " |      create an `ExcelWriter` object with a target file name, and specify a sheet\n",
      " |      in the file to write to.\n",
      " |\n",
      " |      Multiple sheets may be written to by specifying unique `sheet_name`.\n",
      " |      With all data written to the file it is necessary to save the changes.\n",
      " |      Note that creating an `ExcelWriter` object with a file name that already\n",
      " |      exists will result in the contents of the existing file being erased.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      excel_writer : path-like, file-like, or ExcelWriter object\n",
      " |          File path or existing ExcelWriter.\n",
      " |      sheet_name : str, default 'Sheet1'\n",
      " |          Name of sheet which will contain DataFrame.\n",
      " |      na_rep : str, default ''\n",
      " |          Missing data representation.\n",
      " |      float_format : str, optional\n",
      " |          Format string for floating point numbers. For example\n",
      " |          ``float_format=\"%.2f\"`` will format 0.1234 to 0.12.\n",
      " |      columns : sequence or list of str, optional\n",
      " |          Columns to write.\n",
      " |      header : bool or list of str, default True\n",
      " |          Write out the column names. If a list of string is given it is\n",
      " |          assumed to be aliases for the column names.\n",
      " |      index : bool, default True\n",
      " |          Write row names (index).\n",
      " |      index_label : str or sequence, optional\n",
      " |          Column label for index column(s) if desired. If not specified, and\n",
      " |          `header` and `index` are True, then the index names are used. A\n",
      " |          sequence should be given if the DataFrame uses MultiIndex.\n",
      " |      startrow : int, default 0\n",
      " |          Upper left cell row to dump data frame.\n",
      " |      startcol : int, default 0\n",
      " |          Upper left cell column to dump data frame.\n",
      " |      engine : str, optional\n",
      " |          Write engine to use, 'openpyxl' or 'xlsxwriter'. You can also set this\n",
      " |          via the options ``io.excel.xlsx.writer`` or\n",
      " |          ``io.excel.xlsm.writer``.\n",
      " |\n",
      " |      merge_cells : bool, default True\n",
      " |          Write MultiIndex and Hierarchical Rows as merged cells.\n",
      " |      inf_rep : str, default 'inf'\n",
      " |          Representation for infinity (there is no native representation for\n",
      " |          infinity in Excel).\n",
      " |      freeze_panes : tuple of int (length 2), optional\n",
      " |          Specifies the one-based bottommost row and rightmost column that\n",
      " |          is to be frozen.\n",
      " |      storage_options : dict, optional\n",
      " |          Extra options that make sense for a particular storage connection, e.g.\n",
      " |          host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      " |          are forwarded to ``urllib.request.Request`` as header options. For other\n",
      " |          URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      " |          forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      " |          details, and for more examples on storage options refer `here\n",
      " |          <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      " |          highlight=storage_options#reading-writing-remote-files>`_.\n",
      " |\n",
      " |          .. versionadded:: 1.2.0\n",
      " |      engine_kwargs : dict, optional\n",
      " |          Arbitrary keyword arguments passed to excel engine.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      " |      ExcelWriter : Class for writing DataFrame objects into excel sheets.\n",
      " |      read_excel : Read an Excel file into a pandas DataFrame.\n",
      " |      read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
      " |      io.formats.style.Styler.to_excel : Add styles to Excel sheet.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      For compatibility with :meth:`~DataFrame.to_csv`,\n",
      " |      to_excel serializes lists and dicts to strings before writing.\n",
      " |\n",
      " |      Once a workbook has been saved it is not possible to write further\n",
      " |      data without rewriting the whole workbook.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |\n",
      " |      Create, write to and save a workbook:\n",
      " |\n",
      " |      >>> df1 = pd.DataFrame([['a', 'b'], ['c', 'd']],\n",
      " |      ...                    index=['row 1', 'row 2'],\n",
      " |      ...                    columns=['col 1', 'col 2'])\n",
      " |      >>> df1.to_excel(\"output.xlsx\")  # doctest: +SKIP\n",
      " |\n",
      " |      To specify the sheet name:\n",
      " |\n",
      " |      >>> df1.to_excel(\"output.xlsx\",\n",
      " |      ...              sheet_name='Sheet_name_1')  # doctest: +SKIP\n",
      " |\n",
      " |      If you wish to write to more than one sheet in the workbook, it is\n",
      " |      necessary to specify an ExcelWriter object:\n",
      " |\n",
      " |      >>> df2 = df1.copy()\n",
      " |      >>> with pd.ExcelWriter('output.xlsx') as writer:  # doctest: +SKIP\n",
      " |      ...     df1.to_excel(writer, sheet_name='Sheet_name_1')\n",
      " |      ...     df2.to_excel(writer, sheet_name='Sheet_name_2')\n",
      " |\n",
      " |      ExcelWriter can also be used to append to an existing Excel file:\n",
      " |\n",
      " |      >>> with pd.ExcelWriter('output.xlsx',\n",
      " |      ...                     mode='a') as writer:  # doctest: +SKIP\n",
      " |      ...     df1.to_excel(writer, sheet_name='Sheet_name_3')\n",
      " |\n",
      " |      To set the library that is used to write the Excel file,\n",
      " |      you can pass the `engine` keyword (the default engine is\n",
      " |      automatically chosen depending on the file extension):\n",
      " |\n",
      " |      >>> df1.to_excel('output1.xlsx', engine='xlsxwriter')  # doctest: +SKIP\n",
      " |\n",
      " |  to_hdf(self, path_or_buf: 'FilePath | HDFStore', *, key: 'str', mode: \"Literal['a', 'w', 'r+']\" = 'a', complevel: 'int | None' = None, complib: \"Literal['zlib', 'lzo', 'bzip2', 'blosc'] | None\" = None, append: 'bool_t' = False, format: \"Literal['fixed', 'table'] | None\" = None, index: 'bool_t' = True, min_itemsize: 'int | dict[str, int] | None' = None, nan_rep=None, dropna: 'bool_t | None' = None, data_columns: 'Literal[True] | list[str] | None' = None, errors: 'OpenFileErrors' = 'strict', encoding: 'str' = 'UTF-8') -> 'None'\n",
      " |      Write the contained data to an HDF5 file using HDFStore.\n",
      " |\n",
      " |      Hierarchical Data Format (HDF) is self-describing, allowing an\n",
      " |      application to interpret the structure and contents of a file with\n",
      " |      no outside information. One HDF file can hold a mix of related objects\n",
      " |      which can be accessed as a group or as individual objects.\n",
      " |\n",
      " |      In order to add another DataFrame or Series to an existing HDF file\n",
      " |      please use append mode and a different a key.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |         One can store a subclass of ``DataFrame`` or ``Series`` to HDF5,\n",
      " |         but the type of the subclass is lost upon storing.\n",
      " |\n",
      " |      For more information see the :ref:`user guide <io.hdf5>`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path_or_buf : str or pandas.HDFStore\n",
      " |          File path or HDFStore object.\n",
      " |      key : str\n",
      " |          Identifier for the group in the store.\n",
      " |      mode : {'a', 'w', 'r+'}, default 'a'\n",
      " |          Mode to open file:\n",
      " |\n",
      " |          - 'w': write, a new file is created (an existing file with\n",
      " |            the same name would be deleted).\n",
      " |          - 'a': append, an existing file is opened for reading and\n",
      " |            writing, and if the file does not exist it is created.\n",
      " |          - 'r+': similar to 'a', but the file must already exist.\n",
      " |      complevel : {0-9}, default None\n",
      " |          Specifies a compression level for data.\n",
      " |          A value of 0 or None disables compression.\n",
      " |      complib : {'zlib', 'lzo', 'bzip2', 'blosc'}, default 'zlib'\n",
      " |          Specifies the compression library to be used.\n",
      " |          These additional compressors for Blosc are supported\n",
      " |          (default if no compressor specified: 'blosc:blosclz'):\n",
      " |          {'blosc:blosclz', 'blosc:lz4', 'blosc:lz4hc', 'blosc:snappy',\n",
      " |          'blosc:zlib', 'blosc:zstd'}.\n",
      " |          Specifying a compression library which is not available issues\n",
      " |          a ValueError.\n",
      " |      append : bool, default False\n",
      " |          For Table formats, append the input data to the existing.\n",
      " |      format : {'fixed', 'table', None}, default 'fixed'\n",
      " |          Possible values:\n",
      " |\n",
      " |          - 'fixed': Fixed format. Fast writing/reading. Not-appendable,\n",
      " |            nor searchable.\n",
      " |          - 'table': Table format. Write as a PyTables Table structure\n",
      " |            which may perform worse but allow more flexible operations\n",
      " |            like searching / selecting subsets of the data.\n",
      " |          - If None, pd.get_option('io.hdf.default_format') is checked,\n",
      " |            followed by fallback to \"fixed\".\n",
      " |      index : bool, default True\n",
      " |          Write DataFrame index as a column.\n",
      " |      min_itemsize : dict or int, optional\n",
      " |          Map column names to minimum string sizes for columns.\n",
      " |      nan_rep : Any, optional\n",
      " |          How to represent null values as str.\n",
      " |          Not allowed with append=True.\n",
      " |      dropna : bool, default False, optional\n",
      " |          Remove missing values.\n",
      " |      data_columns : list of columns or True, optional\n",
      " |          List of columns to create as indexed data columns for on-disk\n",
      " |          queries, or True to use all columns. By default only the axes\n",
      " |          of the object are indexed. See\n",
      " |          :ref:`Query via data columns<io.hdf5-query-data-columns>`. for\n",
      " |          more information.\n",
      " |          Applicable only to format='table'.\n",
      " |      errors : str, default 'strict'\n",
      " |          Specifies how encoding and decoding errors are to be handled.\n",
      " |          See the errors argument for :func:`open` for a full list\n",
      " |          of options.\n",
      " |      encoding : str, default \"UTF-8\"\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      read_hdf : Read from HDF file.\n",
      " |      DataFrame.to_orc : Write a DataFrame to the binary orc format.\n",
      " |      DataFrame.to_parquet : Write a DataFrame to the binary parquet format.\n",
      " |      DataFrame.to_sql : Write to a SQL table.\n",
      " |      DataFrame.to_feather : Write out feather-format for DataFrames.\n",
      " |      DataFrame.to_csv : Write out to a csv file.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},\n",
      " |      ...                   index=['a', 'b', 'c'])  # doctest: +SKIP\n",
      " |      >>> df.to_hdf('data.h5', key='df', mode='w')  # doctest: +SKIP\n",
      " |\n",
      " |      We can add another object to the same file:\n",
      " |\n",
      " |      >>> s = pd.Series([1, 2, 3, 4])  # doctest: +SKIP\n",
      " |      >>> s.to_hdf('data.h5', key='s')  # doctest: +SKIP\n",
      " |\n",
      " |      Reading from HDF file:\n",
      " |\n",
      " |      >>> pd.read_hdf('data.h5', 'df')  # doctest: +SKIP\n",
      " |      A  B\n",
      " |      a  1  4\n",
      " |      b  2  5\n",
      " |      c  3  6\n",
      " |      >>> pd.read_hdf('data.h5', 's')  # doctest: +SKIP\n",
      " |      0    1\n",
      " |      1    2\n",
      " |      2    3\n",
      " |      3    4\n",
      " |      dtype: int64\n",
      " |\n",
      " |  to_latex(self, buf: 'FilePath | WriteBuffer[str] | None' = None, *, columns: 'Sequence[Hashable] | None' = None, header: 'bool_t | SequenceNotStr[str]' = True, index: 'bool_t' = True, na_rep: 'str' = 'NaN', formatters: 'FormattersType | None' = None, float_format: 'FloatFormatType | None' = None, sparsify: 'bool_t | None' = None, index_names: 'bool_t' = True, bold_rows: 'bool_t' = False, column_format: 'str | None' = None, longtable: 'bool_t | None' = None, escape: 'bool_t | None' = None, encoding: 'str | None' = None, decimal: 'str' = '.', multicolumn: 'bool_t | None' = None, multicolumn_format: 'str | None' = None, multirow: 'bool_t | None' = None, caption: 'str | tuple[str, str] | None' = None, label: 'str | None' = None, position: 'str | None' = None) -> 'str | None'\n",
      " |      Render object to a LaTeX tabular, longtable, or nested table.\n",
      " |\n",
      " |      Requires ``\\usepackage{{booktabs}}``.  The output can be copy/pasted\n",
      " |      into a main LaTeX document or read from an external file\n",
      " |      with ``\\input{{table.tex}}``.\n",
      " |\n",
      " |      .. versionchanged:: 2.0.0\n",
      " |         Refactored to use the Styler implementation via jinja2 templating.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      buf : str, Path or StringIO-like, optional, default None\n",
      " |          Buffer to write to. If None, the output is returned as a string.\n",
      " |      columns : list of label, optional\n",
      " |          The subset of columns to write. Writes all columns by default.\n",
      " |      header : bool or list of str, default True\n",
      " |          Write out the column names. If a list of strings is given,\n",
      " |          it is assumed to be aliases for the column names.\n",
      " |      index : bool, default True\n",
      " |          Write row names (index).\n",
      " |      na_rep : str, default 'NaN'\n",
      " |          Missing data representation.\n",
      " |      formatters : list of functions or dict of {{str: function}}, optional\n",
      " |          Formatter functions to apply to columns' elements by position or\n",
      " |          name. The result of each function must be a unicode string.\n",
      " |          List must be of length equal to the number of columns.\n",
      " |      float_format : one-parameter function or str, optional, default None\n",
      " |          Formatter for floating point numbers. For example\n",
      " |          ``float_format=\"%.2f\"`` and ``float_format=\"{{:0.2f}}\".format`` will\n",
      " |          both result in 0.1234 being formatted as 0.12.\n",
      " |      sparsify : bool, optional\n",
      " |          Set to False for a DataFrame with a hierarchical index to print\n",
      " |          every multiindex key at each row. By default, the value will be\n",
      " |          read from the config module.\n",
      " |      index_names : bool, default True\n",
      " |          Prints the names of the indexes.\n",
      " |      bold_rows : bool, default False\n",
      " |          Make the row labels bold in the output.\n",
      " |      column_format : str, optional\n",
      " |          The columns format as specified in `LaTeX table format\n",
      " |          <https://en.wikibooks.org/wiki/LaTeX/Tables>`__ e.g. 'rcl' for 3\n",
      " |          columns. By default, 'l' will be used for all columns except\n",
      " |          columns of numbers, which default to 'r'.\n",
      " |      longtable : bool, optional\n",
      " |          Use a longtable environment instead of tabular. Requires\n",
      " |          adding a \\usepackage{{longtable}} to your LaTeX preamble.\n",
      " |          By default, the value will be read from the pandas config\n",
      " |          module, and set to `True` if the option ``styler.latex.environment`` is\n",
      " |          `\"longtable\"`.\n",
      " |\n",
      " |          .. versionchanged:: 2.0.0\n",
      " |             The pandas option affecting this argument has changed.\n",
      " |      escape : bool, optional\n",
      " |          By default, the value will be read from the pandas config\n",
      " |          module and set to `True` if the option ``styler.format.escape`` is\n",
      " |          `\"latex\"`. When set to False prevents from escaping latex special\n",
      " |          characters in column names.\n",
      " |\n",
      " |          .. versionchanged:: 2.0.0\n",
      " |             The pandas option affecting this argument has changed, as has the\n",
      " |             default value to `False`.\n",
      " |      encoding : str, optional\n",
      " |          A string representing the encoding to use in the output file,\n",
      " |          defaults to 'utf-8'.\n",
      " |      decimal : str, default '.'\n",
      " |          Character recognized as decimal separator, e.g. ',' in Europe.\n",
      " |      multicolumn : bool, default True\n",
      " |          Use \\multicolumn to enhance MultiIndex columns.\n",
      " |          The default will be read from the config module, and is set\n",
      " |          as the option ``styler.sparse.columns``.\n",
      " |\n",
      " |          .. versionchanged:: 2.0.0\n",
      " |             The pandas option affecting this argument has changed.\n",
      " |      multicolumn_format : str, default 'r'\n",
      " |          The alignment for multicolumns, similar to `column_format`\n",
      " |          The default will be read from the config module, and is set as the option\n",
      " |          ``styler.latex.multicol_align``.\n",
      " |\n",
      " |          .. versionchanged:: 2.0.0\n",
      " |             The pandas option affecting this argument has changed, as has the\n",
      " |             default value to \"r\".\n",
      " |      multirow : bool, default True\n",
      " |          Use \\multirow to enhance MultiIndex rows. Requires adding a\n",
      " |          \\usepackage{{multirow}} to your LaTeX preamble. Will print\n",
      " |          centered labels (instead of top-aligned) across the contained\n",
      " |          rows, separating groups via clines. The default will be read\n",
      " |          from the pandas config module, and is set as the option\n",
      " |          ``styler.sparse.index``.\n",
      " |\n",
      " |          .. versionchanged:: 2.0.0\n",
      " |             The pandas option affecting this argument has changed, as has the\n",
      " |             default value to `True`.\n",
      " |      caption : str or tuple, optional\n",
      " |          Tuple (full_caption, short_caption),\n",
      " |          which results in ``\\caption[short_caption]{{full_caption}}``;\n",
      " |          if a single string is passed, no short caption will be set.\n",
      " |      label : str, optional\n",
      " |          The LaTeX label to be placed inside ``\\label{{}}`` in the output.\n",
      " |          This is used with ``\\ref{{}}`` in the main ``.tex`` file.\n",
      " |\n",
      " |      position : str, optional\n",
      " |          The LaTeX positional argument for tables, to be placed after\n",
      " |          ``\\begin{{}}`` in the output.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      str or None\n",
      " |          If buf is None, returns the result as a string. Otherwise returns None.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      io.formats.style.Styler.to_latex : Render a DataFrame to LaTeX\n",
      " |          with conditional formatting.\n",
      " |      DataFrame.to_string : Render a DataFrame to a console-friendly\n",
      " |          tabular output.\n",
      " |      DataFrame.to_html : Render a DataFrame as an HTML table.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      As of v2.0.0 this method has changed to use the Styler implementation as\n",
      " |      part of :meth:`.Styler.to_latex` via ``jinja2`` templating. This means\n",
      " |      that ``jinja2`` is a requirement, and needs to be installed, for this method\n",
      " |      to function. It is advised that users switch to using Styler, since that\n",
      " |      implementation is more frequently updated and contains much more\n",
      " |      flexibility with the output.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Convert a general DataFrame to LaTeX with formatting:\n",
      " |\n",
      " |      >>> df = pd.DataFrame(dict(name=['Raphael', 'Donatello'],\n",
      " |      ...                        age=[26, 45],\n",
      " |      ...                        height=[181.23, 177.65]))\n",
      " |      >>> print(df.to_latex(index=False,\n",
      " |      ...                   formatters={\"name\": str.upper},\n",
      " |      ...                   float_format=\"{:.1f}\".format,\n",
      " |      ... ))  # doctest: +SKIP\n",
      " |      \\begin{tabular}{lrr}\n",
      " |      \\toprule\n",
      " |      name & age & height \\\\\n",
      " |      \\midrule\n",
      " |      RAPHAEL & 26 & 181.2 \\\\\n",
      " |      DONATELLO & 45 & 177.7 \\\\\n",
      " |      \\bottomrule\n",
      " |      \\end{tabular}\n",
      " |\n",
      " |  to_pickle(self, path: 'FilePath | WriteBuffer[bytes]', *, compression: 'CompressionOptions' = 'infer', protocol: 'int' = 5, storage_options: 'StorageOptions | None' = None) -> 'None'\n",
      " |      Pickle (serialize) object to file.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str, path object, or file-like object\n",
      " |          String, path object (implementing ``os.PathLike[str]``), or file-like\n",
      " |          object implementing a binary ``write()`` function. File path where\n",
      " |          the pickled object will be stored.\n",
      " |      compression : str or dict, default 'infer'\n",
      " |          For on-the-fly compression of the output data. If 'infer' and 'path' is\n",
      " |          path-like, then detect compression from the following extensions: '.gz',\n",
      " |          '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
      " |          (otherwise no compression).\n",
      " |          Set to ``None`` for no compression.\n",
      " |          Can also be a dict with key ``'method'`` set\n",
      " |          to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n",
      " |          other key-value pairs are forwarded to\n",
      " |          ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      " |          ``bz2.BZ2File``, ``zstandard.ZstdCompressor``, ``lzma.LZMAFile`` or\n",
      " |          ``tarfile.TarFile``, respectively.\n",
      " |          As an example, the following could be passed for faster compression and to create\n",
      " |          a reproducible gzip archive:\n",
      " |          ``compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}``.\n",
      " |\n",
      " |          .. versionadded:: 1.5.0\n",
      " |              Added support for `.tar` files.\n",
      " |      protocol : int\n",
      " |          Int which indicates which protocol should be used by the pickler,\n",
      " |          default HIGHEST_PROTOCOL (see [1]_ paragraph 12.1.2). The possible\n",
      " |          values are 0, 1, 2, 3, 4, 5. A negative value for the protocol\n",
      " |          parameter is equivalent to setting its value to HIGHEST_PROTOCOL.\n",
      " |\n",
      " |          .. [1] https://docs.python.org/3/library/pickle.html.\n",
      " |\n",
      " |      storage_options : dict, optional\n",
      " |          Extra options that make sense for a particular storage connection, e.g.\n",
      " |          host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      " |          are forwarded to ``urllib.request.Request`` as header options. For other\n",
      " |          URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      " |          forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      " |          details, and for more examples on storage options refer `here\n",
      " |          <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      " |          highlight=storage_options#reading-writing-remote-files>`_.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      read_pickle : Load pickled pandas object (or any object) from file.\n",
      " |      DataFrame.to_hdf : Write DataFrame to an HDF5 file.\n",
      " |      DataFrame.to_sql : Write DataFrame to a SQL database.\n",
      " |      DataFrame.to_parquet : Write a DataFrame to the binary parquet format.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> original_df = pd.DataFrame({\"foo\": range(5), \"bar\": range(5, 10)})  # doctest: +SKIP\n",
      " |      >>> original_df  # doctest: +SKIP\n",
      " |         foo  bar\n",
      " |      0    0    5\n",
      " |      1    1    6\n",
      " |      2    2    7\n",
      " |      3    3    8\n",
      " |      4    4    9\n",
      " |      >>> original_df.to_pickle(\"./dummy.pkl\")  # doctest: +SKIP\n",
      " |\n",
      " |      >>> unpickled_df = pd.read_pickle(\"./dummy.pkl\")  # doctest: +SKIP\n",
      " |      >>> unpickled_df  # doctest: +SKIP\n",
      " |         foo  bar\n",
      " |      0    0    5\n",
      " |      1    1    6\n",
      " |      2    2    7\n",
      " |      3    3    8\n",
      " |      4    4    9\n",
      " |\n",
      " |  to_sql(self, name: 'str', con, *, schema: 'str | None' = None, if_exists: \"Literal['fail', 'replace', 'append']\" = 'fail', index: 'bool_t' = True, index_label: 'IndexLabel | None' = None, chunksize: 'int | None' = None, dtype: 'DtypeArg | None' = None, method: \"Literal['multi'] | Callable | None\" = None) -> 'int | None'\n",
      " |      Write records stored in a DataFrame to a SQL database.\n",
      " |\n",
      " |      Databases supported by SQLAlchemy [1]_ are supported. Tables can be\n",
      " |      newly created, appended to, or overwritten.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          Name of SQL table.\n",
      " |      con : sqlalchemy.engine.(Engine or Connection) or sqlite3.Connection\n",
      " |          Using SQLAlchemy makes it possible to use any DB supported by that\n",
      " |          library. Legacy support is provided for sqlite3.Connection objects. The user\n",
      " |          is responsible for engine disposal and connection closure for the SQLAlchemy\n",
      " |          connectable. See `here                 <https://docs.sqlalchemy.org/en/20/core/connections.html>`_.\n",
      " |          If passing a sqlalchemy.engine.Connection which is already in a transaction,\n",
      " |          the transaction will not be committed.  If passing a sqlite3.Connection,\n",
      " |          it will not be possible to roll back the record insertion.\n",
      " |\n",
      " |      schema : str, optional\n",
      " |          Specify the schema (if database flavor supports this). If None, use\n",
      " |          default schema.\n",
      " |      if_exists : {'fail', 'replace', 'append'}, default 'fail'\n",
      " |          How to behave if the table already exists.\n",
      " |\n",
      " |          * fail: Raise a ValueError.\n",
      " |          * replace: Drop the table before inserting new values.\n",
      " |          * append: Insert new values to the existing table.\n",
      " |\n",
      " |      index : bool, default True\n",
      " |          Write DataFrame index as a column. Uses `index_label` as the column\n",
      " |          name in the table. Creates a table index for this column.\n",
      " |      index_label : str or sequence, default None\n",
      " |          Column label for index column(s). If None is given (default) and\n",
      " |          `index` is True, then the index names are used.\n",
      " |          A sequence should be given if the DataFrame uses MultiIndex.\n",
      " |      chunksize : int, optional\n",
      " |          Specify the number of rows in each batch to be written at a time.\n",
      " |          By default, all rows will be written at once.\n",
      " |      dtype : dict or scalar, optional\n",
      " |          Specifying the datatype for columns. If a dictionary is used, the\n",
      " |          keys should be the column names and the values should be the\n",
      " |          SQLAlchemy types or strings for the sqlite3 legacy mode. If a\n",
      " |          scalar is provided, it will be applied to all columns.\n",
      " |      method : {None, 'multi', callable}, optional\n",
      " |          Controls the SQL insertion clause used:\n",
      " |\n",
      " |          * None : Uses standard SQL ``INSERT`` clause (one per row).\n",
      " |          * 'multi': Pass multiple values in a single ``INSERT`` clause.\n",
      " |          * callable with signature ``(pd_table, conn, keys, data_iter)``.\n",
      " |\n",
      " |          Details and a sample callable implementation can be found in the\n",
      " |          section :ref:`insert method <io.sql.method>`.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      None or int\n",
      " |          Number of rows affected by to_sql. None is returned if the callable\n",
      " |          passed into ``method`` does not return an integer number of rows.\n",
      " |\n",
      " |          The number of returned rows affected is the sum of the ``rowcount``\n",
      " |          attribute of ``sqlite3.Cursor`` or SQLAlchemy connectable which may not\n",
      " |          reflect the exact number of written rows as stipulated in the\n",
      " |          `sqlite3 <https://docs.python.org/3/library/sqlite3.html#sqlite3.Cursor.rowcount>`__ or\n",
      " |          `SQLAlchemy <https://docs.sqlalchemy.org/en/20/core/connections.html#sqlalchemy.engine.CursorResult.rowcount>`__.\n",
      " |\n",
      " |          .. versionadded:: 1.4.0\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      ValueError\n",
      " |          When the table already exists and `if_exists` is 'fail' (the\n",
      " |          default).\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      read_sql : Read a DataFrame from a table.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Timezone aware datetime columns will be written as\n",
      " |      ``Timestamp with timezone`` type with SQLAlchemy if supported by the\n",
      " |      database. Otherwise, the datetimes will be stored as timezone unaware\n",
      " |      timestamps local to the original timezone.\n",
      " |\n",
      " |      Not all datastores support ``method=\"multi\"``. Oracle, for example,\n",
      " |      does not support multi-value insert.\n",
      " |\n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://docs.sqlalchemy.org\n",
      " |      .. [2] https://www.python.org/dev/peps/pep-0249/\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Create an in-memory SQLite database.\n",
      " |\n",
      " |      >>> from sqlalchemy import create_engine\n",
      " |      >>> engine = create_engine('sqlite://', echo=False)\n",
      " |\n",
      " |      Create a table from scratch with 3 rows.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n",
      " |      >>> df\n",
      " |           name\n",
      " |      0  User 1\n",
      " |      1  User 2\n",
      " |      2  User 3\n",
      " |\n",
      " |      >>> df.to_sql(name='users', con=engine)\n",
      " |      3\n",
      " |      >>> from sqlalchemy import text\n",
      " |      >>> with engine.connect() as conn:\n",
      " |      ...    conn.execute(text(\"SELECT * FROM users\")).fetchall()\n",
      " |      [(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]\n",
      " |\n",
      " |      An `sqlalchemy.engine.Connection` can also be passed to `con`:\n",
      " |\n",
      " |      >>> with engine.begin() as connection:\n",
      " |      ...     df1 = pd.DataFrame({'name' : ['User 4', 'User 5']})\n",
      " |      ...     df1.to_sql(name='users', con=connection, if_exists='append')\n",
      " |      2\n",
      " |\n",
      " |      This is allowed to support operations that require that the same\n",
      " |      DBAPI connection is used for the entire operation.\n",
      " |\n",
      " |      >>> df2 = pd.DataFrame({'name' : ['User 6', 'User 7']})\n",
      " |      >>> df2.to_sql(name='users', con=engine, if_exists='append')\n",
      " |      2\n",
      " |      >>> with engine.connect() as conn:\n",
      " |      ...    conn.execute(text(\"SELECT * FROM users\")).fetchall()\n",
      " |      [(0, 'User 1'), (1, 'User 2'), (2, 'User 3'),\n",
      " |       (0, 'User 4'), (1, 'User 5'), (0, 'User 6'),\n",
      " |       (1, 'User 7')]\n",
      " |\n",
      " |      Overwrite the table with just ``df2``.\n",
      " |\n",
      " |      >>> df2.to_sql(name='users', con=engine, if_exists='replace',\n",
      " |      ...            index_label='id')\n",
      " |      2\n",
      " |      >>> with engine.connect() as conn:\n",
      " |      ...    conn.execute(text(\"SELECT * FROM users\")).fetchall()\n",
      " |      [(0, 'User 6'), (1, 'User 7')]\n",
      " |\n",
      " |      Use ``method`` to define a callable insertion method to do nothing\n",
      " |      if there's a primary key conflict on a table in a PostgreSQL database.\n",
      " |\n",
      " |      >>> from sqlalchemy.dialects.postgresql import insert\n",
      " |      >>> def insert_on_conflict_nothing(table, conn, keys, data_iter):\n",
      " |      ...     # \"a\" is the primary key in \"conflict_table\"\n",
      " |      ...     data = [dict(zip(keys, row)) for row in data_iter]\n",
      " |      ...     stmt = insert(table.table).values(data).on_conflict_do_nothing(index_elements=[\"a\"])\n",
      " |      ...     result = conn.execute(stmt)\n",
      " |      ...     return result.rowcount\n",
      " |      >>> df_conflict.to_sql(name=\"conflict_table\", con=conn, if_exists=\"append\", method=insert_on_conflict_nothing)  # doctest: +SKIP\n",
      " |      0\n",
      " |\n",
      " |      For MySQL, a callable to update columns ``b`` and ``c`` if there's a conflict\n",
      " |      on a primary key.\n",
      " |\n",
      " |      >>> from sqlalchemy.dialects.mysql import insert\n",
      " |      >>> def insert_on_conflict_update(table, conn, keys, data_iter):\n",
      " |      ...     # update columns \"b\" and \"c\" on primary key conflict\n",
      " |      ...     data = [dict(zip(keys, row)) for row in data_iter]\n",
      " |      ...     stmt = (\n",
      " |      ...         insert(table.table)\n",
      " |      ...         .values(data)\n",
      " |      ...     )\n",
      " |      ...     stmt = stmt.on_duplicate_key_update(b=stmt.inserted.b, c=stmt.inserted.c)\n",
      " |      ...     result = conn.execute(stmt)\n",
      " |      ...     return result.rowcount\n",
      " |      >>> df_conflict.to_sql(name=\"conflict_table\", con=conn, if_exists=\"append\", method=insert_on_conflict_update)  # doctest: +SKIP\n",
      " |      2\n",
      " |\n",
      " |      Specify the dtype (especially useful for integers with missing values).\n",
      " |      Notice that while pandas is forced to store the data as floating point,\n",
      " |      the database supports nullable integers. When fetching the data with\n",
      " |      Python, we get back integer scalars.\n",
      " |\n",
      " |      >>> df = pd.DataFrame({\"A\": [1, None, 2]})\n",
      " |      >>> df\n",
      " |           A\n",
      " |      0  1.0\n",
      " |      1  NaN\n",
      " |      2  2.0\n",
      " |\n",
      " |      >>> from sqlalchemy.types import Integer\n",
      " |      >>> df.to_sql(name='integers', con=engine, index=False,\n",
      " |      ...           dtype={\"A\": Integer()})\n",
      " |      3\n",
      " |\n",
      " |      >>> with engine.connect() as conn:\n",
      " |      ...   conn.execute(text(\"SELECT * FROM integers\")).fetchall()\n",
      " |      [(1,), (None,), (2,)]\n",
      " |\n",
      " |  to_xarray(self)\n",
      " |      Return an xarray object from the pandas object.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      xarray.DataArray or xarray.Dataset\n",
      " |          Data in the pandas structure converted to Dataset if the object is\n",
      " |          a DataFrame, or a DataArray if the object is a Series.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.to_hdf : Write DataFrame to an HDF5 file.\n",
      " |      DataFrame.to_parquet : Write a DataFrame to the binary parquet format.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      See the `xarray docs <https://xarray.pydata.org/en/stable/>`__\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([('falcon', 'bird', 389.0, 2),\n",
      " |      ...                    ('parrot', 'bird', 24.0, 2),\n",
      " |      ...                    ('lion', 'mammal', 80.5, 4),\n",
      " |      ...                    ('monkey', 'mammal', np.nan, 4)],\n",
      " |      ...                   columns=['name', 'class', 'max_speed',\n",
      " |      ...                            'num_legs'])\n",
      " |      >>> df\n",
      " |           name   class  max_speed  num_legs\n",
      " |      0  falcon    bird      389.0         2\n",
      " |      1  parrot    bird       24.0         2\n",
      " |      2    lion  mammal       80.5         4\n",
      " |      3  monkey  mammal        NaN         4\n",
      " |\n",
      " |      >>> df.to_xarray()  # doctest: +SKIP\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:    (index: 4)\n",
      " |      Coordinates:\n",
      " |        * index      (index) int64 32B 0 1 2 3\n",
      " |      Data variables:\n",
      " |          name       (index) object 32B 'falcon' 'parrot' 'lion' 'monkey'\n",
      " |          class      (index) object 32B 'bird' 'bird' 'mammal' 'mammal'\n",
      " |          max_speed  (index) float64 32B 389.0 24.0 80.5 nan\n",
      " |          num_legs   (index) int64 32B 2 2 4 4\n",
      " |\n",
      " |      >>> df['max_speed'].to_xarray()  # doctest: +SKIP\n",
      " |      <xarray.DataArray 'max_speed' (index: 4)>\n",
      " |      array([389. ,  24. ,  80.5,   nan])\n",
      " |      Coordinates:\n",
      " |        * index    (index) int64 0 1 2 3\n",
      " |\n",
      " |      >>> dates = pd.to_datetime(['2018-01-01', '2018-01-01',\n",
      " |      ...                         '2018-01-02', '2018-01-02'])\n",
      " |      >>> df_multiindex = pd.DataFrame({'date': dates,\n",
      " |      ...                               'animal': ['falcon', 'parrot',\n",
      " |      ...                                          'falcon', 'parrot'],\n",
      " |      ...                               'speed': [350, 18, 361, 15]})\n",
      " |      >>> df_multiindex = df_multiindex.set_index(['date', 'animal'])\n",
      " |\n",
      " |      >>> df_multiindex\n",
      " |                         speed\n",
      " |      date       animal\n",
      " |      2018-01-01 falcon    350\n",
      " |                 parrot     18\n",
      " |      2018-01-02 falcon    361\n",
      " |                 parrot     15\n",
      " |\n",
      " |      >>> df_multiindex.to_xarray()  # doctest: +SKIP\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (date: 2, animal: 2)\n",
      " |      Coordinates:\n",
      " |        * date     (date) datetime64[ns] 2018-01-01 2018-01-02\n",
      " |        * animal   (animal) object 'falcon' 'parrot'\n",
      " |      Data variables:\n",
      " |          speed    (date, animal) int64 350 18 361 15\n",
      " |\n",
      " |  truncate(self, before=None, after=None, axis: 'Axis | None' = None, copy: 'bool_t | None' = None) -> 'Self'\n",
      " |      Truncate a Series or DataFrame before and after some index value.\n",
      " |\n",
      " |      This is a useful shorthand for boolean indexing based on index\n",
      " |      values above or below certain thresholds.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      before : date, str, int\n",
      " |          Truncate all rows before this index value.\n",
      " |      after : date, str, int\n",
      " |          Truncate all rows after this index value.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, optional\n",
      " |          Axis to truncate. Truncates the index (rows) by default.\n",
      " |          For `Series` this parameter is unused and defaults to 0.\n",
      " |      copy : bool, default is True,\n",
      " |          Return a copy of the truncated section.\n",
      " |\n",
      " |          .. note::\n",
      " |              The `copy` keyword will change behavior in pandas 3.0.\n",
      " |              `Copy-on-Write\n",
      " |              <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n",
      " |              will be enabled by default, which means that all methods with a\n",
      " |              `copy` keyword will use a lazy copy mechanism to defer the copy and\n",
      " |              ignore the `copy` keyword. The `copy` keyword will be removed in a\n",
      " |              future version of pandas.\n",
      " |\n",
      " |              You can already get the future behavior and improvements through\n",
      " |              enabling copy on write ``pd.options.mode.copy_on_write = True``\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      type of caller\n",
      " |          The truncated Series or DataFrame.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.loc : Select a subset of a DataFrame by label.\n",
      " |      DataFrame.iloc : Select a subset of a DataFrame by position.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      If the index being truncated contains only datetime values,\n",
      " |      `before` and `after` may be specified as strings instead of\n",
      " |      Timestamps.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'A': ['a', 'b', 'c', 'd', 'e'],\n",
      " |      ...                    'B': ['f', 'g', 'h', 'i', 'j'],\n",
      " |      ...                    'C': ['k', 'l', 'm', 'n', 'o']},\n",
      " |      ...                   index=[1, 2, 3, 4, 5])\n",
      " |      >>> df\n",
      " |         A  B  C\n",
      " |      1  a  f  k\n",
      " |      2  b  g  l\n",
      " |      3  c  h  m\n",
      " |      4  d  i  n\n",
      " |      5  e  j  o\n",
      " |\n",
      " |      >>> df.truncate(before=2, after=4)\n",
      " |         A  B  C\n",
      " |      2  b  g  l\n",
      " |      3  c  h  m\n",
      " |      4  d  i  n\n",
      " |\n",
      " |      The columns of a DataFrame can be truncated.\n",
      " |\n",
      " |      >>> df.truncate(before=\"A\", after=\"B\", axis=\"columns\")\n",
      " |         A  B\n",
      " |      1  a  f\n",
      " |      2  b  g\n",
      " |      3  c  h\n",
      " |      4  d  i\n",
      " |      5  e  j\n",
      " |\n",
      " |      For Series, only rows can be truncated.\n",
      " |\n",
      " |      >>> df['A'].truncate(before=2, after=4)\n",
      " |      2    b\n",
      " |      3    c\n",
      " |      4    d\n",
      " |      Name: A, dtype: object\n",
      " |\n",
      " |      The index values in ``truncate`` can be datetimes or string\n",
      " |      dates.\n",
      " |\n",
      " |      >>> dates = pd.date_range('2016-01-01', '2016-02-01', freq='s')\n",
      " |      >>> df = pd.DataFrame(index=dates, data={'A': 1})\n",
      " |      >>> df.tail()\n",
      " |                           A\n",
      " |      2016-01-31 23:59:56  1\n",
      " |      2016-01-31 23:59:57  1\n",
      " |      2016-01-31 23:59:58  1\n",
      " |      2016-01-31 23:59:59  1\n",
      " |      2016-02-01 00:00:00  1\n",
      " |\n",
      " |      >>> df.truncate(before=pd.Timestamp('2016-01-05'),\n",
      " |      ...             after=pd.Timestamp('2016-01-10')).tail()\n",
      " |                           A\n",
      " |      2016-01-09 23:59:56  1\n",
      " |      2016-01-09 23:59:57  1\n",
      " |      2016-01-09 23:59:58  1\n",
      " |      2016-01-09 23:59:59  1\n",
      " |      2016-01-10 00:00:00  1\n",
      " |\n",
      " |      Because the index is a DatetimeIndex containing only dates, we can\n",
      " |      specify `before` and `after` as strings. They will be coerced to\n",
      " |      Timestamps before truncation.\n",
      " |\n",
      " |      >>> df.truncate('2016-01-05', '2016-01-10').tail()\n",
      " |                           A\n",
      " |      2016-01-09 23:59:56  1\n",
      " |      2016-01-09 23:59:57  1\n",
      " |      2016-01-09 23:59:58  1\n",
      " |      2016-01-09 23:59:59  1\n",
      " |      2016-01-10 00:00:00  1\n",
      " |\n",
      " |      Note that ``truncate`` assumes a 0 value for any unspecified time\n",
      " |      component (midnight). This differs from partial string slicing, which\n",
      " |      returns any partially matching dates.\n",
      " |\n",
      " |      >>> df.loc['2016-01-05':'2016-01-10', :].tail()\n",
      " |                           A\n",
      " |      2016-01-10 23:59:55  1\n",
      " |      2016-01-10 23:59:56  1\n",
      " |      2016-01-10 23:59:57  1\n",
      " |      2016-01-10 23:59:58  1\n",
      " |      2016-01-10 23:59:59  1\n",
      " |\n",
      " |  tz_convert(self, tz, axis: 'Axis' = 0, level=None, copy: 'bool_t | None' = None) -> 'Self'\n",
      " |      Convert tz-aware axis to target time zone.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tz : str or tzinfo object or None\n",
      " |          Target time zone. Passing ``None`` will convert to\n",
      " |          UTC and remove the timezone information.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The axis to convert\n",
      " |      level : int, str, default None\n",
      " |          If axis is a MultiIndex, convert a specific level. Otherwise\n",
      " |          must be None.\n",
      " |      copy : bool, default True\n",
      " |          Also make a copy of the underlying data.\n",
      " |\n",
      " |          .. note::\n",
      " |              The `copy` keyword will change behavior in pandas 3.0.\n",
      " |              `Copy-on-Write\n",
      " |              <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n",
      " |              will be enabled by default, which means that all methods with a\n",
      " |              `copy` keyword will use a lazy copy mechanism to defer the copy and\n",
      " |              ignore the `copy` keyword. The `copy` keyword will be removed in a\n",
      " |              future version of pandas.\n",
      " |\n",
      " |              You can already get the future behavior and improvements through\n",
      " |              enabling copy on write ``pd.options.mode.copy_on_write = True``\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series/DataFrame\n",
      " |          Object with time zone converted axis.\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      TypeError\n",
      " |          If the axis is tz-naive.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Change to another time zone:\n",
      " |\n",
      " |      >>> s = pd.Series(\n",
      " |      ...     [1],\n",
      " |      ...     index=pd.DatetimeIndex(['2018-09-15 01:30:00+02:00']),\n",
      " |      ... )\n",
      " |      >>> s.tz_convert('Asia/Shanghai')\n",
      " |      2018-09-15 07:30:00+08:00    1\n",
      " |      dtype: int64\n",
      " |\n",
      " |      Pass None to convert to UTC and get a tz-naive index:\n",
      " |\n",
      " |      >>> s = pd.Series([1],\n",
      " |      ...               index=pd.DatetimeIndex(['2018-09-15 01:30:00+02:00']))\n",
      " |      >>> s.tz_convert(None)\n",
      " |      2018-09-14 23:30:00    1\n",
      " |      dtype: int64\n",
      " |\n",
      " |  tz_localize(self, tz, axis: 'Axis' = 0, level=None, copy: 'bool_t | None' = None, ambiguous: 'TimeAmbiguous' = 'raise', nonexistent: 'TimeNonexistent' = 'raise') -> 'Self'\n",
      " |      Localize tz-naive index of a Series or DataFrame to target time zone.\n",
      " |\n",
      " |      This operation localizes the Index. To localize the values in a\n",
      " |      timezone-naive Series, use :meth:`Series.dt.tz_localize`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tz : str or tzinfo or None\n",
      " |          Time zone to localize. Passing ``None`` will remove the\n",
      " |          time zone information and preserve local time.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The axis to localize\n",
      " |      level : int, str, default None\n",
      " |          If axis ia a MultiIndex, localize a specific level. Otherwise\n",
      " |          must be None.\n",
      " |      copy : bool, default True\n",
      " |          Also make a copy of the underlying data.\n",
      " |\n",
      " |          .. note::\n",
      " |              The `copy` keyword will change behavior in pandas 3.0.\n",
      " |              `Copy-on-Write\n",
      " |              <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n",
      " |              will be enabled by default, which means that all methods with a\n",
      " |              `copy` keyword will use a lazy copy mechanism to defer the copy and\n",
      " |              ignore the `copy` keyword. The `copy` keyword will be removed in a\n",
      " |              future version of pandas.\n",
      " |\n",
      " |              You can already get the future behavior and improvements through\n",
      " |              enabling copy on write ``pd.options.mode.copy_on_write = True``\n",
      " |      ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'\n",
      " |          When clocks moved backward due to DST, ambiguous times may arise.\n",
      " |          For example in Central European Time (UTC+01), when going from\n",
      " |          03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at\n",
      " |          00:30:00 UTC and at 01:30:00 UTC. In such a situation, the\n",
      " |          `ambiguous` parameter dictates how ambiguous times should be\n",
      " |          handled.\n",
      " |\n",
      " |          - 'infer' will attempt to infer fall dst-transition hours based on\n",
      " |            order\n",
      " |          - bool-ndarray where True signifies a DST time, False designates\n",
      " |            a non-DST time (note that this flag is only applicable for\n",
      " |            ambiguous times)\n",
      " |          - 'NaT' will return NaT where there are ambiguous times\n",
      " |          - 'raise' will raise an AmbiguousTimeError if there are ambiguous\n",
      " |            times.\n",
      " |      nonexistent : str, default 'raise'\n",
      " |          A nonexistent time does not exist in a particular timezone\n",
      " |          where clocks moved forward due to DST. Valid values are:\n",
      " |\n",
      " |          - 'shift_forward' will shift the nonexistent time forward to the\n",
      " |            closest existing time\n",
      " |          - 'shift_backward' will shift the nonexistent time backward to the\n",
      " |            closest existing time\n",
      " |          - 'NaT' will return NaT where there are nonexistent times\n",
      " |          - timedelta objects will shift nonexistent times by the timedelta\n",
      " |          - 'raise' will raise an NonExistentTimeError if there are\n",
      " |            nonexistent times.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series/DataFrame\n",
      " |          Same type as the input.\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      TypeError\n",
      " |          If the TimeSeries is tz-aware and tz is not None.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Localize local times:\n",
      " |\n",
      " |      >>> s = pd.Series(\n",
      " |      ...     [1],\n",
      " |      ...     index=pd.DatetimeIndex(['2018-09-15 01:30:00']),\n",
      " |      ... )\n",
      " |      >>> s.tz_localize('CET')\n",
      " |      2018-09-15 01:30:00+02:00    1\n",
      " |      dtype: int64\n",
      " |\n",
      " |      Pass None to convert to tz-naive index and preserve local time:\n",
      " |\n",
      " |      >>> s = pd.Series([1],\n",
      " |      ...               index=pd.DatetimeIndex(['2018-09-15 01:30:00+02:00']))\n",
      " |      >>> s.tz_localize(None)\n",
      " |      2018-09-15 01:30:00    1\n",
      " |      dtype: int64\n",
      " |\n",
      " |      Be careful with DST changes. When there is sequential data, pandas\n",
      " |      can infer the DST time:\n",
      " |\n",
      " |      >>> s = pd.Series(range(7),\n",
      " |      ...               index=pd.DatetimeIndex(['2018-10-28 01:30:00',\n",
      " |      ...                                       '2018-10-28 02:00:00',\n",
      " |      ...                                       '2018-10-28 02:30:00',\n",
      " |      ...                                       '2018-10-28 02:00:00',\n",
      " |      ...                                       '2018-10-28 02:30:00',\n",
      " |      ...                                       '2018-10-28 03:00:00',\n",
      " |      ...                                       '2018-10-28 03:30:00']))\n",
      " |      >>> s.tz_localize('CET', ambiguous='infer')\n",
      " |      2018-10-28 01:30:00+02:00    0\n",
      " |      2018-10-28 02:00:00+02:00    1\n",
      " |      2018-10-28 02:30:00+02:00    2\n",
      " |      2018-10-28 02:00:00+01:00    3\n",
      " |      2018-10-28 02:30:00+01:00    4\n",
      " |      2018-10-28 03:00:00+01:00    5\n",
      " |      2018-10-28 03:30:00+01:00    6\n",
      " |      dtype: int64\n",
      " |\n",
      " |      In some cases, inferring the DST is impossible. In such cases, you can\n",
      " |      pass an ndarray to the ambiguous parameter to set the DST explicitly\n",
      " |\n",
      " |      >>> s = pd.Series(range(3),\n",
      " |      ...               index=pd.DatetimeIndex(['2018-10-28 01:20:00',\n",
      " |      ...                                       '2018-10-28 02:36:00',\n",
      " |      ...                                       '2018-10-28 03:46:00']))\n",
      " |      >>> s.tz_localize('CET', ambiguous=np.array([True, True, False]))\n",
      " |      2018-10-28 01:20:00+02:00    0\n",
      " |      2018-10-28 02:36:00+02:00    1\n",
      " |      2018-10-28 03:46:00+01:00    2\n",
      " |      dtype: int64\n",
      " |\n",
      " |      If the DST transition causes nonexistent times, you can shift these\n",
      " |      dates forward or backward with a timedelta object or `'shift_forward'`\n",
      " |      or `'shift_backward'`.\n",
      " |\n",
      " |      >>> s = pd.Series(range(2),\n",
      " |      ...               index=pd.DatetimeIndex(['2015-03-29 02:30:00',\n",
      " |      ...                                       '2015-03-29 03:30:00']))\n",
      " |      >>> s.tz_localize('Europe/Warsaw', nonexistent='shift_forward')\n",
      " |      2015-03-29 03:00:00+02:00    0\n",
      " |      2015-03-29 03:30:00+02:00    1\n",
      " |      dtype: int64\n",
      " |      >>> s.tz_localize('Europe/Warsaw', nonexistent='shift_backward')\n",
      " |      2015-03-29 01:59:59.999999999+01:00    0\n",
      " |      2015-03-29 03:30:00+02:00              1\n",
      " |      dtype: int64\n",
      " |      >>> s.tz_localize('Europe/Warsaw', nonexistent=pd.Timedelta('1h'))\n",
      " |      2015-03-29 03:30:00+02:00    0\n",
      " |      2015-03-29 03:30:00+02:00    1\n",
      " |      dtype: int64\n",
      " |\n",
      " |  where(self, cond, other=nan, *, inplace: 'bool_t' = False, axis: 'Axis | None' = None, level: 'Level | None' = None) -> 'Self | None'\n",
      " |      Replace values where the condition is False.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cond : bool Series/DataFrame, array-like, or callable\n",
      " |          Where `cond` is True, keep the original value. Where\n",
      " |          False, replace with corresponding value from `other`.\n",
      " |          If `cond` is callable, it is computed on the Series/DataFrame and\n",
      " |          should return boolean Series/DataFrame or array. The callable must\n",
      " |          not change input Series/DataFrame (though pandas doesn't check it).\n",
      " |      other : scalar, Series/DataFrame, or callable\n",
      " |          Entries where `cond` is False are replaced with\n",
      " |          corresponding value from `other`.\n",
      " |          If other is callable, it is computed on the Series/DataFrame and\n",
      " |          should return scalar or Series/DataFrame. The callable must not\n",
      " |          change input Series/DataFrame (though pandas doesn't check it).\n",
      " |          If not specified, entries will be filled with the corresponding\n",
      " |          NULL value (``np.nan`` for numpy dtypes, ``pd.NA`` for extension\n",
      " |          dtypes).\n",
      " |      inplace : bool, default False\n",
      " |          Whether to perform the operation in place on the data.\n",
      " |      axis : int, default None\n",
      " |          Alignment axis if needed. For `Series` this parameter is\n",
      " |          unused and defaults to 0.\n",
      " |      level : int, default None\n",
      " |          Alignment level if needed.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Same type as caller or None if ``inplace=True``.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :func:`DataFrame.mask` : Return an object of same shape as\n",
      " |          self.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      The where method is an application of the if-then idiom. For each\n",
      " |      element in the calling DataFrame, if ``cond`` is ``True`` the\n",
      " |      element is used; otherwise the corresponding element from the DataFrame\n",
      " |      ``other`` is used. If the axis of ``other`` does not align with axis of\n",
      " |      ``cond`` Series/DataFrame, the misaligned index positions will be filled with\n",
      " |      False.\n",
      " |\n",
      " |      The signature for :func:`DataFrame.where` differs from\n",
      " |      :func:`numpy.where`. Roughly ``df1.where(m, df2)`` is equivalent to\n",
      " |      ``np.where(m, df1, df2)``.\n",
      " |\n",
      " |      For further details and examples see the ``where`` documentation in\n",
      " |      :ref:`indexing <indexing.where_mask>`.\n",
      " |\n",
      " |      The dtype of the object takes precedence. The fill value is casted to\n",
      " |      the object's dtype, if this can be done losslessly.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> s = pd.Series(range(5))\n",
      " |      >>> s.where(s > 0)\n",
      " |      0    NaN\n",
      " |      1    1.0\n",
      " |      2    2.0\n",
      " |      3    3.0\n",
      " |      4    4.0\n",
      " |      dtype: float64\n",
      " |      >>> s.mask(s > 0)\n",
      " |      0    0.0\n",
      " |      1    NaN\n",
      " |      2    NaN\n",
      " |      3    NaN\n",
      " |      4    NaN\n",
      " |      dtype: float64\n",
      " |\n",
      " |      >>> s = pd.Series(range(5))\n",
      " |      >>> t = pd.Series([True, False])\n",
      " |      >>> s.where(t, 99)\n",
      " |      0     0\n",
      " |      1    99\n",
      " |      2    99\n",
      " |      3    99\n",
      " |      4    99\n",
      " |      dtype: int64\n",
      " |      >>> s.mask(t, 99)\n",
      " |      0    99\n",
      " |      1     1\n",
      " |      2    99\n",
      " |      3    99\n",
      " |      4    99\n",
      " |      dtype: int64\n",
      " |\n",
      " |      >>> s.where(s > 1, 10)\n",
      " |      0    10\n",
      " |      1    10\n",
      " |      2    2\n",
      " |      3    3\n",
      " |      4    4\n",
      " |      dtype: int64\n",
      " |      >>> s.mask(s > 1, 10)\n",
      " |      0     0\n",
      " |      1     1\n",
      " |      2    10\n",
      " |      3    10\n",
      " |      4    10\n",
      " |      dtype: int64\n",
      " |\n",
      " |      >>> df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B'])\n",
      " |      >>> df\n",
      " |         A  B\n",
      " |      0  0  1\n",
      " |      1  2  3\n",
      " |      2  4  5\n",
      " |      3  6  7\n",
      " |      4  8  9\n",
      " |      >>> m = df % 3 == 0\n",
      " |      >>> df.where(m, -df)\n",
      " |         A  B\n",
      " |      0  0 -1\n",
      " |      1 -2  3\n",
      " |      2 -4 -5\n",
      " |      3  6 -7\n",
      " |      4 -8  9\n",
      " |      >>> df.where(m, -df) == np.where(m, df, -df)\n",
      " |            A     B\n",
      " |      0  True  True\n",
      " |      1  True  True\n",
      " |      2  True  True\n",
      " |      3  True  True\n",
      " |      4  True  True\n",
      " |      >>> df.where(m, -df) == df.mask(~m, -df)\n",
      " |            A     B\n",
      " |      0  True  True\n",
      " |      1  True  True\n",
      " |      2  True  True\n",
      " |      3  True  True\n",
      " |      4  True  True\n",
      " |\n",
      " |  xs(self, key: 'IndexLabel', axis: 'Axis' = 0, level: 'IndexLabel | None' = None, drop_level: 'bool_t' = True) -> 'Self'\n",
      " |      Return cross-section from the Series/DataFrame.\n",
      " |\n",
      " |      This method takes a `key` argument to select data at a particular\n",
      " |      level of a MultiIndex.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : label or tuple of label\n",
      " |          Label contained in the index, or partially in a MultiIndex.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          Axis to retrieve cross-section on.\n",
      " |      level : object, defaults to first n levels (n=1 or len(key))\n",
      " |          In case of a key partially contained in a MultiIndex, indicate\n",
      " |          which levels are used. Levels can be referred by label or position.\n",
      " |      drop_level : bool, default True\n",
      " |          If False, returns object with same levels as self.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          Cross-section from the original Series or DataFrame\n",
      " |          corresponding to the selected index levels.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.loc : Access a group of rows and columns\n",
      " |          by label(s) or a boolean array.\n",
      " |      DataFrame.iloc : Purely integer-location based indexing\n",
      " |          for selection by position.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      `xs` can not be used to set values.\n",
      " |\n",
      " |      MultiIndex Slicers is a generic way to get/set values on\n",
      " |      any level or levels.\n",
      " |      It is a superset of `xs` functionality, see\n",
      " |      :ref:`MultiIndex Slicers <advanced.mi_slicers>`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> d = {'num_legs': [4, 4, 2, 2],\n",
      " |      ...      'num_wings': [0, 0, 2, 2],\n",
      " |      ...      'class': ['mammal', 'mammal', 'mammal', 'bird'],\n",
      " |      ...      'animal': ['cat', 'dog', 'bat', 'penguin'],\n",
      " |      ...      'locomotion': ['walks', 'walks', 'flies', 'walks']}\n",
      " |      >>> df = pd.DataFrame(data=d)\n",
      " |      >>> df = df.set_index(['class', 'animal', 'locomotion'])\n",
      " |      >>> df\n",
      " |                                 num_legs  num_wings\n",
      " |      class  animal  locomotion\n",
      " |      mammal cat     walks              4          0\n",
      " |             dog     walks              4          0\n",
      " |             bat     flies              2          2\n",
      " |      bird   penguin walks              2          2\n",
      " |\n",
      " |      Get values at specified index\n",
      " |\n",
      " |      >>> df.xs('mammal')\n",
      " |                         num_legs  num_wings\n",
      " |      animal locomotion\n",
      " |      cat    walks              4          0\n",
      " |      dog    walks              4          0\n",
      " |      bat    flies              2          2\n",
      " |\n",
      " |      Get values at several indexes\n",
      " |\n",
      " |      >>> df.xs(('mammal', 'dog', 'walks'))\n",
      " |      num_legs     4\n",
      " |      num_wings    0\n",
      " |      Name: (mammal, dog, walks), dtype: int64\n",
      " |\n",
      " |      Get values at specified index and level\n",
      " |\n",
      " |      >>> df.xs('cat', level=1)\n",
      " |                         num_legs  num_wings\n",
      " |      class  locomotion\n",
      " |      mammal walks              4          0\n",
      " |\n",
      " |      Get values at several indexes and levels\n",
      " |\n",
      " |      >>> df.xs(('bird', 'walks'),\n",
      " |      ...       level=[0, 'locomotion'])\n",
      " |               num_legs  num_wings\n",
      " |      animal\n",
      " |      penguin         2          2\n",
      " |\n",
      " |      Get values at specified column and axis\n",
      " |\n",
      " |      >>> df.xs('num_wings', axis=1)\n",
      " |      class   animal   locomotion\n",
      " |      mammal  cat      walks         0\n",
      " |              dog      walks         0\n",
      " |              bat      flies         2\n",
      " |      bird    penguin  walks         2\n",
      " |      Name: num_wings, dtype: int64\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pandas.core.generic.NDFrame:\n",
      " |\n",
      " |  dtypes\n",
      " |      Return the dtypes in the DataFrame.\n",
      " |\n",
      " |      This returns a Series with the data type of each column.\n",
      " |      The result's index is the original DataFrame's columns. Columns\n",
      " |      with mixed types are stored with the ``object`` dtype. See\n",
      " |      :ref:`the User Guide <basics.dtypes>` for more.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      pandas.Series\n",
      " |          The data type of each column.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'float': [1.0],\n",
      " |      ...                    'int': [1],\n",
      " |      ...                    'datetime': [pd.Timestamp('20180310')],\n",
      " |      ...                    'string': ['foo']})\n",
      " |      >>> df.dtypes\n",
      " |      float              float64\n",
      " |      int                  int64\n",
      " |      datetime    datetime64[ns]\n",
      " |      string              object\n",
      " |      dtype: object\n",
      " |\n",
      " |  empty\n",
      " |      Indicator whether Series/DataFrame is empty.\n",
      " |\n",
      " |      True if Series/DataFrame is entirely empty (no items), meaning any of the\n",
      " |      axes are of length 0.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          If Series/DataFrame is empty, return True, if not return False.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.dropna : Return series without null values.\n",
      " |      DataFrame.dropna : Return DataFrame with labels on given axis omitted\n",
      " |          where (all or any) data are missing.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      If Series/DataFrame contains only NaNs, it is still not considered empty. See\n",
      " |      the example below.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      An example of an actual empty DataFrame. Notice the index is empty:\n",
      " |\n",
      " |      >>> df_empty = pd.DataFrame({'A' : []})\n",
      " |      >>> df_empty\n",
      " |      Empty DataFrame\n",
      " |      Columns: [A]\n",
      " |      Index: []\n",
      " |      >>> df_empty.empty\n",
      " |      True\n",
      " |\n",
      " |      If we only have NaNs in our DataFrame, it is not considered empty! We\n",
      " |      will need to drop the NaNs to make the DataFrame empty:\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'A' : [np.nan]})\n",
      " |      >>> df\n",
      " |          A\n",
      " |      0 NaN\n",
      " |      >>> df.empty\n",
      " |      False\n",
      " |      >>> df.dropna().empty\n",
      " |      True\n",
      " |\n",
      " |      >>> ser_empty = pd.Series({'A' : []})\n",
      " |      >>> ser_empty\n",
      " |      A    []\n",
      " |      dtype: object\n",
      " |      >>> ser_empty.empty\n",
      " |      False\n",
      " |      >>> ser_empty = pd.Series()\n",
      " |      >>> ser_empty.empty\n",
      " |      True\n",
      " |\n",
      " |  flags\n",
      " |      Get the properties associated with this pandas object.\n",
      " |\n",
      " |      The available flags are\n",
      " |\n",
      " |      * :attr:`Flags.allows_duplicate_labels`\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Flags : Flags that apply to pandas objects.\n",
      " |      DataFrame.attrs : Global metadata applying to this dataset.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      \"Flags\" differ from \"metadata\". Flags reflect properties of the\n",
      " |      pandas object (the Series or DataFrame). Metadata refer to properties\n",
      " |      of the dataset, and should be stored in :attr:`DataFrame.attrs`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({\"A\": [1, 2]})\n",
      " |      >>> df.flags\n",
      " |      <Flags(allows_duplicate_labels=True)>\n",
      " |\n",
      " |      Flags can be get or set using ``.``\n",
      " |\n",
      " |      >>> df.flags.allows_duplicate_labels\n",
      " |      True\n",
      " |      >>> df.flags.allows_duplicate_labels = False\n",
      " |\n",
      " |      Or by slicing with a key\n",
      " |\n",
      " |      >>> df.flags[\"allows_duplicate_labels\"]\n",
      " |      False\n",
      " |      >>> df.flags[\"allows_duplicate_labels\"] = True\n",
      " |\n",
      " |  ndim\n",
      " |      Return an int representing the number of axes / array dimensions.\n",
      " |\n",
      " |      Return 1 if Series. Otherwise return 2 if DataFrame.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      ndarray.ndim : Number of array dimensions.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> s = pd.Series({'a': 1, 'b': 2, 'c': 3})\n",
      " |      >>> s.ndim\n",
      " |      1\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n",
      " |      >>> df.ndim\n",
      " |      2\n",
      " |\n",
      " |  size\n",
      " |      Return an int representing the number of elements in this object.\n",
      " |\n",
      " |      Return the number of rows if Series. Otherwise return the number of\n",
      " |      rows times number of columns if DataFrame.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      ndarray.size : Number of elements in the array.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> s = pd.Series({'a': 1, 'b': 2, 'c': 3})\n",
      " |      >>> s.size\n",
      " |      3\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n",
      " |      >>> df.size\n",
      " |      4\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pandas.core.generic.NDFrame:\n",
      " |\n",
      " |  attrs\n",
      " |      Dictionary of global attributes of this dataset.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |         attrs is experimental and may change without warning.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.flags : Global flags applying to this object.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Many operations that create new datasets will copy ``attrs``. Copies\n",
      " |      are always deep so that changing ``attrs`` will only affect the\n",
      " |      present dataset. ``pandas.concat`` copies ``attrs`` only if all input\n",
      " |      datasets have the same ``attrs``.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      For Series:\n",
      " |\n",
      " |      >>> ser = pd.Series([1, 2, 3])\n",
      " |      >>> ser.attrs = {\"A\": [10, 20, 30]}\n",
      " |      >>> ser.attrs\n",
      " |      {'A': [10, 20, 30]}\n",
      " |\n",
      " |      For DataFrame:\n",
      " |\n",
      " |      >>> df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
      " |      >>> df.attrs = {\"A\": [10, 20, 30]}\n",
      " |      >>> df.attrs\n",
      " |      {'A': [10, 20, 30]}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pandas.core.generic.NDFrame:\n",
      " |\n",
      " |  __array_priority__ = 1000\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pandas.core.base.PandasObject:\n",
      " |\n",
      " |  __sizeof__(self) -> 'int'\n",
      " |      Generates the total memory usage for an object that returns\n",
      " |      either a value or Series of values\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pandas.core.accessor.DirNamesMixin:\n",
      " |\n",
      " |  __dir__(self) -> 'list[str]'\n",
      " |      Provide method name lookup and completion.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Only provide 'public' methods.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pandas.core.indexing.IndexingMixin:\n",
      " |\n",
      " |  at\n",
      " |      Access a single value for a row/column label pair.\n",
      " |\n",
      " |      Similar to ``loc``, in that both provide label-based lookups. Use\n",
      " |      ``at`` if you only need to get or set a single value in a DataFrame\n",
      " |      or Series.\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If getting a value and 'label' does not exist in a DataFrame or Series.\n",
      " |\n",
      " |      ValueError\n",
      " |          If row/column label pair is not a tuple or if any label\n",
      " |          from the pair is not a scalar for DataFrame.\n",
      " |          If label is list-like (*excluding* NamedTuple) for Series.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.at : Access a single value for a row/column pair by label.\n",
      " |      DataFrame.iat : Access a single value for a row/column pair by integer\n",
      " |          position.\n",
      " |      DataFrame.loc : Access a group of rows and columns by label(s).\n",
      " |      DataFrame.iloc : Access a group of rows and columns by integer\n",
      " |          position(s).\n",
      " |      Series.at : Access a single value by label.\n",
      " |      Series.iat : Access a single value by integer position.\n",
      " |      Series.loc : Access a group of rows by label(s).\n",
      " |      Series.iloc : Access a group of rows by integer position(s).\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      See :ref:`Fast scalar value getting and setting <indexing.basics.get_value>`\n",
      " |      for more details.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n",
      " |      ...                   index=[4, 5, 6], columns=['A', 'B', 'C'])\n",
      " |      >>> df\n",
      " |          A   B   C\n",
      " |      4   0   2   3\n",
      " |      5   0   4   1\n",
      " |      6  10  20  30\n",
      " |\n",
      " |      Get value at specified row/column pair\n",
      " |\n",
      " |      >>> df.at[4, 'B']\n",
      " |      2\n",
      " |\n",
      " |      Set value at specified row/column pair\n",
      " |\n",
      " |      >>> df.at[4, 'B'] = 10\n",
      " |      >>> df.at[4, 'B']\n",
      " |      10\n",
      " |\n",
      " |      Get value within a Series\n",
      " |\n",
      " |      >>> df.loc[5].at['B']\n",
      " |      4\n",
      " |\n",
      " |  iat\n",
      " |      Access a single value for a row/column pair by integer position.\n",
      " |\n",
      " |      Similar to ``iloc``, in that both provide integer-based lookups. Use\n",
      " |      ``iat`` if you only need to get or set a single value in a DataFrame\n",
      " |      or Series.\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      IndexError\n",
      " |          When integer position is out of bounds.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.at : Access a single value for a row/column label pair.\n",
      " |      DataFrame.loc : Access a group of rows and columns by label(s).\n",
      " |      DataFrame.iloc : Access a group of rows and columns by integer position(s).\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n",
      " |      ...                   columns=['A', 'B', 'C'])\n",
      " |      >>> df\n",
      " |          A   B   C\n",
      " |      0   0   2   3\n",
      " |      1   0   4   1\n",
      " |      2  10  20  30\n",
      " |\n",
      " |      Get value at specified row/column pair\n",
      " |\n",
      " |      >>> df.iat[1, 2]\n",
      " |      1\n",
      " |\n",
      " |      Set value at specified row/column pair\n",
      " |\n",
      " |      >>> df.iat[1, 2] = 10\n",
      " |      >>> df.iat[1, 2]\n",
      " |      10\n",
      " |\n",
      " |      Get value within a series\n",
      " |\n",
      " |      >>> df.loc[0].iat[1]\n",
      " |      2\n",
      " |\n",
      " |  iloc\n",
      " |      Purely integer-location based indexing for selection by position.\n",
      " |\n",
      " |      .. deprecated:: 2.2.0\n",
      " |\n",
      " |         Returning a tuple from a callable is deprecated.\n",
      " |\n",
      " |      ``.iloc[]`` is primarily integer position based (from ``0`` to\n",
      " |      ``length-1`` of the axis), but may also be used with a boolean\n",
      " |      array.\n",
      " |\n",
      " |      Allowed inputs are:\n",
      " |\n",
      " |      - An integer, e.g. ``5``.\n",
      " |      - A list or array of integers, e.g. ``[4, 3, 0]``.\n",
      " |      - A slice object with ints, e.g. ``1:7``.\n",
      " |      - A boolean array.\n",
      " |      - A ``callable`` function with one argument (the calling Series or\n",
      " |        DataFrame) and that returns valid output for indexing (one of the above).\n",
      " |        This is useful in method chains, when you don't have a reference to the\n",
      " |        calling object, but would like to base your selection on\n",
      " |        some value.\n",
      " |      - A tuple of row and column indexes. The tuple elements consist of one of the\n",
      " |        above inputs, e.g. ``(0, 1)``.\n",
      " |\n",
      " |      ``.iloc`` will raise ``IndexError`` if a requested indexer is\n",
      " |      out-of-bounds, except *slice* indexers which allow out-of-bounds\n",
      " |      indexing (this conforms with python/numpy *slice* semantics).\n",
      " |\n",
      " |      See more at :ref:`Selection by Position <indexing.integer>`.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.iat : Fast integer location scalar accessor.\n",
      " |      DataFrame.loc : Purely label-location based indexer for selection by label.\n",
      " |      Series.iloc : Purely integer-location based indexing for\n",
      " |                     selection by position.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> mydict = [{'a': 1, 'b': 2, 'c': 3, 'd': 4},\n",
      " |      ...           {'a': 100, 'b': 200, 'c': 300, 'd': 400},\n",
      " |      ...           {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000}]\n",
      " |      >>> df = pd.DataFrame(mydict)\n",
      " |      >>> df\n",
      " |            a     b     c     d\n",
      " |      0     1     2     3     4\n",
      " |      1   100   200   300   400\n",
      " |      2  1000  2000  3000  4000\n",
      " |\n",
      " |      **Indexing just the rows**\n",
      " |\n",
      " |      With a scalar integer.\n",
      " |\n",
      " |      >>> type(df.iloc[0])\n",
      " |      <class 'pandas.core.series.Series'>\n",
      " |      >>> df.iloc[0]\n",
      " |      a    1\n",
      " |      b    2\n",
      " |      c    3\n",
      " |      d    4\n",
      " |      Name: 0, dtype: int64\n",
      " |\n",
      " |      With a list of integers.\n",
      " |\n",
      " |      >>> df.iloc[[0]]\n",
      " |         a  b  c  d\n",
      " |      0  1  2  3  4\n",
      " |      >>> type(df.iloc[[0]])\n",
      " |      <class 'pandas.core.frame.DataFrame'>\n",
      " |\n",
      " |      >>> df.iloc[[0, 1]]\n",
      " |           a    b    c    d\n",
      " |      0    1    2    3    4\n",
      " |      1  100  200  300  400\n",
      " |\n",
      " |      With a `slice` object.\n",
      " |\n",
      " |      >>> df.iloc[:3]\n",
      " |            a     b     c     d\n",
      " |      0     1     2     3     4\n",
      " |      1   100   200   300   400\n",
      " |      2  1000  2000  3000  4000\n",
      " |\n",
      " |      With a boolean mask the same length as the index.\n",
      " |\n",
      " |      >>> df.iloc[[True, False, True]]\n",
      " |            a     b     c     d\n",
      " |      0     1     2     3     4\n",
      " |      2  1000  2000  3000  4000\n",
      " |\n",
      " |      With a callable, useful in method chains. The `x` passed\n",
      " |      to the ``lambda`` is the DataFrame being sliced. This selects\n",
      " |      the rows whose index label even.\n",
      " |\n",
      " |      >>> df.iloc[lambda x: x.index % 2 == 0]\n",
      " |            a     b     c     d\n",
      " |      0     1     2     3     4\n",
      " |      2  1000  2000  3000  4000\n",
      " |\n",
      " |      **Indexing both axes**\n",
      " |\n",
      " |      You can mix the indexer types for the index and columns. Use ``:`` to\n",
      " |      select the entire axis.\n",
      " |\n",
      " |      With scalar integers.\n",
      " |\n",
      " |      >>> df.iloc[0, 1]\n",
      " |      2\n",
      " |\n",
      " |      With lists of integers.\n",
      " |\n",
      " |      >>> df.iloc[[0, 2], [1, 3]]\n",
      " |            b     d\n",
      " |      0     2     4\n",
      " |      2  2000  4000\n",
      " |\n",
      " |      With `slice` objects.\n",
      " |\n",
      " |      >>> df.iloc[1:3, 0:3]\n",
      " |            a     b     c\n",
      " |      1   100   200   300\n",
      " |      2  1000  2000  3000\n",
      " |\n",
      " |      With a boolean array whose length matches the columns.\n",
      " |\n",
      " |      >>> df.iloc[:, [True, False, True, False]]\n",
      " |            a     c\n",
      " |      0     1     3\n",
      " |      1   100   300\n",
      " |      2  1000  3000\n",
      " |\n",
      " |      With a callable function that expects the Series or DataFrame.\n",
      " |\n",
      " |      >>> df.iloc[:, lambda df: [0, 2]]\n",
      " |            a     c\n",
      " |      0     1     3\n",
      " |      1   100   300\n",
      " |      2  1000  3000\n",
      " |\n",
      " |  loc\n",
      " |      Access a group of rows and columns by label(s) or a boolean array.\n",
      " |\n",
      " |      ``.loc[]`` is primarily label based, but may also be used with a\n",
      " |      boolean array.\n",
      " |\n",
      " |      Allowed inputs are:\n",
      " |\n",
      " |      - A single label, e.g. ``5`` or ``'a'``, (note that ``5`` is\n",
      " |        interpreted as a *label* of the index, and **never** as an\n",
      " |        integer position along the index).\n",
      " |      - A list or array of labels, e.g. ``['a', 'b', 'c']``.\n",
      " |      - A slice object with labels, e.g. ``'a':'f'``.\n",
      " |\n",
      " |        .. warning:: Note that contrary to usual python slices, **both** the\n",
      " |            start and the stop are included\n",
      " |\n",
      " |      - A boolean array of the same length as the axis being sliced,\n",
      " |        e.g. ``[True, False, True]``.\n",
      " |      - An alignable boolean Series. The index of the key will be aligned before\n",
      " |        masking.\n",
      " |      - An alignable Index. The Index of the returned selection will be the input.\n",
      " |      - A ``callable`` function with one argument (the calling Series or\n",
      " |        DataFrame) and that returns valid output for indexing (one of the above)\n",
      " |\n",
      " |      See more at :ref:`Selection by Label <indexing.label>`.\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If any items are not found.\n",
      " |      IndexingError\n",
      " |          If an indexed key is passed and its index is unalignable to the frame index.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.at : Access a single value for a row/column label pair.\n",
      " |      DataFrame.iloc : Access group of rows and columns by integer position(s).\n",
      " |      DataFrame.xs : Returns a cross-section (row(s) or column(s)) from the\n",
      " |                     Series/DataFrame.\n",
      " |      Series.loc : Access group of values using labels.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      **Getting values**\n",
      " |\n",
      " |      >>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n",
      " |      ...                   index=['cobra', 'viper', 'sidewinder'],\n",
      " |      ...                   columns=['max_speed', 'shield'])\n",
      " |      >>> df\n",
      " |                  max_speed  shield\n",
      " |      cobra               1       2\n",
      " |      viper               4       5\n",
      " |      sidewinder          7       8\n",
      " |\n",
      " |      Single label. Note this returns the row as a Series.\n",
      " |\n",
      " |      >>> df.loc['viper']\n",
      " |      max_speed    4\n",
      " |      shield       5\n",
      " |      Name: viper, dtype: int64\n",
      " |\n",
      " |      List of labels. Note using ``[[]]`` returns a DataFrame.\n",
      " |\n",
      " |      >>> df.loc[['viper', 'sidewinder']]\n",
      " |                  max_speed  shield\n",
      " |      viper               4       5\n",
      " |      sidewinder          7       8\n",
      " |\n",
      " |      Single label for row and column\n",
      " |\n",
      " |      >>> df.loc['cobra', 'shield']\n",
      " |      2\n",
      " |\n",
      " |      Slice with labels for row and single label for column. As mentioned\n",
      " |      above, note that both the start and stop of the slice are included.\n",
      " |\n",
      " |      >>> df.loc['cobra':'viper', 'max_speed']\n",
      " |      cobra    1\n",
      " |      viper    4\n",
      " |      Name: max_speed, dtype: int64\n",
      " |\n",
      " |      Boolean list with the same length as the row axis\n",
      " |\n",
      " |      >>> df.loc[[False, False, True]]\n",
      " |                  max_speed  shield\n",
      " |      sidewinder          7       8\n",
      " |\n",
      " |      Alignable boolean Series:\n",
      " |\n",
      " |      >>> df.loc[pd.Series([False, True, False],\n",
      " |      ...                  index=['viper', 'sidewinder', 'cobra'])]\n",
      " |                           max_speed  shield\n",
      " |      sidewinder          7       8\n",
      " |\n",
      " |      Index (same behavior as ``df.reindex``)\n",
      " |\n",
      " |      >>> df.loc[pd.Index([\"cobra\", \"viper\"], name=\"foo\")]\n",
      " |             max_speed  shield\n",
      " |      foo\n",
      " |      cobra          1       2\n",
      " |      viper          4       5\n",
      " |\n",
      " |      Conditional that returns a boolean Series\n",
      " |\n",
      " |      >>> df.loc[df['shield'] > 6]\n",
      " |                  max_speed  shield\n",
      " |      sidewinder          7       8\n",
      " |\n",
      " |      Conditional that returns a boolean Series with column labels specified\n",
      " |\n",
      " |      >>> df.loc[df['shield'] > 6, ['max_speed']]\n",
      " |                  max_speed\n",
      " |      sidewinder          7\n",
      " |\n",
      " |      Multiple conditional using ``&`` that returns a boolean Series\n",
      " |\n",
      " |      >>> df.loc[(df['max_speed'] > 1) & (df['shield'] < 8)]\n",
      " |                  max_speed  shield\n",
      " |      viper          4       5\n",
      " |\n",
      " |      Multiple conditional using ``|`` that returns a boolean Series\n",
      " |\n",
      " |      >>> df.loc[(df['max_speed'] > 4) | (df['shield'] < 5)]\n",
      " |                  max_speed  shield\n",
      " |      cobra               1       2\n",
      " |      sidewinder          7       8\n",
      " |\n",
      " |      Please ensure that each condition is wrapped in parentheses ``()``.\n",
      " |      See the :ref:`user guide<indexing.boolean>`\n",
      " |      for more details and explanations of Boolean indexing.\n",
      " |\n",
      " |      .. note::\n",
      " |          If you find yourself using 3 or more conditionals in ``.loc[]``,\n",
      " |          consider using :ref:`advanced indexing<advanced.advanced_hierarchical>`.\n",
      " |\n",
      " |          See below for using ``.loc[]`` on MultiIndex DataFrames.\n",
      " |\n",
      " |      Callable that returns a boolean Series\n",
      " |\n",
      " |      >>> df.loc[lambda df: df['shield'] == 8]\n",
      " |                  max_speed  shield\n",
      " |      sidewinder          7       8\n",
      " |\n",
      " |      **Setting values**\n",
      " |\n",
      " |      Set value for all items matching the list of labels\n",
      " |\n",
      " |      >>> df.loc[['viper', 'sidewinder'], ['shield']] = 50\n",
      " |      >>> df\n",
      " |                  max_speed  shield\n",
      " |      cobra               1       2\n",
      " |      viper               4      50\n",
      " |      sidewinder          7      50\n",
      " |\n",
      " |      Set value for an entire row\n",
      " |\n",
      " |      >>> df.loc['cobra'] = 10\n",
      " |      >>> df\n",
      " |                  max_speed  shield\n",
      " |      cobra              10      10\n",
      " |      viper               4      50\n",
      " |      sidewinder          7      50\n",
      " |\n",
      " |      Set value for an entire column\n",
      " |\n",
      " |      >>> df.loc[:, 'max_speed'] = 30\n",
      " |      >>> df\n",
      " |                  max_speed  shield\n",
      " |      cobra              30      10\n",
      " |      viper              30      50\n",
      " |      sidewinder         30      50\n",
      " |\n",
      " |      Set value for rows matching callable condition\n",
      " |\n",
      " |      >>> df.loc[df['shield'] > 35] = 0\n",
      " |      >>> df\n",
      " |                  max_speed  shield\n",
      " |      cobra              30      10\n",
      " |      viper               0       0\n",
      " |      sidewinder          0       0\n",
      " |\n",
      " |      Add value matching location\n",
      " |\n",
      " |      >>> df.loc[\"viper\", \"shield\"] += 5\n",
      " |      >>> df\n",
      " |                  max_speed  shield\n",
      " |      cobra              30      10\n",
      " |      viper               0       5\n",
      " |      sidewinder          0       0\n",
      " |\n",
      " |      Setting using a ``Series`` or a ``DataFrame`` sets the values matching the\n",
      " |      index labels, not the index positions.\n",
      " |\n",
      " |      >>> shuffled_df = df.loc[[\"viper\", \"cobra\", \"sidewinder\"]]\n",
      " |      >>> df.loc[:] += shuffled_df\n",
      " |      >>> df\n",
      " |                  max_speed  shield\n",
      " |      cobra              60      20\n",
      " |      viper               0      10\n",
      " |      sidewinder          0       0\n",
      " |\n",
      " |      **Getting values on a DataFrame with an index that has integer labels**\n",
      " |\n",
      " |      Another example using integers for the index\n",
      " |\n",
      " |      >>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n",
      " |      ...                   index=[7, 8, 9], columns=['max_speed', 'shield'])\n",
      " |      >>> df\n",
      " |         max_speed  shield\n",
      " |      7          1       2\n",
      " |      8          4       5\n",
      " |      9          7       8\n",
      " |\n",
      " |      Slice with integer labels for rows. As mentioned above, note that both\n",
      " |      the start and stop of the slice are included.\n",
      " |\n",
      " |      >>> df.loc[7:9]\n",
      " |         max_speed  shield\n",
      " |      7          1       2\n",
      " |      8          4       5\n",
      " |      9          7       8\n",
      " |\n",
      " |      **Getting values with a MultiIndex**\n",
      " |\n",
      " |      A number of examples using a DataFrame with a MultiIndex\n",
      " |\n",
      " |      >>> tuples = [\n",
      " |      ...     ('cobra', 'mark i'), ('cobra', 'mark ii'),\n",
      " |      ...     ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'),\n",
      " |      ...     ('viper', 'mark ii'), ('viper', 'mark iii')\n",
      " |      ... ]\n",
      " |      >>> index = pd.MultiIndex.from_tuples(tuples)\n",
      " |      >>> values = [[12, 2], [0, 4], [10, 20],\n",
      " |      ...           [1, 4], [7, 1], [16, 36]]\n",
      " |      >>> df = pd.DataFrame(values, columns=['max_speed', 'shield'], index=index)\n",
      " |      >>> df\n",
      " |                           max_speed  shield\n",
      " |      cobra      mark i           12       2\n",
      " |                 mark ii           0       4\n",
      " |      sidewinder mark i           10      20\n",
      " |                 mark ii           1       4\n",
      " |      viper      mark ii           7       1\n",
      " |                 mark iii         16      36\n",
      " |\n",
      " |      Single label. Note this returns a DataFrame with a single index.\n",
      " |\n",
      " |      >>> df.loc['cobra']\n",
      " |               max_speed  shield\n",
      " |      mark i          12       2\n",
      " |      mark ii          0       4\n",
      " |\n",
      " |      Single index tuple. Note this returns a Series.\n",
      " |\n",
      " |      >>> df.loc[('cobra', 'mark ii')]\n",
      " |      max_speed    0\n",
      " |      shield       4\n",
      " |      Name: (cobra, mark ii), dtype: int64\n",
      " |\n",
      " |      Single label for row and column. Similar to passing in a tuple, this\n",
      " |      returns a Series.\n",
      " |\n",
      " |      >>> df.loc['cobra', 'mark i']\n",
      " |      max_speed    12\n",
      " |      shield        2\n",
      " |      Name: (cobra, mark i), dtype: int64\n",
      " |\n",
      " |      Single tuple. Note using ``[[]]`` returns a DataFrame.\n",
      " |\n",
      " |      >>> df.loc[[('cobra', 'mark ii')]]\n",
      " |                     max_speed  shield\n",
      " |      cobra mark ii          0       4\n",
      " |\n",
      " |      Single tuple for the index with a single label for the column\n",
      " |\n",
      " |      >>> df.loc[('cobra', 'mark i'), 'shield']\n",
      " |      2\n",
      " |\n",
      " |      Slice from index tuple to single label\n",
      " |\n",
      " |      >>> df.loc[('cobra', 'mark i'):'viper']\n",
      " |                           max_speed  shield\n",
      " |      cobra      mark i           12       2\n",
      " |                 mark ii           0       4\n",
      " |      sidewinder mark i           10      20\n",
      " |                 mark ii           1       4\n",
      " |      viper      mark ii           7       1\n",
      " |                 mark iii         16      36\n",
      " |\n",
      " |      Slice from index tuple to index tuple\n",
      " |\n",
      " |      >>> df.loc[('cobra', 'mark i'):('viper', 'mark ii')]\n",
      " |                          max_speed  shield\n",
      " |      cobra      mark i          12       2\n",
      " |                 mark ii          0       4\n",
      " |      sidewinder mark i          10      20\n",
      " |                 mark ii          1       4\n",
      " |      viper      mark ii          7       1\n",
      " |\n",
      " |      Please see the :ref:`user guide<advanced.advanced_hierarchical>`\n",
      " |      for more details and explanations of advanced indexing.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pandas.core.arraylike.OpsMixin:\n",
      " |\n",
      " |  __add__(self, other)\n",
      " |      Get Addition of DataFrame and other, column-wise.\n",
      " |\n",
      " |      Equivalent to ``DataFrame.add(other)``.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, dict or DataFrame\n",
      " |          Object to be added to the DataFrame.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          The result of adding ``other`` to DataFrame.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add a DataFrame and another object, with option for index-\n",
      " |          or column-oriented addition.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'height': [1.5, 2.6], 'weight': [500, 800]},\n",
      " |      ...                   index=['elk', 'moose'])\n",
      " |      >>> df\n",
      " |             height  weight\n",
      " |      elk       1.5     500\n",
      " |      moose     2.6     800\n",
      " |\n",
      " |      Adding a scalar affects all rows and columns.\n",
      " |\n",
      " |      >>> df[['height', 'weight']] + 1.5\n",
      " |             height  weight\n",
      " |      elk       3.0   501.5\n",
      " |      moose     4.1   801.5\n",
      " |\n",
      " |      Each element of a list is added to a column of the DataFrame, in order.\n",
      " |\n",
      " |      >>> df[['height', 'weight']] + [0.5, 1.5]\n",
      " |             height  weight\n",
      " |      elk       2.0   501.5\n",
      " |      moose     3.1   801.5\n",
      " |\n",
      " |      Keys of a dictionary are aligned to the DataFrame, based on column names;\n",
      " |      each value in the dictionary is added to the corresponding column.\n",
      " |\n",
      " |      >>> df[['height', 'weight']] + {'height': 0.5, 'weight': 1.5}\n",
      " |             height  weight\n",
      " |      elk       2.0   501.5\n",
      " |      moose     3.1   801.5\n",
      " |\n",
      " |      When `other` is a :class:`Series`, the index of `other` is aligned with the\n",
      " |      columns of the DataFrame.\n",
      " |\n",
      " |      >>> s1 = pd.Series([0.5, 1.5], index=['weight', 'height'])\n",
      " |      >>> df[['height', 'weight']] + s1\n",
      " |             height  weight\n",
      " |      elk       3.0   500.5\n",
      " |      moose     4.1   800.5\n",
      " |\n",
      " |      Even when the index of `other` is the same as the index of the DataFrame,\n",
      " |      the :class:`Series` will not be reoriented. If index-wise alignment is desired,\n",
      " |      :meth:`DataFrame.add` should be used with `axis='index'`.\n",
      " |\n",
      " |      >>> s2 = pd.Series([0.5, 1.5], index=['elk', 'moose'])\n",
      " |      >>> df[['height', 'weight']] + s2\n",
      " |             elk  height  moose  weight\n",
      " |      elk    NaN     NaN    NaN     NaN\n",
      " |      moose  NaN     NaN    NaN     NaN\n",
      " |\n",
      " |      >>> df[['height', 'weight']].add(s2, axis='index')\n",
      " |             height  weight\n",
      " |      elk       2.0   500.5\n",
      " |      moose     4.1   801.5\n",
      " |\n",
      " |      When `other` is a :class:`DataFrame`, both columns names and the\n",
      " |      index are aligned.\n",
      " |\n",
      " |      >>> other = pd.DataFrame({'height': [0.2, 0.4, 0.6]},\n",
      " |      ...                      index=['elk', 'moose', 'deer'])\n",
      " |      >>> df[['height', 'weight']] + other\n",
      " |             height  weight\n",
      " |      deer      NaN     NaN\n",
      " |      elk       1.7     NaN\n",
      " |      moose     3.0     NaN\n",
      " |\n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __floordiv__(self, other)\n",
      " |\n",
      " |  __ge__(self, other)\n",
      " |      Return self>=value.\n",
      " |\n",
      " |  __gt__(self, other)\n",
      " |      Return self>value.\n",
      " |\n",
      " |  __le__(self, other)\n",
      " |      Return self<=value.\n",
      " |\n",
      " |  __lt__(self, other)\n",
      " |      Return self<value.\n",
      " |\n",
      " |  __mod__(self, other)\n",
      " |\n",
      " |  __mul__(self, other)\n",
      " |\n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      " |\n",
      " |  __pow__(self, other)\n",
      " |\n",
      " |  __radd__(self, other)\n",
      " |\n",
      " |  __rand__(self, other)\n",
      " |\n",
      " |  __rfloordiv__(self, other)\n",
      " |\n",
      " |  __rmod__(self, other)\n",
      " |\n",
      " |  __rmul__(self, other)\n",
      " |\n",
      " |  __ror__(self, other)\n",
      " |      Return value|self.\n",
      " |\n",
      " |  __rpow__(self, other)\n",
      " |\n",
      " |  __rsub__(self, other)\n",
      " |\n",
      " |  __rtruediv__(self, other)\n",
      " |\n",
      " |  __rxor__(self, other)\n",
      " |\n",
      " |  __truediv__(self, other)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pandas.core.arraylike.OpsMixin:\n",
      " |\n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(shaanxi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制陕西省的边界图\n",
    "shaanxi.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 获取西安市的GeoDataFrame\n",
    "xian = ox.geocode_to_gdf(query='Xi\\'an, Shaanxi, China')\n",
    "# 绘制西安市的边界图\n",
    "xian.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: folium in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.17.0)\n",
      "Requirement already satisfied: matplotlib in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: mapclassify in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.8.1)\n",
      "Requirement already satisfied: branca in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: jinja2>=2.9 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from folium) (3.1.4)\n",
      "Requirement already satisfied: numpy in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from folium) (1.26.4)\n",
      "Requirement already satisfied: requests in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from folium) (2.32.3)\n",
      "Requirement already satisfied: xyzservices in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from folium) (2024.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: networkx>=2.7 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mapclassify) (3.3)\n",
      "Requirement already satisfied: pandas!=1.5.0,>=1.4 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mapclassify) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mapclassify) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.8 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mapclassify) (1.14.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2>=2.9->folium) (3.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas!=1.5.0,>=1.4->mapclassify) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas!=1.5.0,>=1.4->mapclassify) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=1.0->mapclassify) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=1.0->mapclassify) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->folium) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->folium) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->folium) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->folium) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install folium matplotlib mapclassify branca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap-glyphicons.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_615e738a475963454fcca2f5aba9c304 {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "    \n",
       "                    &lt;style&gt;\n",
       "                        .foliumtooltip {\n",
       "                            \n",
       "                        }\n",
       "                       .foliumtooltip table{\n",
       "                            margin: auto;\n",
       "                        }\n",
       "                        .foliumtooltip tr{\n",
       "                            text-align: left;\n",
       "                        }\n",
       "                        .foliumtooltip th{\n",
       "                            padding: 2px; padding-right: 8px;\n",
       "                        }\n",
       "                    &lt;/style&gt;\n",
       "            \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_615e738a475963454fcca2f5aba9c304&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_615e738a475963454fcca2f5aba9c304 = L.map(\n",
       "                &quot;map_615e738a475963454fcca2f5aba9c304&quot;,\n",
       "                {\n",
       "                    center: [34.2205857, 108.72352975000001],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    zoom: 10,\n",
       "                    zoomControl: true,\n",
       "                    preferCanvas: false,\n",
       "                }\n",
       "            );\n",
       "            L.control.scale().addTo(map_615e738a475963454fcca2f5aba9c304);\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_b3526ac1387ee4a29f2ee8038fa77f8e = L.tileLayer(\n",
       "                &quot;https://tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
       "                {&quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 19, &quot;maxZoom&quot;: 19, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
       "            );\n",
       "        \n",
       "    \n",
       "            tile_layer_b3526ac1387ee4a29f2ee8038fa77f8e.addTo(map_615e738a475963454fcca2f5aba9c304);\n",
       "        \n",
       "    \n",
       "            map_615e738a475963454fcca2f5aba9c304.fitBounds(\n",
       "                [[33.695219, 107.655623], [34.7459524, 109.7914365]],\n",
       "                {}\n",
       "            );\n",
       "        \n",
       "    \n",
       "        function geo_json_88104525c6835467cb6a87f4392d65d8_styler(feature) {\n",
       "            switch(feature.id) {\n",
       "                default:\n",
       "                    return {&quot;fillOpacity&quot;: 0.5, &quot;weight&quot;: 2};\n",
       "            }\n",
       "        }\n",
       "        function geo_json_88104525c6835467cb6a87f4392d65d8_highlighter(feature) {\n",
       "            switch(feature.id) {\n",
       "                default:\n",
       "                    return {&quot;fillOpacity&quot;: 0.75};\n",
       "            }\n",
       "        }\n",
       "        function geo_json_88104525c6835467cb6a87f4392d65d8_pointToLayer(feature, latlng) {\n",
       "            var opts = {&quot;bubblingMouseEvents&quot;: true, &quot;color&quot;: &quot;#3388ff&quot;, &quot;dashArray&quot;: null, &quot;dashOffset&quot;: null, &quot;fill&quot;: true, &quot;fillColor&quot;: &quot;#3388ff&quot;, &quot;fillOpacity&quot;: 0.2, &quot;fillRule&quot;: &quot;evenodd&quot;, &quot;lineCap&quot;: &quot;round&quot;, &quot;lineJoin&quot;: &quot;round&quot;, &quot;opacity&quot;: 1.0, &quot;radius&quot;: 2, &quot;stroke&quot;: true, &quot;weight&quot;: 3};\n",
       "            \n",
       "            let style = geo_json_88104525c6835467cb6a87f4392d65d8_styler(feature)\n",
       "            Object.assign(opts, style)\n",
       "            \n",
       "            return new L.CircleMarker(latlng, opts)\n",
       "        }\n",
       "\n",
       "        function geo_json_88104525c6835467cb6a87f4392d65d8_onEachFeature(feature, layer) {\n",
       "            layer.on({\n",
       "                mouseout: function(e) {\n",
       "                    if(typeof e.target.setStyle === &quot;function&quot;){\n",
       "                            geo_json_88104525c6835467cb6a87f4392d65d8.resetStyle(e.target);\n",
       "                    }\n",
       "                },\n",
       "                mouseover: function(e) {\n",
       "                    if(typeof e.target.setStyle === &quot;function&quot;){\n",
       "                        const highlightStyle = geo_json_88104525c6835467cb6a87f4392d65d8_highlighter(e.target.feature)\n",
       "                        e.target.setStyle(highlightStyle);\n",
       "                    }\n",
       "                },\n",
       "            });\n",
       "        };\n",
       "        var geo_json_88104525c6835467cb6a87f4392d65d8 = L.geoJson(null, {\n",
       "                onEachFeature: geo_json_88104525c6835467cb6a87f4392d65d8_onEachFeature,\n",
       "            \n",
       "                style: geo_json_88104525c6835467cb6a87f4392d65d8_styler,\n",
       "                pointToLayer: geo_json_88104525c6835467cb6a87f4392d65d8_pointToLayer,\n",
       "        });\n",
       "\n",
       "        function geo_json_88104525c6835467cb6a87f4392d65d8_add (data) {\n",
       "            geo_json_88104525c6835467cb6a87f4392d65d8\n",
       "                .addData(data);\n",
       "        }\n",
       "            geo_json_88104525c6835467cb6a87f4392d65d8_add({&quot;bbox&quot;: [107.655623, 33.695219, 109.7914365, 34.7459524], &quot;features&quot;: [{&quot;bbox&quot;: [107.655623, 33.695219, 109.7914365, 34.7459524], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[[107.655623, 33.779532], [107.6588157, 33.7702437], [107.6667973, 33.7658203], [107.6811643, 33.7587423], [107.6795679, 33.7463546], [107.6880817, 33.7352926], [107.6916198, 33.7237279], [107.6955422, 33.7254049], [107.7012442, 33.7269163], [107.7075903, 33.7276302], [107.710185, 33.727122], [107.7170011, 33.7258285], [107.719775, 33.7242442], [107.7239875, 33.7220009], [107.728757, 33.7203704], [107.7310127, 33.7182239], [107.7366119, 33.7192478], [107.7432095, 33.7181587], [107.7470929, 33.7162451], [107.7547171, 33.71793], [107.7571816, 33.7212654], [107.7616814, 33.7217328], [107.76329, 33.7195223], [107.7684777, 33.7202137], [107.7703254, 33.7171535], [107.7744686, 33.7167158], [107.7800981, 33.7173981], [107.7835277, 33.7144715], [107.7869418, 33.712971], [107.7901437, 33.7120292], [107.7929054, 33.7105093], [107.7975249, 33.7097103], [107.8024837, 33.7111883], [107.8086856, 33.7141777], [107.8119049, 33.7095733], [107.8171495, 33.7062952], [107.8195078, 33.7022261], [107.8237493, 33.7014271], [107.8272069, 33.6969969], [107.8350534, 33.6969864], [107.8404615, 33.6996294], [107.8418468, 33.6989751], [107.8450256, 33.695219], [107.8476973, 33.6952599], [107.8514209, 33.6974881], [107.8547133, 33.6979714], [107.8591333, 33.7009049], [107.8582514, 33.7041137], [107.8586943, 33.7059956], [107.8620467, 33.709932], [107.8644511, 33.7127146], [107.8651742, 33.7171659], [107.8687547, 33.7186947], [107.8727356, 33.7181516], [107.8737515, 33.7193092], [107.8746262, 33.7211649], [107.8762782, 33.7230752], [107.8779564, 33.725179], [107.8821399, 33.7295546], [107.8858535, 33.7348954], [107.8843681, 33.7402025], [107.8864074, 33.7412413], [107.8862237, 33.7445195], [107.8902808, 33.7462698], [107.8917904, 33.7500356], [107.897262, 33.7503689], [107.900974, 33.7431316], [107.9059833, 33.7423724], [107.9092258, 33.7393257], [107.9091867, 33.737136], [107.9130966, 33.7358874], [107.9145854, 33.7341159], [107.9184308, 33.7337521], [107.9220685, 33.7344331], [107.9247818, 33.7322484], [107.9272996, 33.732675], [107.929152, 33.7340587], [107.9309732, 33.7373331], [107.9314091, 33.7394316], [107.935246, 33.7394662], [107.9413567, 33.7442823], [107.9476379, 33.7474551], [107.9497855, 33.7437593], [107.9525094, 33.7424135], [107.9555663, 33.7408144], [107.9622564, 33.7359704], [107.9653468, 33.7367324], [107.9712672, 33.7370226], [107.973934, 33.7354488], [107.977444, 33.7360365], [107.979936, 33.7352665], [107.9824208, 33.7331541], [107.9890983, 33.7329903], [107.9937551, 33.7308783], [107.9955372, 33.7283085], [107.9999998, 33.7289541], [108.0047875, 33.7288098], [108.0084166, 33.7267441], [108.017706, 33.7245961], [108.0211655, 33.7230438], [108.0264886, 33.7183405], [108.0370473, 33.7138784], [108.0453821, 33.7131994], [108.0512322, 33.7112819], [108.0630418, 33.7238234], [108.0707703, 33.7282599], [108.0799788, 33.7309149], [108.0887908, 33.7308044], [108.0990828, 33.7289199], [108.1079713, 33.7226219], [108.1175008, 33.7195589], [108.1292573, 33.7207069], [108.1360398, 33.7225799], [108.1477128, 33.7207449], [108.1581268, 33.7163354], [108.1653213, 33.7156984], [108.1768188, 33.7168349], [108.1851728, 33.7199094], [108.2005233, 33.7260049], [108.2137528, 33.7306404], [108.2241518, 33.7317314], [108.2341233, 33.7355504], [108.2401733, 33.7406004], [108.2515188, 33.7495154], [108.2585828, 33.7562029], [108.2701263, 33.7614594], [108.2898023, 33.7642669], [108.3007433, 33.7658269], [108.3085173, 33.7695579], [108.3178328, 33.7754134], [108.3272323, 33.7849349], [108.3314213, 33.7889899], [108.3373413, 33.7915039], [108.3458023, 33.7929649], [108.3526688, 33.7932244], [108.3574068, 33.7922554], [108.3661391, 33.7941872], [108.3692124, 33.7911189], [108.3711698, 33.7872659], [108.3713602, 33.7833837], [108.3730605, 33.7809291], [108.3872305, 33.7784559], [108.3908918, 33.7685164], [108.4008638, 33.7672804], [108.4127508, 33.7716179], [108.4299163, 33.7800409], [108.4439623, 33.7906454], [108.4558103, 33.8009339], [108.4654923, 33.8054124], [108.4786073, 33.8074914], [108.4871673, 33.8068884], [108.4994428, 33.8041229], [108.5097578, 33.8015174], [108.5176388, 33.7983629], [108.5309068, 33.7919614], [108.5397873, 33.7854044], [108.5505373, 33.7743299], [108.5552673, 33.7678534], [108.5609438, 33.7648469], [108.5699768, 33.7656289], [108.5757673, 33.7708739], [108.5877683, 33.7839089], [108.5941468, 33.7937624], [108.6015093, 33.8061714], [108.6040268, 33.8161124], [108.6097718, 33.8225059], [108.6295168, 33.8351174], [108.6418993, 33.8408239], [108.6537018, 33.8416939], [108.6648633, 33.8388749], [108.6746978, 33.8350904], [108.6819303, 33.8337364], [108.6906623, 33.8312161], [108.6919059, 33.8292727], [108.6935526, 33.8280217], [108.6996447, 33.8273066], [108.7033727, 33.8281454], [108.7077026, 33.8314505], [108.7145868, 33.8313669], [108.7171638, 33.8341887], [108.7215572, 33.835971], [108.7287762, 33.8383706], [108.7337794, 33.8415008], [108.736647, 33.8408527], [108.7406843, 33.8376159], [108.7468, 33.8400266], [108.7519601, 33.8391965], [108.7534769, 33.8324373], [108.7516565, 33.8275492], [108.7519541, 33.8251392], [108.7497285, 33.8216359], [108.7481863, 33.8190671], [108.7502807, 33.8156013], [108.7520964, 33.8115398], [108.7539366, 33.8105002], [108.7637311, 33.8046459], [108.7647023, 33.8029774], [108.7678055, 33.8027208], [108.7721234, 33.7993364], [108.7735077, 33.7989582], [108.7729817, 33.8019515], [108.7749863, 33.8064499], [108.7801669, 33.8095694], [108.7841673, 33.8094705], [108.78815, 33.8074265], [108.7918706, 33.8106864], [108.8015039, 33.8151154], [108.8009388, 33.8172296], [108.8051879, 33.8167443], [108.8059552, 33.8148235], [108.809235, 33.8150181], [108.813452, 33.8187163], [108.8194883, 33.8203239], [108.8258892, 33.8209885], [108.8292085, 33.8166901], [108.8328325, 33.8163732], [108.8342531, 33.8148798], [108.8382684, 33.8152639], [108.8394825, 33.819853], [108.8492378, 33.826386], [108.8527685, 33.824364], [108.8589945, 33.8227899], [108.8605091, 33.8213887], [108.8704448, 33.824652], [108.8706759, 33.8263795], [108.8757141, 33.826963], [108.8790467, 33.8333644], [108.8903161, 33.8350168], [108.8945908, 33.8401988], [108.8994535, 33.8417777], [108.9012916, 33.8436533], [108.9059579, 33.8463407], [108.908339, 33.8493145], [108.9100829, 33.8548388], [108.9089542, 33.8624705], [108.9113428, 33.864665], [108.9122422, 33.86775], [108.9174959, 33.8687739], [108.9191989, 33.8670632], [108.9247444, 33.8665836], [108.9294232, 33.8644741], [108.9337558, 33.8666795], [108.935753, 33.8656569], [108.9462505, 33.8694946], [108.9534918, 33.8719955], [108.9581528, 33.8766939], [108.9663356, 33.8754087], [108.9692142, 33.8732138], [108.9793906, 33.8776148], [108.9837683, 33.8770832], [108.9860914, 33.8786699], [108.988171, 33.8814514], [108.9922232, 33.883009], [108.9986483, 33.8825295], [109.0071028, 33.8849144], [109.0106569, 33.8916873], [109.0195677, 33.895015], [109.0284915, 33.9030849], [109.0314951, 33.9066728], [109.0330667, 33.9124955], [109.0378493, 33.916836], [109.0376183, 33.9194245], [109.0389601, 33.9215908], [109.0422395, 33.9237386], [109.0455933, 33.9234924], [109.0489501, 33.9209171], [109.0548324, 33.9199039], [109.0605104, 33.916438], [109.0609417, 33.9105906], [109.0666166, 33.9075358], [109.0800881, 33.9117001], [109.0889141, 33.9068646], [109.0972323, 33.9087822], [109.1035318, 33.9073151], [109.1068718, 33.9034194], [109.1120203, 33.8997688], [109.1115418, 33.8914184], [109.1133573, 33.8816224], [109.1141315, 33.8780144], [109.1162949, 33.8775189], [109.1259175, 33.8779221], [109.1295211, 33.8757204], [109.1344223, 33.8728829], [109.1384769, 33.8732984], [109.141607, 33.8774332], [109.1519566, 33.8772684], [109.1551134, 33.8734902], [109.159157, 33.8713799], [109.1683566, 33.8740247], [109.1751708, 33.8736799], [109.1797215, 33.8749291], [109.1839962, 33.8745454], [109.1851733, 33.8719214], [109.1876932, 33.8693655], [109.1947152, 33.8676935], [109.2052078, 33.8672639], [109.2127838, 33.8707009], [109.2172928, 33.8747329], [109.2246018, 33.8774759], [109.2328263, 33.8784104], [109.2466508, 33.8774529], [109.2645873, 33.8768424], [109.2768938, 33.8726309], [109.2880173, 33.8637924], [109.2921828, 33.8568154], [109.3004078, 33.8511009], [109.3111268, 33.8514174], [109.3173293, 33.8548089], [109.3330078, 33.8619154], [109.3459168, 33.8627509], [109.3592833, 33.8597024], [109.3774033, 33.8542749], [109.3905488, 33.8496134], [109.4054108, 33.8431664], [109.4131928, 33.8415604], [109.4255983, 33.8412244], [109.4401628, 33.8421019], [109.4546128, 33.8457224], [109.4654613, 33.8499219], [109.4761428, 33.8513719], [109.4855268, 33.8509449], [109.4920728, 33.8525049], [109.4973068, 33.8526499], [109.5019303, 33.8541564], [109.5119628, 33.8581049], [109.5237578, 33.8593484], [109.5302733, 33.8618204], [109.5319673, 33.8673669], [109.5362923, 33.8766479], [109.5435488, 33.8812029], [109.5497893, 33.8836669], [109.5519103, 33.8926619], [109.5594103, 33.8981364], [109.5708998, 33.9002874], [109.5802308, 33.9012259], [109.5860903, 33.9064294], [109.5876238, 33.9165534], [109.5877228, 33.9282379], [109.5943068, 33.9359779], [109.5955733, 33.9460944], [109.5908968, 33.9523809], [109.5839083, 33.9549409], [109.5772628, 33.9559059], [109.5646818, 33.9599149], [109.5566023, 33.9619829], [109.5501398, 33.9650114], [109.5407868, 33.9711724], [109.5333863, 33.9769244], [109.5304338, 33.9821089], [109.5304033, 33.9894409], [109.5312273, 33.9965669], [109.5341798, 34.0055849], [109.5367888, 34.0161974], [109.5429758, 34.0269089], [109.5461273, 34.0311164], [109.5519333, 34.0450209], [109.5522308, 34.0580944], [109.5586013, 34.0646819], [109.5648498, 34.0671424], [109.5737913, 34.0715139], [109.5836183, 34.0745314], [109.6008528, 34.0788919], [109.6179198, 34.0800399], [109.6312483, 34.0854414], [109.6445083, 34.0995409], [109.6495438, 34.1054039], [109.6557233, 34.1096954], [109.6614838, 34.1107634], [109.6804578, 34.1131019], [109.6965633, 34.1110039], [109.7102128, 34.1081539], [109.7206268, 34.1100309], [109.7308503, 34.1176264], [109.7342833, 34.1218409], [109.7417068, 34.1298219], [109.7486118, 34.1375619], [109.7600478, 34.1495399], [109.7710723, 34.1578444], [109.7765428, 34.1595879], [109.7840889, 34.1597735], [109.7914365, 34.161235], [109.7894292, 34.1648589], [109.7874219, 34.1657648], [109.786692, 34.1675767], [109.7866791, 34.1696589], [109.7804185, 34.1738712], [109.7812296, 34.1763697], [109.773574, 34.1846186], [109.7646497, 34.1849817], [109.7617754, 34.1872128], [109.7562661, 34.1858813], [109.7524627, 34.1844434], [109.7492369, 34.1857243], [109.7472614, 34.1871214], [109.7384904, 34.1889979], [109.7445916, 34.1921285], [109.746338, 34.1971854], [109.7503642, 34.2005401], [109.7546799, 34.206875], [109.7560757, 34.2088839], [109.7547367, 34.2122981], [109.7489788, 34.2128322], [109.749265, 34.2176973], [109.7458509, 34.2195637], [109.743101, 34.2118524], [109.7350855, 34.2110001], [109.7299326, 34.2170603], [109.7239783, 34.2396877], [109.7149322, 34.2448939], [109.6954659, 34.2461244], [109.6910628, 34.2480889], [109.6855318, 34.2477149], [109.6695783, 34.2452314], [109.6526563, 34.2463834], [109.6426468, 34.2474899], [109.6299593, 34.2464679], [109.6222303, 34.2460289], [109.6118013, 34.2507894], [109.5861663, 34.2622374], [109.5745773, 34.2681084], [109.5646363, 34.2676084], [109.5501023, 34.2642324], [109.5456923, 34.2638779], [109.5363233, 34.2629354], [109.5240173, 34.2658004], [109.5183563, 34.2688524], [109.5140763, 34.2721709], [109.5000763, 34.2759054], [109.4905393, 34.2793044], [109.4843598, 34.2814214], [109.4728088, 34.2863694], [109.4630733, 34.2941129], [109.4577408, 34.3026694], [109.4530868, 34.3080404], [109.4490968, 34.3109054], [109.4454498, 34.3175089], [109.4474713, 34.3237649], [109.4503213, 34.3427022], [109.4514544, 34.3502298], [109.4561003, 34.3810884], [109.4560928, 34.3833429], [109.4514923, 34.3874929], [109.4478838, 34.3942644], [109.4474713, 34.4043314], [109.4458778, 34.4093284], [109.4412078, 34.4149244], [109.4244974, 34.4191098], [109.4216002, 34.4240284], [109.4203741, 34.4305245], [109.4183454, 34.4358661], [109.4172884, 34.4377356], [109.4175312, 34.4386362], [109.4194834, 34.4395902], [109.4203765, 34.4407616], [109.4206541, 34.4430701], [109.420344, 34.4467979], [109.4156552, 34.4474322], [109.4163247, 34.4504199], [109.4133639, 34.4509213], [109.413825, 34.4526385], [109.4113669, 34.4531696], [109.4125554, 34.4578226], [109.409094, 34.4623188], [109.4058648, 34.4653394], [109.392951, 34.4657048], [109.393079, 34.4687736], [109.3885051, 34.4689437], [109.3865559, 34.4768133], [109.3869546, 34.4877479], [109.3859483, 34.4962844], [109.3896178, 34.5014304], [109.3899333, 34.5020049], [109.3954318, 34.5084684], [109.3949438, 34.5134964], [109.3929823, 34.5205389], [109.3891068, 34.5334894], [109.3989062, 34.5365419], [109.4102827, 34.5372524], [109.415297, 34.5397604], [109.4204657, 34.5475643], [109.4300332, 34.5422173], [109.4353353, 34.5425604], [109.4386429, 34.5486488], [109.445687, 34.5502818], [109.4415453, 34.5640007], [109.4422533, 34.5778389], [109.4365816, 34.5796947], [109.432716, 34.5831773], [109.4310723, 34.6021214], [109.422123, 34.6085854], [109.4322663, 34.6242944], [109.4307783, 34.6336404], [109.4178108, 34.6461769], [109.4224882, 34.6478099], [109.4195175, 34.6548945], [109.4194053, 34.6675518], [109.4157428, 34.6766287], [109.4179521, 34.67962], [109.4194996, 34.6812049], [109.4243318, 34.6815679], [109.4311828, 34.6840514], [109.4378508, 34.6842424], [109.4374083, 34.6883544], [109.4317323, 34.6909409], [109.4244768, 34.6982954], [109.4246368, 34.7076914], [109.4257738, 34.7139089], [109.4225618, 34.7241289], [109.4165573, 34.7344934], [109.4138338, 34.7399179], [109.4089128, 34.7445869], [109.4004973, 34.7459524], [109.3888093, 34.7458459], [109.3796613, 34.7448959], [109.3672713, 34.7417944], [109.3568878, 34.7371404], [109.3524628, 34.7301404], [109.3511733, 34.7209389], [109.3479998, 34.7105369], [109.3415603, 34.7048529], [109.3340833, 34.7039489], [109.3184583, 34.6984519], [109.3121338, 34.6968954], [109.3035583, 34.6957284], [109.2920533, 34.6979104], [109.2783203, 34.7002564], [109.2638093, 34.7012024], [109.2582323, 34.7014924], [109.2492903, 34.7023734], [109.2426378, 34.7017174], [109.2381973, 34.7013554], [109.2295988, 34.7008704], [109.2182773, 34.6989289], [109.2116163, 34.6984979], [109.2082368, 34.6993104], [109.2057188, 34.6996954], [109.1976393, 34.6999094], [109.1895368, 34.7005844], [109.1878814, 34.7013957], [109.1807328, 34.7048989], [109.1736775, 34.6945599], [109.1745517, 34.6866525], [109.1714918, 34.6798227], [109.1728032, 34.665802], [109.1570664, 34.6535769], [109.1548807, 34.6427886], [109.1430781, 34.6417096], [109.142641, 34.6291213], [109.1325869, 34.6269631], [109.1369582, 34.6154518], [109.1373954, 34.6021398], [109.142641, 34.5855868], [109.130133, 34.5813986], [109.1046334, 34.5761297], [109.0919617, 34.590407], [109.0802979, 34.5867808], [109.077022, 34.5945474], [109.0539538, 34.5948983], [109.0115059, 34.5824981], [109.0074522, 34.5805366], [108.9928582, 34.5747787], [108.9831848, 34.5791802], [108.9805851, 34.5706567], [108.9640924, 34.5668133], [108.9595848, 34.5692677], [108.9551105, 34.5629111], [108.9612304, 34.5546311], [108.9603561, 34.5499507], [108.9704102, 34.5459902], [108.9739073, 34.5344677], [108.9732567, 34.5041117], [108.9707111, 34.502853], [108.976057, 34.4973984], [108.9735113, 34.4953003], [108.9770752, 34.4908943], [108.9739073, 34.4760794], [108.9713485, 34.4522925], [108.9750255, 34.4452886], [108.9706141, 34.4428898], [108.9666381, 34.4487105], [108.9565066, 34.4465111], [108.9594741, 34.4401873], [108.9601613, 34.4262325], [108.956661, 34.422252], [108.9461688, 34.4164823], [108.9407737, 34.4121178], [108.9384776, 34.41083], [108.935611, 34.4098546], [108.9323039, 34.4093584], [108.9300494, 34.4098663], [108.9232863, 34.4089867], [108.9187659, 34.4082078], [108.9156728, 34.405982], [108.9116238, 34.4042595], [108.9095624, 34.4042198], [108.9093294, 34.4042033], [108.9055899, 34.4048221], [108.9031723, 34.4045224], [108.9014357, 34.4034444], [108.8976393, 34.4020849], [108.8928341, 34.3997284], [108.8863918, 34.3976889], [108.8819898, 34.3970848], [108.8723894, 34.3921118], [108.8572851, 34.3907031], [108.8546245, 34.389612], [108.8507796, 34.3880352], [108.8450721, 34.3847107], [108.8413185, 34.3765532], [108.8376922, 34.3747241], [108.8347895, 34.3746912], [108.8284357, 34.3760873], [108.8234685, 34.3750882], [108.8186055, 34.3738134], [108.8177331, 34.3733814], [108.8085491, 34.3684226], [108.8083686, 34.3683318], [108.7976182, 34.362918], [108.7971032, 34.361253], [108.7969745, 34.3548937], [108.796202, 34.3534588], [108.794657, 34.3523605], [108.7924469, 34.3469962], [108.7929055, 34.3469211], [108.7955015, 34.3482209], [108.7972792, 34.3486474], [108.7971461, 34.3463017], [108.7967127, 34.3447852], [108.7958827, 34.3447456], [108.7956062, 34.3458265], [108.7950402, 34.3457504], [108.7948849, 34.3447621], [108.7939018, 34.3448135], [108.7933202, 34.3438019], [108.7927044, 34.3431304], [108.7927918, 34.3428061], [108.7932291, 34.3429452], [108.7937537, 34.342845], [108.7937515, 34.342528], [108.7940772, 34.3424533], [108.7952562, 34.3424406], [108.7954772, 34.3418842], [108.7958895, 34.3417327], [108.7966805, 34.3417706], [108.7966205, 34.3415209], [108.7967661, 34.3414398], [108.7976921, 34.3413891], [108.7977642, 34.3410328], [108.7969545, 34.3411146], [108.7969346, 34.3407287], [108.7963405, 34.3406821], [108.7962381, 34.3401202], [108.7960802, 34.3389135], [108.79573, 34.3388144], [108.7957514, 34.3373669], [108.795788, 34.3371981], [108.7957728, 34.3359193], [108.7911165, 34.3357598], [108.7909449, 34.3351574], [108.7876833, 34.3350865], [108.7879408, 34.3321274], [108.7892068, 34.3322338], [108.7892283, 34.3297707], [108.7914169, 34.3296821], [108.7914384, 34.3275912], [108.7900651, 34.3259963], [108.7901938, 34.3204141], [108.7913096, 34.3191736], [108.7915457, 34.3183938], [108.7903655, 34.318447], [108.7900007, 34.3157], [108.7908161, 34.3156645], [108.7909234, 34.314867], [108.7901509, 34.3148316], [108.7903655, 34.3135555], [108.7899793, 34.3132896], [108.7899578, 34.311021], [108.7889493, 34.3110387], [108.7889493, 34.3105779], [108.7859023, 34.3111805], [108.7860096, 34.3072458], [108.7882412, 34.3073167], [108.7884343, 34.3060228], [108.7908161, 34.3059164], [108.791492, 34.3008735], [108.7921143, 34.3007672], [108.7922216, 34.2983032], [108.7888957, 34.2983032], [108.7891531, 34.296176], [108.7878228, 34.2966901], [108.7877799, 34.2952719], [108.7866319, 34.2951922], [108.7861145, 34.2951854], [108.7860135, 34.2954537], [108.7857393, 34.2956039], [108.7853251, 34.2955022], [108.7850958, 34.2951591], [108.7836599, 34.2951161], [108.7791746, 34.2951224], [108.7782126, 34.2951604], [108.7776689, 34.2952934], [108.7738111, 34.2937123], [108.7737172, 34.295291], [108.7718417, 34.2952794], [108.7716428, 34.2962783], [108.7695803, 34.2963179], [108.7692561, 34.2952723], [108.766735, 34.2951983], [108.7666032, 34.296944], [108.7639528, 34.2971302], [108.7632944, 34.2966604], [108.7577567, 34.2968367], [108.7567996, 34.2968618], [108.7567725, 34.2972893], [108.7555309, 34.297309], [108.7554594, 34.2995394], [108.7528855, 34.2993745], [108.7526792, 34.2997487], [108.7504994, 34.2996682], [108.7504236, 34.2983688], [108.7502415, 34.2971512], [108.748932, 34.297483], [108.7483439, 34.2976335], [108.7473487, 34.2976322], [108.7464173, 34.2973341], [108.7425267, 34.2973849], [108.7427412, 34.2967982], [108.7431789, 34.295664], [108.744179, 34.2946264], [108.7453723, 34.2934184], [108.7459111, 34.2926983], [108.7476172, 34.2908314], [108.7487046, 34.2899505], [108.7496975, 34.2885171], [108.7498215, 34.2878755], [108.7500603, 34.2868983], [108.7500126, 34.2862221], [108.7493964, 34.2849386], [108.7492814, 34.2842651], [108.7468518, 34.2821645], [108.7458659, 34.2816056], [108.7445684, 34.2805893], [108.7441959, 34.2797396], [108.7436796, 34.2787748], [108.7436059, 34.278109], [108.7438329, 34.2770114], [108.7432841, 34.2767138], [108.7428534, 34.2766273], [108.7418058, 34.2777878], [108.7404126, 34.2777857], [108.739928, 34.2791584], [108.7394901, 34.279089], [108.7384548, 34.2807058], [108.7372794, 34.2806334], [108.7359026, 34.2838998], [108.7343803, 34.2837383], [108.7340599, 34.2851536], [108.7329549, 34.2853936], [108.7324045, 34.2861558], [108.7266383, 34.2855118], [108.7255062, 34.2853073], [108.7238434, 34.2851842], [108.7238657, 34.2844858], [108.7240664, 34.284246], [108.724971, 34.2839686], [108.7240559, 34.2836523], [108.7243957, 34.282282], [108.7217999, 34.281607], [108.7222648, 34.2796285], [108.7208233, 34.2786384], [108.7210902, 34.277376], [108.7211729, 34.2767067], [108.71799, 34.2759651], [108.7167153, 34.2764935], [108.7149432, 34.276183], [108.7128279, 34.2768339], [108.7119476, 34.2769091], [108.7102623, 34.2765208], [108.7102665, 34.2775462], [108.7084067, 34.2773358], [108.7083235, 34.2766172], [108.7070938, 34.2763522], [108.7056332, 34.2762542], [108.7049197, 34.2754585], [108.7038412, 34.2752907], [108.7025971, 34.2750112], [108.7004606, 34.2746619], [108.7000277, 34.274728], [108.6992319, 34.2753612], [108.6946104, 34.2747524], [108.694779, 34.2737574], [108.6936689, 34.2734109], [108.6933464, 34.2727423], [108.693388, 34.2719059], [108.6934945, 34.2712658], [108.6927101, 34.2712092], [108.6923648, 34.2719959], [108.6901572, 34.2716188], [108.6903603, 34.270485], [108.6914373, 34.2706158], [108.6917976, 34.2694555], [108.6923465, 34.269552], [108.6921949, 34.2673551], [108.6921988, 34.2667096], [108.6931055, 34.2667058], [108.6928755, 34.2647603], [108.6928291, 34.2642233], [108.6939317, 34.2643031], [108.6940734, 34.2626604], [108.6949901, 34.2628338], [108.6951533, 34.2616195], [108.6952432, 34.2612462], [108.6967458, 34.2612861], [108.6967916, 34.2605218], [108.6973851, 34.2605329], [108.6975656, 34.2596757], [108.6983925, 34.2598062], [108.6989928, 34.2579715], [108.6979194, 34.2573174], [108.6992264, 34.2570942], [108.6992079, 34.2556964], [108.6986349, 34.2556119], [108.6986476, 34.2544508], [108.6955365, 34.2546772], [108.6949476, 34.2548345], [108.6949235, 34.2560112], [108.6943456, 34.2560629], [108.6941063, 34.2570068], [108.6931804, 34.2568373], [108.6931957, 34.2551084], [108.6921119, 34.2551333], [108.690955, 34.254901], [108.6901036, 34.254413], [108.6889684, 34.2539776], [108.6879196, 34.2539261], [108.6866566, 34.254021], [108.6861516, 34.2538703], [108.6855004, 34.2534617], [108.6854621, 34.2528744], [108.6861819, 34.2487751], [108.6840842, 34.2484731], [108.6852309, 34.2455949], [108.6844275, 34.245024], [108.6850927, 34.2431149], [108.6850843, 34.242863], [108.6867906, 34.2412805], [108.6867571, 34.2391456], [108.6866238, 34.23678], [108.6849206, 34.2367435], [108.6848094, 34.2376868], [108.683874, 34.2380371], [108.6838679, 34.2386975], [108.6830245, 34.2386418], [108.6816802, 34.2385667], [108.6805757, 34.238519], [108.6774812, 34.2383918], [108.6766732, 34.2404925], [108.6743538, 34.24028], [108.6741832, 34.2420505], [108.6741586, 34.2423889], [108.6716495, 34.2420367], [108.6712384, 34.2440506], [108.6660645, 34.2435606], [108.6661742, 34.2451854], [108.6656257, 34.2461591], [108.6642367, 34.2457991], [108.6638137, 34.2471452], [108.6631989, 34.2489625], [108.6621586, 34.2485787], [108.6610801, 34.2505234], [108.6614939, 34.2509334], [108.6612368, 34.2521309], [108.6605756, 34.2520453], [108.6602994, 34.2535081], [108.6591017, 34.2530625], [108.6577078, 34.2566761], [108.6555577, 34.2566165], [108.6553834, 34.2567065], [108.6544065, 34.2589575], [108.6554585, 34.2592842], [108.6509377, 34.2690509], [108.6501923, 34.2706612], [108.6364654, 34.2657436], [108.6297865, 34.2632145], [108.625893, 34.2626204], [108.6212596, 34.2627313], [108.6176775, 34.2620763], [108.6151265, 34.2614188], [108.610691, 34.2586128], [108.602477, 34.2513257], [108.597001, 34.2461203], [108.5896632, 34.2435401], [108.5819969, 34.2434043], [108.578547, 34.2424537], [108.571483, 34.2344409], [108.5662808, 34.2313171], [108.5596001, 34.2300947], [108.5545075, 34.2312718], [108.546184, 34.2355274], [108.5382986, 34.2355727], [108.526361, 34.23263], [108.5227468, 34.2300041], [108.5185303, 34.2283289], [108.5115758, 34.2317245], [108.5077426, 34.2318603], [108.5045666, 34.230819], [108.4995422, 34.225256], [108.4948193, 34.2208129], [108.4850173, 34.2165565], [108.4818795, 34.2154667], [108.4795414, 34.2146546], [108.4758725, 34.2118018], [108.4706703, 34.2094922], [108.4671109, 34.2095828], [108.4553376, 34.2125263], [108.4488212, 34.2107602], [108.4409905, 34.2047371], [108.4377597, 34.2027897], [108.4315718, 34.2032879], [108.4253292, 34.2059599], [108.4200364, 34.207012], [108.4196342, 34.207092], [108.4137201, 34.2059146], [108.3993731, 34.2061863], [108.3967994, 34.2076808], [108.3891878, 34.2091752], [108.3839856, 34.2093111], [108.3753336, 34.2103074], [108.3723218, 34.2093564], [108.3679958, 34.2060504], [108.3645459, 34.2040578], [108.3555105, 34.2012045], [108.3495965, 34.202065], [108.3436277, 34.200027], [108.3391374, 34.1993929], [108.3331686, 34.19817], [108.3264331, 34.1999364], [108.320519, 34.1999364], [108.3123598, 34.2052353], [108.3091671, 34.2059281], [108.3065608, 34.2062603], [108.2960414, 34.2106243], [108.2897988, 34.2118018], [108.2822967, 34.2115753], [108.2747396, 34.2080777], [108.2722416, 34.2050847], [108.2597357, 34.2004799], [108.2547526, 34.2012045], [108.2419388, 34.2007969], [108.2381604, 34.2052353], [108.2327939, 34.2078166], [108.2147666, 34.2135319], [108.2053206, 34.2186035], [108.2004743, 34.2200751], [108.1968876, 34.2209807], [108.1917128, 34.2241049], [108.1859082, 34.2269574], [108.1811989, 34.2270479], [108.1731492, 34.2263688], [108.1686863, 34.2251916], [108.1583093, 34.2264141], [108.1523953, 34.2261651], [108.1484252, 34.2241276], [108.1437432, 34.2230862], [108.138678, 34.2208222], [108.1344888, 34.2197129], [108.1303271, 34.2199166], [108.1272058, 34.2208902], [108.1171574, 34.2236522], [108.1126554, 34.2244382], [108.1105098, 34.2252131], [108.1083347, 34.2258529], [108.0924881, 34.2336806], [108.0873954, 34.2336127], [108.0784422, 34.2332958], [108.0741162, 34.234405], [108.0666415, 34.2333411], [108.0634381, 34.2342918], [108.059687, 34.2349483], [108.0552515, 34.235152], [108.0516647, 34.2343597], [108.0497016, 34.2334693], [108.047722, 34.2325715], [108.044409, 34.2324356], [108.0394807, 34.2336806], [108.0371807, 34.2332279], [108.0337687, 34.2317767], [108.0320607, 34.2309416], [108.0240208, 34.229176], [108.0241503, 34.2285839], [108.0245649, 34.2255205], [108.0244476, 34.2250732], [108.0238097, 34.2244982], [108.0220246, 34.2228638], [108.0212759, 34.2220192], [108.0254065, 34.2221017], [108.0253601, 34.2212408], [108.0265451, 34.2211706], [108.0266655, 34.220943], [108.0263351, 34.2187809], [108.0294215, 34.2182061], [108.0289983, 34.2162853], [108.0328463, 34.2157909], [108.0322056, 34.2131813], [108.0317302, 34.2112666], [108.0316083, 34.2112518], [108.0313197, 34.2099104], [108.0309552, 34.2086179], [108.0257305, 34.2100434], [108.0256767, 34.2093387], [108.0252429, 34.2093545], [108.0251716, 34.2081434], [108.0257071, 34.2079663], [108.0255401, 34.2062625], [108.0253828, 34.2062998], [108.0250887, 34.2063017], [108.0250549, 34.2059716], [108.0240209, 34.2059281], [108.0237896, 34.2054148], [108.0235575, 34.2037255], [108.024094, 34.2036717], [108.0238647, 34.2024708], [108.023471, 34.2023623], [108.0235777, 34.201581], [108.0246587, 34.2015611], [108.0246355, 34.2012866], [108.0240635, 34.201321], [108.0237022, 34.1989289], [108.0228321, 34.1990164], [108.0226828, 34.197804], [108.0226107, 34.1974087], [108.0209373, 34.1976248], [108.0208689, 34.1967344], [108.020999, 34.1967236], [108.0208881, 34.1960364], [108.0195924, 34.1963058], [108.0194626, 34.1941581], [108.0187143, 34.1929385], [108.0181928, 34.1923506], [108.0173028, 34.1919453], [108.0182616, 34.190844], [108.0186508, 34.1902515], [108.0189037, 34.1897949], [108.0195668, 34.1885669], [108.0192878, 34.18799], [108.0193627, 34.1878553], [108.019537, 34.1876279], [108.0196641, 34.187283], [108.0198344, 34.1869867], [108.0197984, 34.1868003], [108.0203157, 34.1865135], [108.0206182, 34.1863153], [108.0208403, 34.1860594], [108.0208862, 34.1858788], [108.0208278, 34.1850915], [108.0209984, 34.1848316], [108.0211791, 34.1843767], [108.0212275, 34.1839551], [108.0211939, 34.1821223], [108.0210263, 34.181883], [108.0206206, 34.1816913], [108.0203672, 34.1814913], [108.0202125, 34.1812209], [108.0201387, 34.180535], [108.019683, 34.1799626], [108.0193756, 34.1795253], [108.0194556, 34.178809], [108.01931, 34.1773691], [108.0191807, 34.1768171], [108.0179395, 34.1768283], [108.0175664, 34.17708], [108.0174934, 34.1774913], [108.0160345, 34.1776583], [108.0161382, 34.1781034], [108.016265, 34.1781709], [108.0162617, 34.1784537], [108.01607, 34.1788963], [108.0157938, 34.1790884], [108.015241, 34.179324], [108.0127288, 34.1789517], [108.012181, 34.1785761], [108.0116548, 34.178481], [108.0114011, 34.178163], [108.0113006, 34.1778811], [108.0111677, 34.1754209], [108.0113715, 34.1750553], [108.0112659, 34.1724532], [108.0110056, 34.1720287], [108.0103984, 34.1700535], [108.0100388, 34.1693279], [108.009748, 34.1692879], [108.0095349, 34.1675191], [108.0090708, 34.16655], [108.0088087, 34.1665302], [108.0080026, 34.1647253], [108.0069287, 34.1613457], [108.0044708, 34.1619183], [108.0038773, 34.1606401], [108.0033732, 34.1596979], [108.0021309, 34.1597508], [108.0012113, 34.1595503], [108.0007405, 34.1567371], [108.0000786, 34.1567434], [107.999785, 34.1572377], [107.9972758, 34.1572509], [107.9969559, 34.1531519], [107.9972898, 34.1531049], [107.9972304, 34.1500073], [107.9984599, 34.1498753], [107.9983913, 34.1491663], [107.9955504, 34.1493309], [107.9953356, 34.1487075], [107.995221, 34.1480327], [107.9932766, 34.1481142], [107.9932076, 34.1477223], [107.9931574, 34.1473611], [107.9931701, 34.146936], [107.9933185, 34.1465367], [107.9934208, 34.1462582], [107.9937199, 34.1462095], [107.9939749, 34.1461262], [107.9941062, 34.1458893], [107.9941382, 34.1450353], [107.9941471, 34.1436482], [107.9941387, 34.1426885], [107.9952026, 34.1424247], [107.9960508, 34.1424665], [107.9961878, 34.1422438], [107.9960141, 34.1416894], [107.9959124, 34.1410571], [107.9959782, 34.1406615], [107.9962492, 34.140282], [107.9967002, 34.1401391], [107.996848, 34.1396346], [107.9966081, 34.1383945], [107.9965272, 34.1382876], [107.9965154, 34.1381167], [107.9966393, 34.138007], [107.9966632, 34.1378136], [107.99662, 34.1377136], [107.9965553, 34.1375892], [107.996318, 34.1361853], [107.9961772, 34.1352288], [107.9961501, 34.1350173], [107.9960979, 34.1348961], [107.9961507, 34.1347426], [107.9963132, 34.1346766], [107.9963604, 34.1346146], [107.9962436, 34.1345606], [107.9961425, 34.1345006], [107.996039, 34.1344404], [107.9960436, 34.1343155], [107.9961193, 34.134102], [107.9961484, 34.1339445], [107.9960784, 34.1337185], [107.9960783, 34.1330752], [107.9960472, 34.1325818], [107.9959042, 34.1321032], [107.9958753, 34.1313223], [107.9960219, 34.1310311], [107.9960104, 34.1307314], [107.9957892, 34.1299182], [107.9959638, 34.1295447], [107.9958285, 34.1291843], [107.9957406, 34.1287506], [107.9957942, 34.1283878], [107.9956119, 34.1276898], [107.9954509, 34.1268205], [107.9955307, 34.1264973], [107.9953806, 34.1256179], [107.9952855, 34.1250048], [107.995197, 34.1247026], [107.9951384, 34.1238238], [107.9950521, 34.1233972], [107.9951284, 34.1229886], [107.9953104, 34.1227576], [107.9954036, 34.1223935], [107.9956135, 34.1220912], [107.9957874, 34.1215144], [107.9957275, 34.1210877], [107.9959865, 34.1207138], [107.9960578, 34.1202878], [107.9972633, 34.1183759], [107.9953646, 34.1182464], [107.9949424, 34.1178709], [107.9945527, 34.1171438], [107.9941816, 34.1165014], [107.9941192, 34.1162298], [107.9939778, 34.1158625], [107.9937578, 34.1155371], [107.992807, 34.1143565], [107.9932034, 34.11385], [107.9939604, 34.1123057], [107.9942251, 34.1115605], [107.9942621, 34.1103036], [107.9945798, 34.1098616], [107.9960034, 34.1087194], [107.9964269, 34.1070518], [107.9968991, 34.1057341], [107.9969141, 34.1040761], [107.9962499, 34.1028232], [107.9953712, 34.1018279], [107.9948728, 34.1016962], [107.9931873, 34.1010982], [107.9919992, 34.1005776], [107.9917844, 34.1003482], [107.9916956, 34.0998806], [107.9918791, 34.0996219], [107.9925883, 34.0990168], [107.9920538, 34.0986956], [107.9915585, 34.0986468], [107.9909676, 34.0982881], [107.9906815, 34.0979567], [107.9900405, 34.0976115], [107.9891656, 34.0968803], [107.988442, 34.0969436], [107.9873965, 34.0951055], [107.9865537, 34.0933935], [107.9857347, 34.0928155], [107.9816608, 34.0889585], [107.981931, 34.0882369], [107.9819996, 34.0874797], [107.9809206, 34.0866891], [107.9800346, 34.0854168], [107.9778598, 34.0844662], [107.9772455, 34.0833152], [107.9742124, 34.0819127], [107.9726175, 34.0792669], [107.9725063, 34.0772009], [107.970044, 34.0766604], [107.968873, 34.0762816], [107.9677287, 34.0765301], [107.966651, 34.0770982], [107.966893, 34.076425], [107.9667588, 34.0754926], [107.9665849, 34.0740987], [107.9666747, 34.0733579], [107.9672053, 34.0721587], [107.9668751, 34.0711875], [107.9669583, 34.0701757], [107.9665386, 34.0692485], [107.9668526, 34.0681164], [107.9667136, 34.0670079], [107.9673571, 34.066573], [107.9682837, 34.0630186], [107.9659155, 34.0598256], [107.9636332, 34.058268], [107.9599268, 34.0572149], [107.9593012, 34.0562601], [107.9577743, 34.0556608], [107.957553, 34.0551734], [107.9562092, 34.0547319], [107.9554037, 34.0540076], [107.9548557, 34.0526789], [107.9537069, 34.0522391], [107.9523386, 34.051148], [107.9524269, 34.050318], [107.9528573, 34.0482674], [107.9503432, 34.0470506], [107.9476575, 34.0461894], [107.9440401, 34.0449241], [107.9435651, 34.0451153], [107.9417923, 34.0448864], [107.9404135, 34.045466], [107.9396621, 34.0452922], [107.9382572, 34.0438039], [107.9386371, 34.04299], [107.9391753, 34.0422401], [107.9394308, 34.0400654], [107.9391743, 34.0355744], [107.9374789, 34.0332801], [107.9361831, 34.0326288], [107.9357474, 34.0284223], [107.9353705, 34.0255201], [107.9365619, 34.0245226], [107.9375607, 34.0225506], [107.939049, 34.0217181], [107.9388127, 34.0208914], [107.9392513, 34.0199287], [107.9387641, 34.0182182], [107.9392233, 34.0166405], [107.9389231, 34.0130154], [107.9382209, 34.0115492], [107.9373465, 34.0099678], [107.9385297, 34.0094427], [107.9379805, 34.0075563], [107.9368798, 34.0075339], [107.9364918, 34.0069497], [107.9365378, 34.0057411], [107.9401258, 34.0040159], [107.9406822, 34.0015138], [107.9423433, 33.9997791], [107.9440371, 33.9980965], [107.9433061, 33.9956133], [107.9436496, 33.9949538], [107.9450072, 33.9922116], [107.9471886, 33.9882159], [107.9456215, 33.9874345], [107.9444127, 33.9863386], [107.9444887, 33.9852264], [107.9437142, 33.9846785], [107.9373049, 33.9848699], [107.9363963, 33.9843428], [107.9325945, 33.9841148], [107.9303377, 33.9843139], [107.9291335, 33.9851474], [107.925101, 33.9856433], [107.9201623, 33.9860454], [107.9173352, 33.9845739], [107.9093523, 33.9891997], [107.9079391, 33.9909837], [107.9073625, 33.9970429], [107.903463, 34.0006843], [107.8958619, 34.0005997], [107.8956936, 34.000541], [107.8898373, 33.9985004], [107.8814607, 33.9966292], [107.8745719, 33.995763], [107.8593678, 33.9865], [107.8585336, 33.9825036], [107.8492663, 33.9781847], [107.8419427, 33.9757399], [107.8349233, 33.980247], [107.8256762, 33.9861542], [107.8258923, 33.9891394], [107.8249606, 33.9902314], [107.8167233, 33.9941652], [107.8150667, 33.9944283], [107.8136714, 33.9947029], [107.8122478, 33.9953765], [107.81151, 33.9957497], [107.8063728, 33.9966282], [107.8061236, 33.9966122], [107.8057223, 33.9968636], [107.8054464, 33.9966248], [107.8050695, 33.996543], [107.804674, 33.9963528], [107.8044432, 33.9961581], [107.8043564, 33.9959001], [107.8041351, 33.9955493], [107.8037831, 33.9951316], [107.8024962, 33.9944472], [107.8015257, 33.9939641], [107.8000906, 33.9930289], [107.7988303, 33.9921648], [107.7985341, 33.9920038], [107.7982791, 33.991826], [107.7981677, 33.9916514], [107.7977769, 33.9912626], [107.7974373, 33.9910865], [107.797061, 33.9908011], [107.7968813, 33.9906837], [107.7967865, 33.9903902], [107.7966677, 33.9895567], [107.7964511, 33.9881409], [107.795899, 33.9872856], [107.7949058, 33.9867871], [107.794554, 33.9868483], [107.793773, 33.9867761], [107.7925704, 33.9868362], [107.7918659, 33.9866807], [107.7915476, 33.9866273], [107.7902404, 33.9867208], [107.7897132, 33.9868399], [107.7887782, 33.9867766], [107.7880452, 33.9869874], [107.78728, 33.9869936], [107.7868869, 33.9868459], [107.7861327, 33.9863751], [107.7853571, 33.9856375], [107.7841384, 33.9842601], [107.7837589, 33.9840021], [107.7834546, 33.9838687], [107.7832321, 33.983593], [107.7824546, 33.9830237], [107.7819614, 33.9825006], [107.7812962, 33.9817093], [107.7808622, 33.9815429], [107.7803097, 33.9813608], [107.7798003, 33.9811379], [107.7795934, 33.9810226], [107.7792598, 33.9809648], [107.7789157, 33.9808894], [107.7787134, 33.9807833], [107.77816, 33.9807055], [107.777807, 33.9806877], [107.7777416, 33.9799538], [107.77776, 33.9793856], [107.7780044, 33.9787126], [107.7777109, 33.9785678], [107.7770966, 33.9774577], [107.7761625, 33.9767042], [107.7755725, 33.9759343], [107.7753811, 33.9758933], [107.775198, 33.9757524], [107.7750474, 33.9755672], [107.7745204, 33.9746918], [107.7740028, 33.9735996], [107.773762, 33.9727577], [107.7736175, 33.9719441], [107.7732783, 33.9713939], [107.7728334, 33.9691489], [107.7730225, 33.9689877], [107.7730146, 33.9687321], [107.7729111, 33.9686628], [107.7725781, 33.9685588], [107.7719463, 33.9680589], [107.7717007, 33.9678615], [107.7715747, 33.9676785], [107.7714522, 33.9674818], [107.7712545, 33.9672941], [107.7706183, 33.966805], [107.77037, 33.9665471], [107.770101, 33.9663515], [107.7696499, 33.9661557], [107.7693706, 33.9660401], [107.7692353, 33.9659051], [107.7693797, 33.965825], [107.7694833, 33.9656867], [107.7694545, 33.9655283], [107.769212, 33.9654108], [107.7690057, 33.9652782], [107.768656, 33.964671], [107.7685396, 33.9642658], [107.7685017, 33.9640734], [107.7672268, 33.9622529], [107.7664936, 33.9612493], [107.7661018, 33.9608797], [107.764954, 33.9598081], [107.7634285, 33.9585401], [107.7630456, 33.9583246], [107.7630082, 33.9580162], [107.7627081, 33.9578039], [107.7624623, 33.957462], [107.7624179, 33.9573029], [107.7621393, 33.9571028], [107.7621129, 33.9566436], [107.7621101, 33.9563579], [107.7623127, 33.9559638], [107.7629671, 33.9557488], [107.7633599, 33.9556114], [107.7637613, 33.955521], [107.7640086, 33.9555278], [107.7642206, 33.9554656], [107.7642679, 33.9553716], [107.7642597, 33.9552826], [107.7643872, 33.9551517], [107.7646748, 33.9551128], [107.7651096, 33.9551686], [107.7655079, 33.9552834], [107.7656571, 33.9553312], [107.7659921, 33.9555826], [107.7660745, 33.9555733], [107.766206, 33.9553548], [107.7661537, 33.9550765], [107.7659298, 33.954816], [107.7657005, 33.9544804], [107.7645138, 33.9540595], [107.7622588, 33.9534208], [107.7616037, 33.9531597], [107.7611276, 33.9528049], [107.7607615, 33.9523164], [107.7602439, 33.9519267], [107.759359, 33.9510938], [107.7589081, 33.9504801], [107.7539953, 33.9493827], [107.7504451, 33.9487115], [107.7484949, 33.9477852], [107.7467072, 33.9461427], [107.7352507, 33.943125], [107.7276601, 33.9386457], [107.718074, 33.9308724], [107.7177681, 33.9308235], [107.7120579, 33.9299098], [107.7116254, 33.9298406], [107.7090434, 33.9279796], [107.7106522, 33.9209577], [107.7130157, 33.9174966], [107.7095863, 33.90442], [107.708834, 33.8993062], [107.7003202, 33.8816379], [107.7029808, 33.8546867], [107.6870175, 33.8246328], [107.6747789, 33.818886], [107.6795679, 33.8078333], [107.6806322, 33.8020854], [107.6694579, 33.7932416], [107.6641368, 33.7835125], [107.655623, 33.779532]]], [[[109.1435827, 33.8021523], [109.1435911, 33.8005837], [109.1437539, 33.7998282], [109.1444365, 33.7992755], [109.1446868, 33.7981615], [109.146558, 33.7968636], [109.1474434, 33.7966767], [109.147692, 33.7949085], [109.1484213, 33.793126], [109.1494928, 33.7925623], [109.150118, 33.7931114], [109.1515583, 33.7935141], [109.1537196, 33.794145], [109.1563659, 33.7951363], [109.1576547, 33.7957032], [109.1590809, 33.7964984], [109.160028, 33.7975804], [109.1605975, 33.797835], [109.1618536, 33.7995874], [109.1624574, 33.800113], [109.1620582, 33.8016175], [109.1621739, 33.8023231], [109.1632081, 33.8032213], [109.1631587, 33.8042981], [109.1637616, 33.8051323], [109.1646108, 33.8056079], [109.1652884, 33.8062262], [109.1659023, 33.8065823], [109.1660012, 33.8073662], [109.1665423, 33.8087915], [109.1672621, 33.8091697], [109.1678445, 33.8091486], [109.1683766, 33.8100556], [109.169132, 33.811305], [109.1698852, 33.8123408], [109.1700214, 33.8129871], [109.1692307, 33.8132055], [109.1669817, 33.8134702], [109.162684, 33.8130986], [109.1588622, 33.8139357], [109.1575489, 33.8147547], [109.1565352, 33.8150668], [109.1543322, 33.815127], [109.1536927, 33.8153849], [109.153243, 33.815214], [109.1527727, 33.8150087], [109.1522922, 33.8137067], [109.1522736, 33.8132562], [109.1515489, 33.8129343], [109.1498928, 33.812897], [109.1480708, 33.8120938], [109.1471565, 33.8114445], [109.145992, 33.8094887], [109.1459534, 33.8084085], [109.145231, 33.8069485], [109.1452982, 33.8062302], [109.1460297, 33.805593], [109.1450565, 33.8047609], [109.144358, 33.803672], [109.1440905, 33.8028408], [109.1435827, 33.8021523]]], [[[109.1705002, 33.8106904], [109.1716846, 33.8101484], [109.1718391, 33.8094923], [109.1727489, 33.8087648], [109.1733841, 33.8078948], [109.1732124, 33.8071673], [109.1737789, 33.8064969], [109.1747402, 33.8060405], [109.1753925, 33.8060405], [109.1762337, 33.806554], [109.1762852, 33.8071531], [109.1764225, 33.807638], [109.1768517, 33.8077521], [109.1773323, 33.8082371], [109.1777443, 33.8082799], [109.1780018, 33.808451], [109.1786713, 33.8087363], [109.1800274, 33.8091214], [109.1803707, 33.809535], [109.1802162, 33.8100628], [109.1799759, 33.8112751], [109.1796326, 33.8119598], [109.1791691, 33.8127299], [109.1777958, 33.8129154], [109.1765255, 33.8129581], [109.1753582, 33.812844], [109.1744827, 33.8126872], [109.1739506, 33.8126586], [109.1732983, 33.8128868], [109.1727661, 33.812787], [109.1722855, 33.8129581], [109.1719421, 33.8129867], [109.1711782, 33.8128797], [109.1707233, 33.8129154], [109.1705002, 33.8106904]]]], &quot;type&quot;: &quot;MultiPolygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {&quot;addresstype&quot;: &quot;city&quot;, &quot;bbox_east&quot;: 109.7914365, &quot;bbox_north&quot;: 34.7459524, &quot;bbox_south&quot;: 33.695219, &quot;bbox_west&quot;: 107.655623, &quot;class&quot;: &quot;boundary&quot;, &quot;display_name&quot;: &quot;Xi\\u0027an, Shaanxi, China&quot;, &quot;importance&quot;: 0.6490852698857119, &quot;lat&quot;: 34.261004, &quot;lon&quot;: 108.9423363, &quot;name&quot;: &quot;Xi\\u0027an&quot;, &quot;osm_id&quot;: 3226004, &quot;osm_type&quot;: &quot;relation&quot;, &quot;place_id&quot;: 225840611, &quot;place_rank&quot;: 10, &quot;type&quot;: &quot;administrative&quot;}, &quot;type&quot;: &quot;Feature&quot;}], &quot;type&quot;: &quot;FeatureCollection&quot;});\n",
       "\n",
       "        \n",
       "    \n",
       "    geo_json_88104525c6835467cb6a87f4392d65d8.bindTooltip(\n",
       "    function(layer){\n",
       "    let div = L.DomUtil.create(&#x27;div&#x27;);\n",
       "    \n",
       "    let handleObject = feature=&gt;typeof(feature)==&#x27;object&#x27; ? JSON.stringify(feature) : feature;\n",
       "    let fields = [&quot;bbox_north&quot;, &quot;bbox_south&quot;, &quot;bbox_east&quot;, &quot;bbox_west&quot;, &quot;place_id&quot;, &quot;osm_type&quot;, &quot;osm_id&quot;, &quot;lat&quot;, &quot;lon&quot;, &quot;class&quot;, &quot;type&quot;, &quot;place_rank&quot;, &quot;importance&quot;, &quot;addresstype&quot;, &quot;name&quot;, &quot;display_name&quot;];\n",
       "    let aliases = [&quot;bbox_north&quot;, &quot;bbox_south&quot;, &quot;bbox_east&quot;, &quot;bbox_west&quot;, &quot;place_id&quot;, &quot;osm_type&quot;, &quot;osm_id&quot;, &quot;lat&quot;, &quot;lon&quot;, &quot;class&quot;, &quot;type&quot;, &quot;place_rank&quot;, &quot;importance&quot;, &quot;addresstype&quot;, &quot;name&quot;, &quot;display_name&quot;];\n",
       "    let table = &#x27;&lt;table&gt;&#x27; +\n",
       "        String(\n",
       "        fields.map(\n",
       "        (v,i)=&gt;\n",
       "        `&lt;tr&gt;\n",
       "            &lt;th&gt;${aliases[i]}&lt;/th&gt;\n",
       "            \n",
       "            &lt;td&gt;${handleObject(layer.feature.properties[v])}&lt;/td&gt;\n",
       "        &lt;/tr&gt;`).join(&#x27;&#x27;))\n",
       "    +&#x27;&lt;/table&gt;&#x27;;\n",
       "    div.innerHTML=table;\n",
       "    \n",
       "    return div\n",
       "    }\n",
       "    ,{&quot;className&quot;: &quot;foliumtooltip&quot;, &quot;sticky&quot;: true});\n",
       "                     \n",
       "    \n",
       "            geo_json_88104525c6835467cb6a87f4392d65d8.addTo(map_615e738a475963454fcca2f5aba9c304);\n",
       "        \n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x2613d70aba0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 交互式展示西安市边界\n",
    "xian.explore()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
